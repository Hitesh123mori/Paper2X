{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 404,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "from typing import List, Dict, Any\n",
    "from typing_extensions import TypedDict\n",
    "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from IPython.display import Image, display\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "# from langchain.document_loaders import PyMuPDFLoader\n",
    "from typing import List, Dict, Any, Optional\n",
    "import fitz\n",
    "from pydantic import BaseModel, Field\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"LANGSMITH_PROJECT\"] = f\"MineD 2025\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "\n",
    "# API_URL = \"https://api-inference.huggingface.co/models/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B\"\n",
    "# headers = {\n",
    "#     \"Authorization\": \"Bearer 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\",\n",
    "#     \"Content-Type\": \"application/json\",\n",
    "#    \"x-wait-for-model\": \"true\"\n",
    "# }\n",
    "# data = {\n",
    "#     \"inputs\": \"Hey, give some idea about creating a podcast from res paper summary \"\n",
    "# }\n",
    "# response = requests.post(API_URL, headers=headers, json=data)\n",
    "# print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    # other params...\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_huggingface import HuggingFaceEndpoint\n",
    "\n",
    "# llm = HuggingFaceEndpoint(\n",
    "#     # repo_id=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
    "#     repo_id=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "#     task=\"text-generation\", \n",
    "#     do_sample=False,\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm = ChatGroq(model=\"gemma2-9b-it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "#base model to hold the metadata, and slide summeries that the llm will extract\n",
    "class ResPaperText(BaseModel):\n",
    "    # authors: str = Field(..., description=\"List of authors of the research paper\")\n",
    "    # title: str = Field(..., description=\"Title of the research paper\")\n",
    "    # submission_date: str = Field(..., description=\"Submission date of the research paper\")\n",
    "    # keywords: List[str] = Field(..., description=\"List of keywords associated with the research paper\")\n",
    "    # references: List[str] = Field(..., description=\"List of references cited in the research paper\")\n",
    "    # abstract: str = Field(..., description=\"Abstract of the research paper\")\n",
    "    conclusion: str = Field(..., description=\"Conclusion of the research paper\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Pydantic Model for PPT slides\n",
    "class SlideContent(BaseModel):\n",
    "    title: str = Field(..., description=\"Title of the particular slide\")\n",
    "    bullet_points: Optional[List[str]] = Field(None, description=\"Content in bullet points form for the slide\")\n",
    "    notes: Optional[str] = Field(None, description=\"Additional notes for the slide\")\n",
    "    images: Optional[List[str]] = Field(None, description=\"List of relevant image paths for the slide\")\n",
    "\n",
    "class PPTPresentation(BaseModel):\n",
    "    title: str = Field(..., description=\"Title of the presentation\")\n",
    "    authors: List[str] = Field(..., description=\"List of authors of the presentation\")\n",
    "    institution: str = Field(..., description=\"Institution associated with the presentation\")\n",
    "    slides: List[SlideContent] = Field(..., description=\"List of slides, in the presentation,which are SlideContent schemas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResPaperExtractState(TypedDict):\n",
    "    pdf_path: Optional[str] = None  # Path to the PDF file\n",
    "    extracted_text: Optional[str] = None  # Full extracted text from the PDF\n",
    "    extracted_images: Optional[List[str]] = None  # Paths to extracted images\n",
    "    slides_content: Optional[List[Dict[str, str]]] = None  # Prepared content for PowerPoint slides\n",
    "    metadata: str\n",
    "    ppt_object: PPTPresentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import fitz\n",
    "# doc = fitz.open(r\"C:\\Users\\milap\\OneDrive\\Desktop\\CLG\\3rd YR\\SEM VI\\mined_2025\\lib\\server\\Milap_Tathya_ICC_June_2025.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pdf(state: ResPaperExtractState):\n",
    "    pdf_path = state[\"pdf_path\"]\n",
    "    doc = fitz.open(pdf_path)  # Load the PDF only once\n",
    "    \n",
    "    extracted_text = []\n",
    "    extracted_images = []\n",
    "    output_folder = \"extracted_images\"\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Iterate through each page\n",
    "    for page_number, page in enumerate(doc):\n",
    "        # Extract text\n",
    "        text = page.get_text(\"text\")\n",
    "        extracted_text.append(text)\n",
    "\n",
    "        # Extract images\n",
    "        for img_index, img in enumerate(page.get_images(full=True)):\n",
    "            xref = img[0]\n",
    "            base_image = doc.extract_image(xref)\n",
    "            image_bytes = base_image[\"image\"]\n",
    "            image_ext = base_image[\"ext\"]\n",
    "            img_filename = f\"{output_folder}/page_{page_number+1}_img_{img_index+1}.{image_ext}\"\n",
    "            \n",
    "            with open(img_filename, \"wb\") as img_file:\n",
    "                img_file.write(image_bytes)\n",
    "            \n",
    "            extracted_images.append(img_filename)\n",
    "\n",
    "    # Combine text from all pages\n",
    "    full_text = \"\\n\".join(extracted_text)\n",
    "\n",
    "    # Update state\n",
    "    return {\"extracted_text\": full_text, \"extracted_images\": extracted_images}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "# condenser_instruction = \"\"\" \n",
    "# You are an AI assistant specialized in processing research papers. \n",
    "\n",
    "# Here is the text extracted from a research paper: {extracted_text}\n",
    "\n",
    "# When tasked with extracting information from the provided text, follow these guidelines, and structure the content accordingly:\n",
    "# 1. **Metadata Extraction:** Identify and extract:\n",
    "#    - Authors  \n",
    "#    - Title  \n",
    "#    - Submission Date  \n",
    "#    - Keywords  \n",
    "#    - References (return as a list) \n",
    "\n",
    "# 2. **Text Structuring:** Organize the content into:\n",
    "#    - Abstract  \n",
    "#    - Conclusion  \n",
    "#    - Body (as a list of sections or paragraphs)  \n",
    "\n",
    "# Ensure the extracted content is well-structured, concise, and retains essential details.\n",
    "\n",
    "# \"\"\"\n",
    "# parser = PydanticOutputParser(pydantic_object=ResPaperText)\n",
    "\n",
    "# condenser_template = ChatPromptTemplate(\n",
    "#    messages=[(\"system\", condenser_instruction),\n",
    "#    (\"human\", \"Extract the details from the given text\")],\n",
    "#    input_variables=[\"extracted_text\"],\n",
    "#    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    "# )\n",
    "# # summarizer = \"\"\" \n",
    "# # Please provide a concise summary of the following research text, highlighting the main points, key findings, and conclusions. \n",
    "# # Focus on summarizing the purpose of the study, the methods used, and the significant results, while avoiding unnecessary details. The text is as follows: {extracted_text}\n",
    "# # \"\"\"\n",
    "# summarizer = \"\"\"\n",
    "# \"You are an expert at creating PowerPoint presentations. Generate a PowerPoint (PPT) presentation that summarizes a research paper. Follow these guidelines:\"\n",
    "\n",
    "# Title Slide:\n",
    "\n",
    "# Include the title of the research paper.\n",
    "# Mention the author(s) and the institution (if available).\n",
    "# Introduction Slide:\n",
    "\n",
    "# Summarize the research problem and objectives.\n",
    "# Highlight the motivation behind the study.\n",
    "# Methods Slide:\n",
    "\n",
    "# Briefly explain the research methodology.\n",
    "# Mention key techniques, datasets, or experimental setups used.\n",
    "# Results Slide:\n",
    "\n",
    "# Summarize the major findings of the study.\n",
    "# Use bullet points or simple visuals (graphs, tables) to illustrate key results.\n",
    "# Discussion/Analysis Slide:\n",
    "\n",
    "# Explain the significance of the results.\n",
    "# Compare findings with previous research (if applicable).\n",
    "# Conclusion Slide:\n",
    "\n",
    "# Summarize key takeaways from the research.\n",
    "# Mention potential future work or applications of the study.\n",
    "# References Slide:\n",
    "\n",
    "# Include citations or sources (if necessary).\n",
    "# Additional Instructions:\n",
    "\n",
    "# Keep the slides concise with minimal text (bullet points preferred).\n",
    "# Use visuals like diagrams, graphs, or charts where applicable.\n",
    "# Maintain a professional and visually appealing slide design.\n",
    "\n",
    "# Here is the given text: {extracted_text}\n",
    "# \"\"\"\n",
    "\n",
    "\n",
    "# # Initialize the Output Parser\n",
    "# parser = PydanticOutputParser(pydantic_object=PPTPresentation)\n",
    "# summarizer_temp = PromptTemplate(\n",
    "#    template=summarizer,\n",
    "#    input_variables=[\"extracted_text\"],\n",
    "#    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    "# )\n",
    "# def get_data(state: ResPaperExtractState):\n",
    "#    extracted_text = state[\"extracted_text\"]\n",
    "#    #  structured_llm = llm.with_structured_output(ResPaperText)\n",
    "#    #  condenser_prompt = condenser_template.format(extracted_text=extracted_text)\n",
    "#    #  response = structured_llm.invoke(condenser_prompt)\n",
    "#    response = llm.invoke(summarizer_temp.format(extracted_text=extracted_text))\n",
    "#    ppt_object = parser.invoke(response)\n",
    "\n",
    "#    return {\"ppt_object\": ppt_object}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = SystemMessagePromptTemplate.from_template(\n",
    "    \"\"\"You are an expert in creating PowerPoint presentations. Generate a structured PowerPoint (PPT) presentation \n",
    "    that summarizes a research paper based on the provided extracted text. Follow these instructions:\n",
    "\n",
    "    - Title Slide: Include the research paper title, authors, and institution.\n",
    "    - Introduction Slide: Summarize the problem, objectives, and motivation.\n",
    "    - Methods Slide: Briefly explain the methodology, datasets, and experimental setup.\n",
    "    - Results Slide: Summarize key findings with bullet points or visuals.\n",
    "    - Discussion Slide: Explain the significance of results and compare with prior work.\n",
    "    - Conclusion Slide: Summarize key takeaways and potential future work.\n",
    "    - References Slide: Include citations if available.\n",
    "\n",
    "    Additional Guidelines:\n",
    "    - Keep slides concise (use bullet points).\n",
    "    - Incorporate visuals where applicable (e.g., diagrams, graphs).\n",
    "    - Maintain a professional and visually appealing slide design.\n",
    "    - Give the text in markdown format.\n",
    "\n",
    "    {format_instructions}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Human Message: Supplies extracted text from the research paper\n",
    "human_message = HumanMessagePromptTemplate.from_template(\"Here is the extracted text:\\n\\n{extracted_text}\")\n",
    "\n",
    "parser = JsonOutputParser(pydantic_object=PPTPresentation)\n",
    "# Combine into a structured chat prompt\n",
    "chat_prompt = ChatPromptTemplate(\n",
    "    messages=[system_message, human_message],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "def get_data(state):\n",
    "    extracted_text = state[\"extracted_text\"]\n",
    "    \n",
    "    # Format prompt with extracted text\n",
    "    \n",
    "    # Invoke LLM with structured output\n",
    "    chain = chat_prompt | llm | parser\n",
    "\n",
    "    # Parse structured output into Pydantic model\n",
    "    ppt_object = chain.invoke({\"extracted_text\":extracted_text})\n",
    "    \n",
    "    return {\"ppt_object\": ppt_object}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = StateGraph(ResPaperExtractState)\n",
    "\n",
    "builder.add_node(\"pdf-2-text\", load_pdf)\n",
    "builder.add_node(\"text-condensation\", get_data)\n",
    "\n",
    "builder.add_edge(START, \"pdf-2-text\")\n",
    "builder.add_edge(\"pdf-2-text\", \"text-condensation\")\n",
    "builder.add_edge(\"text-condensation\", END)\n",
    "\n",
    "graph = builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_output = graph.invoke({\"pdf_path\":r\"C:\\Users\\milap\\OneDrive\\Desktop\\CLG\\3rd YR\\SEM VI\\mined_2025\\lib\\server\\Milap_Tathya_ICC_June_2025.pdf\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 421,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(state_output[\"ppt_object\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node: pdf_path\n",
      "C:\\Users\\milap\\OneDrive\\Desktop\\CLG\\3rd YR\\SEM VI\\mined_2025\\lib\\server\\Milap_Tathya_ICC_June_2025.pdf\n",
      "Node: extracted_text\n",
      "ConvNeXt-based Multi-Class Hydrocarbon Spill\n",
      "Classification in Hyperspectral Imagery\n",
      "Milap Patel, Tathya Patel, Anuja Nair, Member, IEEE, Tarjni Vyas, Shivani Desai,\n",
      "Sudeep Tanwar, Senior Member, IEEE\n",
      "Department of Computer Science and Engineering, School of Technology, Nirma University, Ahmedabad, Gujarat, India\n",
      "Emails: 22bce186@nirmauni.ac.in, 22bce352@nirmauni.ac.in, anuja.nair@nirmauni.ac.in,\n",
      "tarjni.vyas@nirmauni.ac.in, shivani.desai@nirmauni.ac.in, sudeep.tanwar@nirmauni.ac.in\n",
      "Abstract—This paper proposes a new approach of hydrocarbon\n",
      "spill detection using hyperspectral imaging (HSI) and fine-tuning\n",
      "ConvNeXt convolutional neural network (CNN). Hydrocarbon\n",
      "spill hyperspectral dataset (HSHD) containing 124 HSIs into four\n",
      "classes-cleans, gasoline, motor oil, and thinner is used in the\n",
      "training as well as testing phase. To overcome the computational\n",
      "complexity associated with the high spatial dimensions of HSIs\n",
      "(1024 × 1024 × 20), instead of resizing, each image is divided\n",
      "into 16 smaller patches of size 256 × 256 × 20 to ensure that no\n",
      "critical spatial-spectral information is lost. The ConvNeXt model\n",
      "is adapted for 20 spectral channels and has its classification head\n",
      "modified for multi-class prediction. This patch-based approach,\n",
      "coupled with the model’s spectral-spatial learning capabilities,\n",
      "allows for accurate classification with minimal misclassifications,\n",
      "as shown by the confusion matrix. The proposed framework\n",
      "underlines the efficacy of deep learning (DL) in hyperspectral\n",
      "data analysis, offering significant advantages for environmental\n",
      "monitoring and rapid hydrocarbon spill identification.\n",
      "Index Terms—Hydrocarbon spill, ConvNeXt, CNN, spectral-\n",
      "spatial, hyperspectral imaging\n",
      "I. INTRODUCTION\n",
      "Oil spill and hydrocarbon spill is one of the significant\n",
      "environmental challenges, causing adverse and harmful effects\n",
      "on marine ecosystems and coastal communities. Such a release\n",
      "of hydrocarbons into water not only puts aquatic organisms at\n",
      "risk but impacts human activities related to these ecosystems.\n",
      "Oil spill detection, which in the past could only be based on\n",
      "visual analysis or restricted by remote sensing technologies,\n",
      "may be inefficient and time-consuming, as it might not even\n",
      "guarantee the exact nature of the oil involved. On the other\n",
      "hand, hyperspectral imaging (HSI) is advantageous since it\n",
      "captures more spectral information with high spatial coverage\n",
      "in various wavelength ranges; the data is thus useful in dis-\n",
      "tinguishing and determining types of oils. This speeds up and\n",
      "becomes effective in terms of responding to oil spills [1].\n",
      "HSI are rich information sources, which capture spectral data\n",
      "over hundreds of contiguous wavelength bands and go beyond\n",
      "the visible spectrum to the near-infrared and other regions.\n",
      "Traditional RGB images are a representation of only three\n",
      "bands-red, green, and blue-but in the case of HSIs, the spectral\n",
      "signature is highly detailed for each pixel in the spatial domain.\n",
      "This high spectral resolution allows the differentiation and\n",
      "identification of materials and substances through the patterns\n",
      "they give off in their unique reflectance characteristics, making\n",
      "HSIs extremely valuable in a number of applications, including\n",
      "environmental monitoring, mineral exploration, and chemical\n",
      "spill detection. HSI taps both the spectral and spatial informa-\n",
      "tion available and thus provides powerful means for accurate\n",
      "classification and analysis in complex real-world situations.\n",
      "Many researchers have worked in the field of hydrocardon\n",
      "spill classification over the years. Sandhiya et al. [2] proposed\n",
      "an application of machine learning (ML) techniques for oil spill\n",
      "detection using satellite and drone imagery. They highlighted\n",
      "the significant role that the synthetic aperture radar (SAR)\n",
      "technology plays in identifying and monitoring oil pollution in\n",
      "marine environments. They trained ML methods like support\n",
      "vector machine (SVM), decision tree (DT), linear regression\n",
      "(LR) on labeled datasets comprising images of clean water\n",
      "and oil-contaminated water. Moving further, Sherif et al. [3]\n",
      "implemented a deep learning (DL) approach using an artificial\n",
      "neural network (ANN). The model was trained on processed\n",
      "satellite image data, employing techniques like gradient descent\n",
      "to minimize prediction errors. Bui et al. [4] presented another\n",
      "solution with data augmentation and attention mechanism.\n",
      "They used a tailored data augmentation strategy leveraging a\n",
      "conditional generative adversarial network (GAN), specifically\n",
      "the Pix2Pix framework was implemented to generate images\n",
      "that would mimic real oil spills to enhance the diversity of the\n",
      "training dataset. Then, a dual attention mechanism-based DL\n",
      "model was employed, integrating spatial and channel attention\n",
      "modules to boost oil spill classification accuracy. Yang et\n",
      "al. [5] studied high spectral resolution from HSI data and\n",
      "thermal infrared’s sensitivity to temperature differences for\n",
      "identifying oil types. They focused on crude oil, emulsions, and\n",
      "refined products, collecting data using airborne HSI sensors and\n",
      "thermal cameras. The study employed SVM, RF, and convolu-\n",
      "tional neural network (CNN), for classification and found that\n",
      "combining modalities improved recognition accuracy.\n",
      "Based on the research gaps found in state-of-the-art ap-\n",
      "proaches, this paper proposes a new approach of hydrocarbon\n",
      "spill detection using HSI and fine-tuning ConvNeXt CNN. Hy-\n",
      "drocarbon spill hyperspectral dataset (HSHD) containing 124\n",
      "HSIs into four classes-cleans, gasoline, motor oil, and thinner\n",
      "is used in the training as well as testing phase. To overcome\n",
      "the computational complexity associated with the high spatial\n",
      "dimensions of HSIs (1024×1024×20), instead of resizing, each\n",
      "image is divided into 16 smaller patches of size 256×256×20\n",
      "to ensure that no critical spatial-spectral information is lost. The\n",
      "\n",
      "ConvNeXt model is adapted for 20 spectral channels and has\n",
      "its classification head modified for multi-class prediction. This\n",
      "patch-based approach, coupled with the model’s spectral-spatial\n",
      "learning capabilities, allows for accurate classification with\n",
      "minimal misclassifications, as shown by the confusion matrix.\n",
      "The proposed framework underlines the efficacy of DL in HSI\n",
      "data analysis, offering significant advantages for environmental\n",
      "monitoring and rapid hydrocarbon spill identification.\n",
      "A. Motivation\n",
      "1) Oil spills or hydrocarbon spills can severely damage\n",
      "marine ecosystems or agricultural lands sometimes. Early\n",
      "detection using HSI is important for quick response and\n",
      "minimal environmental damage.\n",
      "2) Traditional detection methods do not have the accuracy\n",
      "and spectral resolution required in changing conditions.\n",
      "HSI provides superior accuracy in the identification and\n",
      "quantification of oil spills.\n",
      "3) Recent developments in satellite and HSI sensor technol-\n",
      "ogy improve remote sensing ability, covering huge areas\n",
      "with oil spill monitoring very efficiently, so it finds usage\n",
      "in remedial actions.\n",
      "B. Research Contributions\n",
      "This paper has the following research contributions.\n",
      "1) We utilise the HSHD dataset, which provides detailed\n",
      "HSI data to enhance the model’s ability to accurately\n",
      "classify oil spills.\n",
      "2) We propose a DL-based approach for detecting oil and\n",
      "hydrocarbon spills by fine-tuning various state-of-art\n",
      "CNN architectures, leveraging their strengths in feature\n",
      "extraction from HSI.\n",
      "3) We demonstrate the model’s predictive accuracy using\n",
      "various performance metrics, such as accuracy, precision,\n",
      "recall and F1-score, to evaluate its results .\n",
      "C. Paper Organization\n",
      "The paper is further organized as follows: Section II proposes\n",
      "the system model and the problem formulation, Section III\n",
      "explains the proposed framework, Section IV discusses the\n",
      "results, and Section V provides the conclusion and future work.\n",
      "II. SYSTEM MODEL AND PROBLEM FORMULATION\n",
      "A. System Model\n",
      "The system model of the proposed framework is illustrated\n",
      "in Fig. 1. The detection of oil spills using HSI data leverages\n",
      "advanced remote sensing and DL technologies. The system\n",
      "operates through a multi-step pipeline, starting with data ac-\n",
      "quisition via HSI sensors mounted on satellites and drones. Let\n",
      "X ⊆RH×W ×C represent the set of HSIs captured, where H\n",
      "and W denote the height and width of the images, respectively,\n",
      "and C is the number of spectral channels. Each image is\n",
      "segmented into smaller patches {Pi}, where Pi ∈Rh×w×C,\n",
      "with h and w representing the height and width of the patches,\n",
      "respectively. This segmentation facilitates localized analysis and\n",
      "reduces the computational overhead associated with processing\n",
      "large images. After data acquisition, the HSI data is transmitted\n",
      "to centralized data centers via secure communication channels\n",
      "for preprocessing and analysis. Let X ′ = {P ′\n",
      "i} denote the\n",
      "set of preprocessed patches. Each patch is then fed into the\n",
      "DL model for predictive analysis. The processed insights are\n",
      "subsequently communicated to relevant stakeholders, such as\n",
      "disaster management teams and local rescue operators, en-\n",
      "abling timely remedial actions. The primary objective of the\n",
      "system is to classify each patch {Pi} into its respective class\n",
      "yi ∈Y. A DL-based approach is employed, fine-tuning various\n",
      "CNN architectures for feature extraction and classification.\n",
      "The models are optimized using the AdamW optimizer and\n",
      "CrossEntropyLoss function, with performance metrics such as\n",
      "accuracy, precision, recall, and F1-score used to evaluate their\n",
      "effectiveness.\n",
      "Fig. 1: System model.\n",
      "B. Problem Formulation\n",
      "Let the hyperspectral dataset be as follows:\n",
      "D = {(P ′\n",
      "i, yi)}N\n",
      "i=1,\n",
      "(1)\n",
      "where N is the total number of patches, P ′\n",
      "i\n",
      "∈Rh×w×C\n",
      "represents the ith preprocessed patch of spatial dimensions\n",
      "h×w with C spectral channels, and yi ∈Y is its corresponding\n",
      "class label. The dataset comprises patches derived from high-\n",
      "resolution HSIs, where each patch inherits the label of its parent\n",
      "image, ensuring the preservation of spatial and spectral infor-\n",
      "mation. The objective is to train a DL model fθ, parameterized\n",
      "by θ, that maps an input patch P ′\n",
      "i to its predicted label ˆyi.\n",
      "Mathematically, this can be expressed as:\n",
      "fθ(P ′\n",
      "i) = ˆyi,\n",
      "∀i ∈{1, 2, . . . , N},\n",
      "(2)\n",
      "where ˆyi ∈Y is the predicted label for the ith patch. The\n",
      "training process involves minimizing the CrossEntropyLoss L,\n",
      "which measures the discrepancy between the true labels yi and\n",
      "the predicted probabilities for each class. The loss function is\n",
      "defined as:\n",
      "L(θ) = −1\n",
      "N\n",
      "N\n",
      "X\n",
      "i=1\n",
      "K\n",
      "X\n",
      "k=1\n",
      "1(yi = k) log(pθ(yi = k | P ′\n",
      "i))\n",
      "(3)\n",
      "where:\n",
      "\n",
      "• K is the total number of classes,\n",
      "• pθ(yi = k | P ′\n",
      "i) represents the predicted probability for\n",
      "class k,\n",
      "• 1(·) is an indicator function that evaluates to 1 if yi = k,\n",
      "otherwise 0.\n",
      "The primary objective is to optimize the model parameters θ\n",
      "such that the loss function L(θ) is minimized. This optimization\n",
      "problem can be formally expressed as:\n",
      "θ∗= arg min\n",
      "θ\n",
      "L(θ),\n",
      "(4)\n",
      "where θ∗represents the set of parameters that minimizes the\n",
      "average loss across all training samples.\n",
      "By minimizing L(θ), the model learns to accurately cap-\n",
      "ture the spatial and spectral patterns within the HSI data,\n",
      "enabling robust classification across all classes. The patch-\n",
      "based approach ensures that the large spatial dimensions of the\n",
      "original images do not impose excessive computational burdens\n",
      "while preserving critical spatial-spectral details necessary for\n",
      "accurate classification. Additionally, the use of CrossEntropy-\n",
      "Loss ensures the model effectively handles imbalances in class\n",
      "distributions, promoting stable and efficient learning.\n",
      "III. THE PROPOSED FRAMEWORK\n",
      "The proposed framework, as demonstrated in Fig. 2. It is\n",
      "a three-layered approach containing the data collection layer,\n",
      "intelligent layer, and application layer.\n",
      "Fig. 2: Proposed framework.\n",
      "A. Data Collection Layer\n",
      "The data collection layer forms the base of the framework,\n",
      "using satellite and drone platforms equipped with HSI cameras\n",
      "to collect raw data in a number of electromagnetic spectra.\n",
      "HSI is fundamental in oil spill detection, since it provides\n",
      "detailed spectral information that differentiates oil from water,\n",
      "vegetation, and other substances due to its high spectral res-\n",
      "olution. Recent satellite missions, such as ESA’s Copernicus\n",
      "program with Sentinel satellites and the upcoming EnMAP,\n",
      "provide high-resolution HSI data over large ocean areas. The\n",
      "drones complement these satellites, being flexible and of higher\n",
      "spatial resolution, thus enabling fast, localized data acquisition\n",
      "with advanced HSI cameras. These are then transmitted to the\n",
      "centralized data center via satellite links or through 4G/5G\n",
      "networks, depending on the location of the platform. The in-\n",
      "frastructure will provide real-time data ingestion, preprocessing,\n",
      "and storage of data, readying the data for further analysis by\n",
      "the intelligence layer for effective oil spill classification.\n",
      "B. Intelligence Layer\n",
      "1) Data Preprocessing: The HSHD [8] is a specific set of\n",
      "124 HSIs to classify four different classes: clean samples (un-\n",
      "contaminated), and samples contaminated with gasoline, motor\n",
      "oil, or thinner. Every HSI has a resolution of 1024×1024×20,\n",
      "where the first two dimensions are the spatial resolution, and\n",
      "the third dimension is the spectral channels. The dataset is\n",
      "stored in standard ENVI format, where each sample will have\n",
      "two accompanying files, such as the header metadata (.hdr)\n",
      "that provides wavelength information as well as the spatial\n",
      "resolution; the .dat file will carry the HSI data cube. To\n",
      "load images, Spectral Python (SPy) [9] was used: it enables\n",
      "transformation of ENVI files into appropriate numpy arrays\n",
      "which are more tractable for calculation.\n",
      "Once loaded, the images undergo normalization along the\n",
      "spatial dimensions to ensure consistent scaling and facilitate\n",
      "convergence during model training. Due to the large spatial\n",
      "resolution of 1024×1024, training on full-sized images is com-\n",
      "putationally expensive and risks exceeding available hardware\n",
      "memory. However, directly reducing the spatial dimensions\n",
      "through downsampling can degrade performance by discarding\n",
      "critical spatial information, which is essential for accurate\n",
      "classification. To address this, each HSI is split into 16 smaller\n",
      "patches of 256 × 256 × 20, with each patch inheriting the\n",
      "same label as the original image as shown in Fig. 3. This\n",
      "method does not lose any spatial information but increases the\n",
      "size of the training dataset, which helps mitigate overfitting by\n",
      "exposing the model to more variations during training. Lastly,\n",
      "these processed images are converted to tensors so that they\n",
      "can be used to train the models implemented in the PyTorch\n",
      "deeplearning framework . This preprocessing pipeline balances\n",
      "computational feasibility, meanwhile retaining rich spatial and\n",
      "spectral details required for optimal training of the ConvNeXt\n",
      "model.\n",
      "2) Model Architecture and Training: The ConvNeXt model\n",
      "[10] was adapted and fine-tuned to classify HSIs into four\n",
      "classes: clean, gasoline, motor oil, and thinner. HSIs with\n",
      "20 spectral channels were input to the model, necessitating\n",
      "modifications to the initial convolution layer to handle the 20-\n",
      "channel input. This was accomplished by replacing the stem\n",
      "cell with a 4 × 4 convolution with a stride of 4, which means\n",
      "it downsamples the input dimensions by a factor of 4 × 4 and\n",
      "projects the 20 channels into 96 feature maps. The stages in\n",
      "ConvNeXt consist of depthwise convolutions for spatial feature\n",
      "extraction, inverted bottleneck blocks with an expansion ratio\n",
      "of 4, and 1 × 1 convolutions for channel mixing. It represents\n",
      "\n",
      "Fig. 3: A sample image of type clean (no contamination) from\n",
      "the dataset. The 11th, 8th and 4th band were selected as the\n",
      "red, green and blue components respectively, to display the HSI\n",
      "as RGB image. The image is divided into 16 patches.\n",
      "the spatial feature extraction at every stage as follows:\n",
      "Yi,h,w =\n",
      "kh\n",
      "X\n",
      "u=1\n",
      "kw\n",
      "X\n",
      "v=1\n",
      "Wi,u,vXi,h+u,w+v + bi,\n",
      "(5)\n",
      "where kh and kw denote the kernel dimensions, W and b are the\n",
      "learnable weights and biases, and h, w index spatial dimensions.\n",
      "Depthwise convolutions separately process each input channel,\n",
      "reducing computational overhead.\n",
      "ConvNeXt incorporates inverted bottleneck blocks, where the\n",
      "hidden dimension of the multi-layer perceptron (MLP) block is\n",
      "expanded by a factor of 4:\n",
      "H′ = GELU(W1H + b1),\n",
      "Y = W2H′ + b2,\n",
      "(6)\n",
      "where W1 ∈Rd′×d, W2 ∈Rd×d′, and d′ is the expanded\n",
      "dimension. Layer normalization (LN) replaces batch normal-\n",
      "ization (BN) to stabilize training:\n",
      "LN(X) = X −µ\n",
      "√\n",
      "σ2 + ϵ\n",
      "· γ + β,\n",
      "(7)\n",
      "where µ and σ2 are the mean and variance of the input features,\n",
      "and γ, β are learnable parameters. Gaussian error linear unit\n",
      "(GELU) activation further smooths nonlinear transformations:\n",
      "GELU(x) = x · Φ(x),\n",
      "Φ(x) = 1\n",
      "2\n",
      "\u0014\n",
      "1 + erf\n",
      "\u0012 x\n",
      "√\n",
      "2\n",
      "\u0013\u0015\n",
      ".\n",
      "(8)\n",
      "The classification head was adjusted to output logits for the\n",
      "four target classes via a fully connected layer, minimizing the\n",
      "cross-entropy loss:\n",
      "L = −1\n",
      "N\n",
      "N\n",
      "X\n",
      "i=1\n",
      "4\n",
      "X\n",
      "c=1\n",
      "I[yi = c] log p(c|xi),\n",
      "(9)\n",
      "where p(c|xi) denotes the predicted probability for class c, and\n",
      "I[yi = c] is an indicator function. The model was trained for\n",
      "20 epochs using the AdamW optimizer with a weight decay\n",
      "of 10−2. The learning rate was initialized at 10−4 and reduced\n",
      "by a factor of 0.1 via a ReduceLROnPlateau scheduler if the\n",
      "test loss did not improve for three consecutive epochs. Early\n",
      "stopping with a patience of 5 epochs was employed to prevent\n",
      "overfitting.\n",
      "Algorithm 1 Oil Spill Detection using HSI and ConvNeXt\n",
      "1: Input: Hyperspectral Dataset\n",
      "2: procedure OIL SPILL DETECTOR(Hyperspectral Dataset,\n",
      "ConvNeXt)\n",
      "3: Data preprocessing:\n",
      "4: Create HyperspectralDataset class with following parame-\n",
      "ters:\n",
      "5: patch size = 512\n",
      "6: image size = 1024 × 1024 × 20 channels\n",
      "7: for each Image ∈Hyperspectral Dataset do\n",
      "8:\n",
      "patches ←ExtractPatches(Image, patch size)\n",
      "9:\n",
      "normalized patches ←NormalizeSpectrum(patches)\n",
      "10: end for\n",
      "11: train dataset, test dataset ←Split(dataset, [0.7, 0.3])\n",
      "12: dataloaders ←CreateDataLoader(batch size = 16)\n",
      "13: define ConvNeXt model:\n",
      "14: model ←ConvNeXt Small()\n",
      "15: input layer ←Conv2d(in channels=20, out channels=96)\n",
      "16: output layer ←Linear(out features=4)\n",
      "17: Training Configuration:\n",
      "18: optimizer ←AdamW(lr=0.0001, weight decay=0.01)\n",
      "19: scheduler ←ReduceLROnPlateau(patience=3, factor=0.1)\n",
      "20: loss function ←CrossEntropyLoss()\n",
      "21: for epoch in range(max epochs) do\n",
      "22:\n",
      "Training Phase:\n",
      "23:\n",
      "for each batch ∈train dataloader do\n",
      "24:\n",
      "predictions ←model(batch)\n",
      "25:\n",
      "loss ←loss function(predictions, labels)\n",
      "26:\n",
      "Backward propagation and optimize\n",
      "27:\n",
      "end for\n",
      "28:\n",
      "Evaluation Phase:\n",
      "29:\n",
      "for each batch ∈test dataloader do\n",
      "30:\n",
      "predictions ←model(batch)\n",
      "31:\n",
      "Calculate metrics (F1, Precision, Recall, AUROC)\n",
      "32:\n",
      "end for\n",
      "33:\n",
      "Update learning rate based on validation loss\n",
      "34:\n",
      "Check early stopping condition (patience = 5)\n",
      "35: end for\n",
      "36: Return trained model for oil spill classification\n",
      "C. Application Layer\n",
      "The application layer communicates processed data from the\n",
      "intelligence layer to disaster management authorities, translat-\n",
      "ing HSI insights into actionable responses. Upon identifying oil\n",
      "spills with high precision, real-time information is communi-\n",
      "cated for the quick coordination of containment and remediation\n",
      "\n",
      "actions. Modern disaster management systems rely on real-time\n",
      "data feeds from environmental monitoring platforms to support\n",
      "dynamic decision-making. Advanced tools, such as geographic\n",
      "information systems (GIS) and decision support systems (DSS),\n",
      "help in visualizing the impacts of oil spills, forecasting en-\n",
      "vironmental and economic damage. From this classification\n",
      "data, disaster management teams can initiate a unified response:\n",
      "deployment of containment booms, skimmers, and dispersants,\n",
      "mobilization of cleanup crews and notify affected stakeholders.\n",
      "This integrated approach, using HSI data and advanced tools,\n",
      "enhances the ability to mitigate environmental damage, protect\n",
      "ecosystems, and reduce economic losses.\n",
      "IV. RESULTS AND DISCUSSION\n",
      "A. Experimental Setup\n",
      "The proposed framework is implemented on Kaggle’s code\n",
      "editor. For computational purposes, the Tesla T4 TPU is used.\n",
      "It is equipped with 2,560 CUDA cores and 16GB of GDDR6\n",
      "memory, thus allowing for faster and more efficient processing.\n",
      "We implemented the model using Python version 3.10.14,\n",
      "along with essential libraries such as NumPy version 1.26.4\n",
      "for numerical computations, Pandas version 2.2.3 for data\n",
      "manipulation and preprocessing, Matplotlib version 3.7.5 for\n",
      "data visualization, PyTorch version 2.4.0 for implementing the\n",
      "DL model, and Scikit-learn version 1.2.2 for additional ML\n",
      "functionalities.\n",
      "Fig. 4: Test time accuracy and F1-score comparison between\n",
      "different CNN models.\n",
      "B. Performance Analysis and Discussion\n",
      "Fig. 4 shows the comparative analysis of the four models\n",
      "demonstrating the best performance of ConvNeXt for the given\n",
      "task of classifying the oil spills from the dataset with a test\n",
      "accuracy of 96.93 % and an F1 score of 0.97, which leads\n",
      "to good generalization with balanced precision and recall, thus\n",
      "performing particularly well in identifying spill and non-spill\n",
      "regions correctly. AlexNet performed well, with a test accuracy\n",
      "of 92.82 % and an F1 score of 0.92, which indicates its\n",
      "capability as a simpler yet reliable architecture. EfficientNet\n",
      "v2 achieved moderate results, with a test accuracy of 90.6 %\n",
      "and an F1 score of 0.91, while ResNet50 showed the lowest\n",
      "performance among the models, with a test accuracy of 89.22%\n",
      "and an F1 score of 0.89. These results point to the superiority\n",
      "of ConvNeXt in the ability to tackle complex patterns in\n",
      "hydrocarbon spill detection, thereby suggesting suitability for\n",
      "deployment in real-world applications.\n",
      "The superior performance of ConvNeXt can be attributed\n",
      "to its modern architecture that has the advancements from\n",
      "vision transformers (ViT) [11] with simplicity from CNNs.\n",
      "This hybrid design makes it possible for ConvNeXt to capture\n",
      "both global and local features, which are very important to\n",
      "differentiate between hydrocarbon spills and the background\n",
      "noise in high-resolution images. Moreover, the hierarchical\n",
      "feature extraction in the architecture of ConvNeXt is suitable\n",
      "for data with variable spill patterns and sizes. It should be\n",
      "noticed that all models were trained with the same batch size,\n",
      "which was 16 and patch size, which was 256, and other training\n",
      "parameters, as mentioned in III-B2. Thus, comparison should\n",
      "be fair. ConvNeXt’s architectural improvements and effective\n",
      "feature representation would have been key factors to justify\n",
      "the superiority by this model.\n",
      "Then, Fig. 5 describes the training and test performance of\n",
      "(a)\n",
      "(b)\n",
      "Fig. 5: Training vs test for (a) model accuracy and (b) loss over\n",
      "the epochs for oil spill detection.\n",
      "the proposed ConvNext model on the given task in terms of\n",
      "accuracy and loss. The reduction in both train and test loss\n",
      "over the epochs (Fig. 5b) shows that the model was able to\n",
      "learn well from the dataset, and the overall training process is\n",
      "smooth. This can be credited to the normalized input images,\n",
      "and weight-regularized AdamW optimizer. The test loss is\n",
      "\n",
      "slightly higher than train loss, indicating that the model is\n",
      "slightly overfitting as the epochs increases. one can deduce the\n",
      "robustness of the model from the increasing accuracy of the\n",
      "model over the epochs (Fig. 5a).\n",
      "Fig. 6 shows the confusion matrix for the classification\n",
      "Fig. 6: Confusion matrix for ConvNeXt.\n",
      "performance of the proposed fine-tuned ConvNeXt model on\n",
      "the HSHD. The model shows an overall high accuracy in\n",
      "all four classes: clean, gasoline, motor oil, and thinner, with\n",
      "strong diagonal values. In particular, the model shows a perfect\n",
      "classification rate for the clean class (class 0) with an accuracy\n",
      "of 100%. Moreover, class 2 with motor oil shows a very\n",
      "good correctness level of 99%, making almost all samples\n",
      "get well-classified. Class 1 with gasoline indicates a clear-cut\n",
      "classification with 93% accuracy, though there is a small portion\n",
      "of misclassification as thinner oil (class 3). Class 3 or thinner\n",
      "stands robustly at 95%, causing some minor misclassifications\n",
      "into the gasoline and motor oil classes, respectively.\n",
      "As depcted from Fig. 6 the model is very efficient in distin-\n",
      "guishing the clean class (class 0) from contaminated samples,\n",
      "which is crucial in practical scenarios for rapid detection of\n",
      "uncontaminated areas. Second, the high accuracy for motor\n",
      "oil (class 2) indicates that the spectral features of this class\n",
      "are distinct enough for the model to classify them with near-\n",
      "perfect precision. However, the slight misclassifications ob-\n",
      "served between gasoline (class 1) and thinner (class 3) suggest\n",
      "that these two hydrocarbons share some overlapping spectral\n",
      "characteristics, which might make them harder to differentiate.\n",
      "This overlap might be due to similarities in their chemical\n",
      "compositions.\n",
      "V. CONCLUSION\n",
      "In this work, we proposed a deep DL-based framework for\n",
      "hydrocarbon spill detection using HSI data. We leveraged the\n",
      "ConvNeXt CNN adapted to process 20 spectral channels, and\n",
      "classifying four classes.We divided large HSIs into smaller\n",
      "patches instead of resizing them. This approach ensured that\n",
      "critical spatial-spectral features were preserved and computa-\n",
      "tional constraints could be efficiently managed. This patch-\n",
      "based method reduces the probability of information loss while\n",
      "enhancing model generalization performance across complex\n",
      "HSI datasets. The proposed approach attained high accuracy\n",
      "classification performance with four classes of hydrocarbon\n",
      "contamination, indicating high robustness and reliability. We\n",
      "showed the robustness of the ConvNext model by comparing\n",
      "it with other state-of-the-art CNN models like EfficientNet-\n",
      "V2, AlexNet, and Resnet50. Thus, the overall findings have\n",
      "highlighted the applicability of fine-tuned CNNs to HSI data\n",
      "analysis with many practical implications to environmental\n",
      "monitoring and disaster response systems. Future work could\n",
      "include integrating additional datasets, domain-specific aug-\n",
      "mentation techniques, and real-world deployment to enhance\n",
      "the model’s applicability and performance.\n",
      "REFERENCES\n",
      "[1] A. Bhargava, A. Sachdeva, K. Sharma, M. H. Alsharif, P. Uthansakul, and\n",
      "M. Uthansakul, “Hyperspectral imaging and its applications: A review,”\n",
      "Heliyon, vol. 10, no. 12, p. e33208, 2024.\n",
      "[2] S. S, R. R, S. M, and V. S, “Detection of oil spill events at sea using\n",
      "machine learning,” in 2023 5th International Conference on Inventive\n",
      "Research in Computing Applications (ICIRCA), pp. 53–58, 2023.\n",
      "[3] K. Sherif, F. H. Rizk, A. M. Zaki, M. M. Eid, N. Khodadadi, A. Ibrahim,\n",
      "A. A. Abdelhamid, L. Abualigah, and E.-S. M. El-Kenawy, “Revolution-\n",
      "izing oil spill detection: A machine learning approach for satellite image\n",
      "classification,” in 2024 International Telecommunications Conference\n",
      "(ITC-Egypt), pp. 245–250, 2024.\n",
      "[4] N. A. Bui, Y. Oh, and I. Lee, “Oil spill detection and classification through\n",
      "deep learning and tailored data augmentation,” International Journal of\n",
      "Applied Earth Observation and Geoinformation, vol. 129, p. 103845,\n",
      "2024.\n",
      "[5] J. Yang, Y. Hu, J. Zhang, Y. Ma, Z. Li, and Z. Jiang, “Identification\n",
      "of marine oil spill pollution using hyperspectral combined with thermal\n",
      "infrared remote sensing,” Frontiers in Marine Science, vol. 10, 2023.\n",
      "[6] J. M. Haut, S. Moreno-Alvarez, R. Pastor-Vargas, A. Perez-Garcia, and\n",
      "M. E. Paoletti, “Cloud-based analysis of large-scale hyperspectral imagery\n",
      "for oil spill detection,” IEEE Journal of Selected Topics in Applied Earth\n",
      "Observations and Remote Sensing, vol. 17, pp. 2461–2474, 2024.\n",
      "[7] S. Jia, S. Jiang, Z. Lin, N. Li, M. Xu, and S. Yu, “A survey: Deep\n",
      "learning for hyperspectral image classification with few labeled samples,”\n",
      "Neurocomputing, vol. 448, pp. 179–204, 2021.\n",
      "[8] D. Rivas-Lalaleo and C. Hernandez, “Hydrocarbon spill hyperspectral\n",
      "dataset (hshd,” 2024.\n",
      "[9] Spectral Python Development Team, “Spectral python (spy).” Accessed:\n",
      "2025-01-19.\n",
      "[10] Z. Liu, H. Mao, C.-Y. Wu, C. Feichtenhofer, T. Darrell, and S. Xie, “A\n",
      "convnet for the 2020s,” 2022.\n",
      "[11] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Un-\n",
      "terthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit,\n",
      "and N. Houlsby, “An image is worth 16x16 words: Transformers for image\n",
      "recognition at scale,” 2021.\n",
      "\n",
      "Node: extracted_images\n",
      "['extracted_images/page_2_img_1.png', 'extracted_images/page_3_img_1.png', 'extracted_images/page_4_img_1.png', 'extracted_images/page_5_img_1.png', 'extracted_images/page_5_img_2.png', 'extracted_images/page_5_img_3.png', 'extracted_images/page_6_img_1.png']\n",
      "Node: metadata\n",
      "content=\"## ConvNeXt-based Multi-Class Hydrocarbon Spill Classification in Hyperspectral Imagery\\n\\n**A PowerPoint Presentation**\\n\\n---\\n\\n**Slide 1: Title Slide**\\n\\n**Title:** ConvNeXt-based Multi-Class Hydrocarbon Spill Classification in Hyperspectral Imagery\\n\\n**Authors:** Milap Patel, Tathya Patel, Anuja Nair, Tarjni Vyas, Shivani Desai, Sudeep Tanwar\\n\\n**Institution:** Department of Computer Science and Engineering, School of Technology, Nirma University, Ahmedabad, Gujarat, India\\n\\n\\n---\\n\\n**Slide 2: Introduction**\\n\\n* **Problem:**  Accurate and timely detection of hydrocarbon spills is crucial for environmental protection. Traditional methods are often inefficient.\\n* **Objectives:** Develop a deep learning model for multi-class hydrocarbon spill classification using hyperspectral imagery (HSI).\\n* **Motivation:** HSI offers superior spectral resolution for oil identification; Deep Learning provides powerful feature extraction capabilities.  Faster response times and minimized environmental damage.\\n\\n\\n---\\n\\n**Slide 3: Methods**\\n\\n* **Dataset:** Hydrocarbon Spill Hyperspectral Dataset (HSHD) – 124 HSIs (1024x1024x20) categorized into four classes: clean, gasoline, motor oil, thinner.\\n* **Preprocessing:**  HSIs divided into 16 smaller 256x256x20 patches to manage computational complexity.  Spectral normalization applied.\\n* **Model:** Fine-tuned ConvNeXt CNN architecture.  Input layer modified for 20 spectral channels. Multi-class classification head added.\\n* **Training:** AdamW optimizer, CrossEntropyLoss function, 20 epochs.  Learning rate scheduler and early stopping implemented.\\n\\n\\n---\\n\\n**Slide 4: Results**\\n\\n* **Accuracy:** ConvNeXt achieved 96.93% test accuracy and 0.97 F1-score.  Significantly outperformed AlexNet, EfficientNetV2, and ResNet50 (see comparative graph).\\n* **Confusion Matrix:** (Insert a visually appealing confusion matrix showing high diagonal values indicating accurate classification across all four classes).  Illustrate class-wise accuracy.\\n* **Comparative Graph:** (Bar chart comparing test accuracy and F1-score of ConvNeXt against AlexNet, EfficientNetV2, and ResNet50).\\n\\n\\n---\\n\\n**Slide 5: Discussion/Analysis**\\n\\n* **ConvNeXt Superiority:**  ConvNeXt's hybrid CNN-Transformer architecture effectively captures both local and global features in HSI data, leading to superior performance.\\n* **Comparison with Prior Work:** Outperforms previous methods based on machine learning (ML) and other CNN architectures (cite relevant papers).  Highlights advantages of ConvNeXt's architecture and patch-based approach.\\n* **Limitations:** Minor misclassifications observed between gasoline and thinner classes, potentially due to spectral overlap.\\n\\n\\n---\\n\\n**Slide 6: Conclusion**\\n\\n* **Key Takeaways:**  A fine-tuned ConvNeXt model provides highly accurate multi-class hydrocarbon spill classification using HSI. Patch-based approach effectively addresses computational challenges.\\n* **Future Work:**  Integration of additional datasets, exploration of advanced data augmentation techniques, and real-world deployment for validation and refinement.  Investigate the misclassification between gasoline and thinner.\\n\\n\\n---\\n\\n**Slide 7: References**\\n\\n* [1] Bhargava et al., Heliyon, 2024.\\n* [2] S. S et al., ICIRCA, 2023.\\n* [3] Sherif et al., ITC-Egypt, 2024.\\n* [4] Bui et al., International Journal of Applied Earth Observation and Geoinformation, 2024.\\n* [5] Yang et al., Frontiers in Marine Science, 2023.\\n* [6] Haut et al., IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 2024.\\n* [7] Jia et al., Neurocomputing, 2021.\\n* [8] Rivas-Lalaleo and Hernandez, 2024.\\n* [9] Spectral Python Development Team.\\n* [10] Liu et al., 2022.\\n* [11] Dosovitskiy et al., 2021.\\n\\n\\n---\\n\\n**Note:**  This outline provides a structure for the PowerPoint presentation.  You will need to create the actual slides using PowerPoint software, incorporating the suggested visuals (graphs, charts, confusion matrix) based on the provided research paper.  Remember to keep the text concise and visually appealing.  The graphs and charts should be clearly labeled and easy to understand.  High-quality images will enhance the presentation's impact.\" additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []} id='run-2b0b5f58-2586-43e4-b0a7-a4f601e7a8f7-0' usage_metadata={'input_tokens': 7217, 'output_tokens': 1008, 'total_tokens': 8225, 'input_token_details': {'cache_read': 0}}\n"
     ]
    }
   ],
   "source": [
    "for key, value in state_output.items():\n",
    "    print(f\"Node: {key}\")\n",
    "    if isinstance(value, str) or isinstance(value, list):\n",
    "        print(value)\n",
    "    else:\n",
    "        print(state_output[key])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
