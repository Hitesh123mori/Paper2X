{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "from typing import List, Dict, Any\n",
    "from typing_extensions import TypedDict\n",
    "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from IPython.display import Image, display\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "# from langchain.document_loaders import PyMuPDFLoader\n",
    "from typing import List, Dict, Any, Optional\n",
    "import fitz\n",
    "from pydantic import BaseModel, Field\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"LANGSMITH_PROJECT\"] = f\"MineD 2025\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "\n",
    "# API_URL = \"https://api-inference.huggingface.co/models/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B\"\n",
    "# headers = {\n",
    "#     \"Authorization\": \"Bearer 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\",\n",
    "#     \"Content-Type\": \"application/json\",\n",
    "#    \"x-wait-for-model\": \"true\"\n",
    "# }\n",
    "# data = {\n",
    "#     \"inputs\": \"Hey, give some idea about creating a podcast from res paper summary \"\n",
    "# }\n",
    "# response = requests.post(API_URL, headers=headers, json=data)\n",
    "# print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    # other params...\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_huggingface import HuggingFaceEndpoint\n",
    "\n",
    "# llm = HuggingFaceEndpoint(\n",
    "#     # repo_id=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
    "#     repo_id=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "#     task=\"text-generation\", \n",
    "#     do_sample=False,\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm = ChatGroq(model=\"gemma2-9b-it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#base model to hold the metadata, and slide summeries that the llm will extract\n",
    "class ResPaperText(BaseModel):\n",
    "    # authors: str = Field(..., description=\"List of authors of the research paper\")\n",
    "    # title: str = Field(..., description=\"Title of the research paper\")\n",
    "    # submission_date: str = Field(..., description=\"Submission date of the research paper\")\n",
    "    # keywords: List[str] = Field(..., description=\"List of keywords associated with the research paper\")\n",
    "    # references: List[str] = Field(..., description=\"List of references cited in the research paper\")\n",
    "    # abstract: str = Field(..., description=\"Abstract of the research paper\")\n",
    "    conclusion: str = Field(..., description=\"Conclusion of the research paper\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Pydantic Model for PPT slides\n",
    "class SlideContent(BaseModel):\n",
    "    title: str = Field(..., description=\"Title of the particular slide\")\n",
    "    bullet_points: Optional[List[str]] = Field(None, description=\"Content in bullet points form for the slide\")\n",
    "    notes: Optional[str] = Field(None, description=\"Additional notes for the slide\")\n",
    "    images: Optional[List[str]] = Field(None, description=\"List of relevant image paths for the slide\")\n",
    "\n",
    "class PPTPresentation(BaseModel):\n",
    "    title: str = Field(..., description=\"Title of the presentation\")\n",
    "    authors: List[str] = Field(..., description=\"List of authors of the presentation\")\n",
    "    institution: str = Field(..., description=\"Institution associated with the presentation\")\n",
    "    slides: List[SlideContent] = Field(..., description=\"List of slides, in the presentation,which are SlideContent schemas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResPaperExtractState(TypedDict):\n",
    "    pdf_path: Optional[str] = None  # Path to the PDF file\n",
    "    extracted_text: Optional[str] = None  # Full extracted text from the PDF\n",
    "    extracted_images: Optional[Dict[str,str]] = None  # Paths to extracted images\n",
    "    slides_content: Optional[List[Dict[str, str]]] = None  # Prepared content for PowerPoint slides\n",
    "    metadata: str\n",
    "    ppt_object: PPTPresentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import fitz\n",
    "# doc = fitz.open(r\"C:\\Users\\milap\\OneDrive\\Desktop\\CLG\\3rd YR\\SEM VI\\mined_2025\\lib\\server\\Milap_Tathya_ICC_June_2025.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pdf(state: ResPaperExtractState):\n",
    "    pdf_path = state[\"pdf_path\"]\n",
    "    doc = fitz.open(pdf_path)  # Load the PDF only once\n",
    "    \n",
    "    extracted_text = []\n",
    "    extracted_images = dict()\n",
    "    output_folder = \"extracted_images\"\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Iterate through each page\n",
    "    img_cntr=1\n",
    "    for page_number, page in enumerate(doc):\n",
    "        # Extract text\n",
    "        text = page.get_text(\"text\")\n",
    "        extracted_text.append(text)\n",
    "\n",
    "        # Extract images\n",
    "        for img_index, img in enumerate(page.get_images(full=True)):\n",
    "            xref = img[0]\n",
    "            base_image = doc.extract_image(xref)\n",
    "            image_bytes = base_image[\"image\"]\n",
    "            image_ext = base_image[\"ext\"]\n",
    "            img_filename = f\"{output_folder}/page_{page_number+1}_img_{img_index+1}.{image_ext}\"\n",
    "            \n",
    "            with open(img_filename, \"wb\") as img_file:\n",
    "                img_file.write(image_bytes)\n",
    "            \n",
    "            extracted_images[f\"Fig{img_cntr}\"] = img_filename\n",
    "            img_cntr+=1\n",
    "\n",
    "    # Combine text from all pages\n",
    "    full_text = \"\\n\".join(extracted_text)\n",
    "\n",
    "    # Update state\n",
    "    return {\"extracted_text\": full_text, \"extracted_images\": extracted_images}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# condenser_instruction = \"\"\" \n",
    "# You are an AI assistant specialized in processing research papers. \n",
    "\n",
    "# Here is the text extracted from a research paper: {extracted_text}\n",
    "\n",
    "# When tasked with extracting information from the provided text, follow these guidelines, and structure the content accordingly:\n",
    "# 1. **Metadata Extraction:** Identify and extract:\n",
    "#    - Authors  \n",
    "#    - Title  \n",
    "#    - Submission Date  \n",
    "#    - Keywords  \n",
    "#    - References (return as a list) \n",
    "\n",
    "# 2. **Text Structuring:** Organize the content into:\n",
    "#    - Abstract  \n",
    "#    - Conclusion  \n",
    "#    - Body (as a list of sections or paragraphs)  \n",
    "\n",
    "# Ensure the extracted content is well-structured, concise, and retains essential details.\n",
    "\n",
    "# \"\"\"\n",
    "# parser = PydanticOutputParser(pydantic_object=ResPaperText)\n",
    "\n",
    "# condenser_template = ChatPromptTemplate(\n",
    "#    messages=[(\"system\", condenser_instruction),\n",
    "#    (\"human\", \"Extract the details from the given text\")],\n",
    "#    input_variables=[\"extracted_text\"],\n",
    "#    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    "# )\n",
    "# # summarizer = \"\"\" \n",
    "# # Please provide a concise summary of the following research text, highlighting the main points, key findings, and conclusions. \n",
    "# # Focus on summarizing the purpose of the study, the methods used, and the significant results, while avoiding unnecessary details. The text is as follows: {extracted_text}\n",
    "# # \"\"\"\n",
    "# summarizer = \"\"\"\n",
    "# \"You are an expert at creating PowerPoint presentations. Generate a PowerPoint (PPT) presentation that summarizes a research paper. Follow these guidelines:\"\n",
    "\n",
    "# Title Slide:\n",
    "\n",
    "# Include the title of the research paper.\n",
    "# Mention the author(s) and the institution (if available).\n",
    "# Introduction Slide:\n",
    "\n",
    "# Summarize the research problem and objectives.\n",
    "# Highlight the motivation behind the study.\n",
    "# Methods Slide:\n",
    "\n",
    "# Briefly explain the research methodology.\n",
    "# Mention key techniques, datasets, or experimental setups used.\n",
    "# Results Slide:\n",
    "\n",
    "# Summarize the major findings of the study.\n",
    "# Use bullet points or simple visuals (graphs, tables) to illustrate key results.\n",
    "# Discussion/Analysis Slide:\n",
    "\n",
    "# Explain the significance of the results.\n",
    "# Compare findings with previous research (if applicable).\n",
    "# Conclusion Slide:\n",
    "\n",
    "# Summarize key takeaways from the research.\n",
    "# Mention potential future work or applications of the study.\n",
    "# References Slide:\n",
    "\n",
    "# Include citations or sources (if necessary).\n",
    "# Additional Instructions:\n",
    "\n",
    "# Keep the slides concise with minimal text (bullet points preferred).\n",
    "# Use visuals like diagrams, graphs, or charts where applicable.\n",
    "# Maintain a professional and visually appealing slide design.\n",
    "\n",
    "# Here is the given text: {extracted_text}\n",
    "# \"\"\"\n",
    "\n",
    "\n",
    "# # Initialize the Output Parser\n",
    "# parser = PydanticOutputParser(pydantic_object=PPTPresentation)\n",
    "# summarizer_temp = PromptTemplate(\n",
    "#    template=summarizer,\n",
    "#    input_variables=[\"extracted_text\"],\n",
    "#    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    "# )\n",
    "# def get_data(state: ResPaperExtractState):\n",
    "#    extracted_text = state[\"extracted_text\"]\n",
    "#    #  structured_llm = llm.with_structured_output(ResPaperText)\n",
    "#    #  condenser_prompt = condenser_template.format(extracted_text=extracted_text)\n",
    "#    #  response = structured_llm.invoke(condenser_prompt)\n",
    "#    response = llm.invoke(summarizer_temp.format(extracted_text=extracted_text))\n",
    "#    ppt_object = parser.invoke(response)\n",
    "\n",
    "#    return {\"ppt_object\": ppt_object}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = SystemMessagePromptTemplate.from_template(\n",
    "    \"\"\"You are an expert in creating PowerPoint presentations. Generate a structured PowerPoint (PPT) presentation \n",
    "    that summarizes a research paper based on the provided extracted text. Follow these instructions:\n",
    "    \n",
    "    Remember that the objective of this PPT is for a third party to understand the key points of the research paper, and \n",
    "    give them a gist of the research paper.\n",
    "\n",
    "    - Title Slide: Include the research paper title, authors, and institution.\n",
    "    - Introduction Slide: Summarize the problem, objectives, and motivation.\n",
    "    - Methods Slide: Briefly explain the methodology, datasets, and experimental setup.\n",
    "    - Results Slide: Summarize key findings with bullet points. Mention any visuals (graphs, tables) found from the extracted text. You should definetly mention in the presentation any figures related to a performance metric or tables that are mentioned in the extracted text.\n",
    "    - Graphics: Include any images of graphs or charts or other images, relevant to the results,or images depicting a performance metric,\n",
    "      that are mentioned in the extracted text. You can find such images by looking for any captions that mention figures or tables. \n",
    "      It is necessary to name this slide as Graphics.\n",
    "      Note that you should only mention the image number, like Fig1, Fig2, etc...\n",
    "      Include only relevant image names.\n",
    "    - Discussion Slide: Explain the significance of results and compare with prior work.\n",
    "    - Conclusion Slide: Summarize key takeaways and potential future work.\n",
    "    - References Slide: Include citations if available.\n",
    "\n",
    "    Additional Guidelines:\n",
    "    - Keep slides concise (use bullet points).\n",
    "    - Maintain a professional and visually appealing slide design.\n",
    "    - Give the text in markdown format.\n",
    "    - Each slide should have rich information content, summarizing the information related to the particular slide heading, \n",
    "    and also include some content that is related to the slide heading but not directly mentioned in the extracted text.\n",
    "    - Also keep in mind that the text for each slide should not be too lengthy, and should be concise and to the point.\n",
    "\n",
    "    {format_instructions}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Human Message: Supplies extracted text from the research paper\n",
    "human_message = HumanMessagePromptTemplate.from_template(\"Here is the extracted text:\\n\\n{extracted_text}\")\n",
    "\n",
    "parser = JsonOutputParser(pydantic_object=PPTPresentation)\n",
    "# Combine into a structured chat prompt\n",
    "chat_prompt = ChatPromptTemplate(\n",
    "    messages=[system_message, human_message],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "def get_data(state):\n",
    "    extracted_text = state[\"extracted_text\"]\n",
    "    \n",
    "    # Format prompt with extracted text\n",
    "    \n",
    "    # Invoke LLM with structured output\n",
    "    chain = chat_prompt | llm | parser\n",
    "\n",
    "    # Parse structured output into Pydantic model\n",
    "    ppt_object = chain.invoke({\"extracted_text\":extracted_text})\n",
    "    \n",
    "    return {\"ppt_object\": ppt_object}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = StateGraph(ResPaperExtractState)\n",
    "\n",
    "builder.add_node(\"pdf-2-text\", load_pdf)\n",
    "builder.add_node(\"text-condensation\", get_data)\n",
    "\n",
    "builder.add_edge(START, \"pdf-2-text\")\n",
    "builder.add_edge(\"pdf-2-text\", \"text-condensation\")\n",
    "builder.add_edge(\"text-condensation\", END)\n",
    "\n",
    "graph = builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "path1 = r\"C:\\Users\\milap\\OneDrive\\Desktop\\CLG\\3rd YR\\SEM VI\\mined_2025\\lib\\server\\Milap_Tathya_ICC_June_2025.pdf\"\n",
    "path2 = r\"C:\\Users\\milap\\OneDrive\\Desktop\\CLG\\3rd YR\\SEM VI\\mined_2025\\lib\\server\\STORM.pdf\"\n",
    "path3 = r\"C:\\Users\\milap\\OneDrive\\Desktop\\CLG\\3rd YR\\SEM VI\\mined_2025\\lib\\server\\SuFIA.pdf\"\n",
    "path4 = r\"C:\\Users\\milap\\OneDrive\\Desktop\\CLG\\3rd YR\\SEM VI\\mined_2025\\lib\\server\\ankit review.pdf\"\n",
    "state_output = graph.invoke({\"pdf_path\":path1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ConvNeXt-based Multi-Class Hydrocarbon Spill\\nClassification in Hyperspectral Imagery\\nMilap Patel, Tathya Patel, Anuja Nair, Member, IEEE, Tarjni Vyas, Shivani Desai,\\nSudeep Tanwar, Senior Member, IEEE\\nDepartment of Computer Science and Engineering, School of Technology, Nirma University, Ahmedabad, Gujarat, India\\nEmails: 22bce186@nirmauni.ac.in, 22bce352@nirmauni.ac.in, anuja.nair@nirmauni.ac.in,\\ntarjni.vyas@nirmauni.ac.in, shivani.desai@nirmauni.ac.in, sudeep.tanwar@nirmauni.ac.in\\nAbstract—This paper proposes a new approach of hydrocarbon\\nspill detection using hyperspectral imaging (HSI) and fine-tuning\\nConvNeXt convolutional neural network (CNN). Hydrocarbon\\nspill hyperspectral dataset (HSHD) containing 124 HSIs into four\\nclasses-cleans, gasoline, motor oil, and thinner is used in the\\ntraining as well as testing phase. To overcome the computational\\ncomplexity associated with the high spatial dimensions of HSIs\\n(1024 × 1024 × 20), instead of resizing, each image is divided\\ninto 16 smaller patches of size 256 × 256 × 20 to ensure that no\\ncritical spatial-spectral information is lost. The ConvNeXt model\\nis adapted for 20 spectral channels and has its classification head\\nmodified for multi-class prediction. This patch-based approach,\\ncoupled with the model’s spectral-spatial learning capabilities,\\nallows for accurate classification with minimal misclassifications,\\nas shown by the confusion matrix. The proposed framework\\nunderlines the efficacy of deep learning (DL) in hyperspectral\\ndata analysis, offering significant advantages for environmental\\nmonitoring and rapid hydrocarbon spill identification.\\nIndex Terms—Hydrocarbon spill, ConvNeXt, CNN, spectral-\\nspatial, hyperspectral imaging\\nI. INTRODUCTION\\nOil spill and hydrocarbon spill is one of the significant\\nenvironmental challenges, causing adverse and harmful effects\\non marine ecosystems and coastal communities. Such a release\\nof hydrocarbons into water not only puts aquatic organisms at\\nrisk but impacts human activities related to these ecosystems.\\nOil spill detection, which in the past could only be based on\\nvisual analysis or restricted by remote sensing technologies,\\nmay be inefficient and time-consuming, as it might not even\\nguarantee the exact nature of the oil involved. On the other\\nhand, hyperspectral imaging (HSI) is advantageous since it\\ncaptures more spectral information with high spatial coverage\\nin various wavelength ranges; the data is thus useful in dis-\\ntinguishing and determining types of oils. This speeds up and\\nbecomes effective in terms of responding to oil spills [1].\\nHSI are rich information sources, which capture spectral data\\nover hundreds of contiguous wavelength bands and go beyond\\nthe visible spectrum to the near-infrared and other regions.\\nTraditional RGB images are a representation of only three\\nbands-red, green, and blue-but in the case of HSIs, the spectral\\nsignature is highly detailed for each pixel in the spatial domain.\\nThis high spectral resolution allows the differentiation and\\nidentification of materials and substances through the patterns\\nthey give off in their unique reflectance characteristics, making\\nHSIs extremely valuable in a number of applications, including\\nenvironmental monitoring, mineral exploration, and chemical\\nspill detection. HSI taps both the spectral and spatial informa-\\ntion available and thus provides powerful means for accurate\\nclassification and analysis in complex real-world situations.\\nMany researchers have worked in the field of hydrocardon\\nspill classification over the years. Sandhiya et al. [2] proposed\\nan application of machine learning (ML) techniques for oil spill\\ndetection using satellite and drone imagery. They highlighted\\nthe significant role that the synthetic aperture radar (SAR)\\ntechnology plays in identifying and monitoring oil pollution in\\nmarine environments. They trained ML methods like support\\nvector machine (SVM), decision tree (DT), linear regression\\n(LR) on labeled datasets comprising images of clean water\\nand oil-contaminated water. Moving further, Sherif et al. [3]\\nimplemented a deep learning (DL) approach using an artificial\\nneural network (ANN). The model was trained on processed\\nsatellite image data, employing techniques like gradient descent\\nto minimize prediction errors. Bui et al. [4] presented another\\nsolution with data augmentation and attention mechanism.\\nThey used a tailored data augmentation strategy leveraging a\\nconditional generative adversarial network (GAN), specifically\\nthe Pix2Pix framework was implemented to generate images\\nthat would mimic real oil spills to enhance the diversity of the\\ntraining dataset. Then, a dual attention mechanism-based DL\\nmodel was employed, integrating spatial and channel attention\\nmodules to boost oil spill classification accuracy. Yang et\\nal. [5] studied high spectral resolution from HSI data and\\nthermal infrared’s sensitivity to temperature differences for\\nidentifying oil types. They focused on crude oil, emulsions, and\\nrefined products, collecting data using airborne HSI sensors and\\nthermal cameras. The study employed SVM, RF, and convolu-\\ntional neural network (CNN), for classification and found that\\ncombining modalities improved recognition accuracy.\\nBased on the research gaps found in state-of-the-art ap-\\nproaches, this paper proposes a new approach of hydrocarbon\\nspill detection using HSI and fine-tuning ConvNeXt CNN. Hy-\\ndrocarbon spill hyperspectral dataset (HSHD) containing 124\\nHSIs into four classes-cleans, gasoline, motor oil, and thinner\\nis used in the training as well as testing phase. To overcome\\nthe computational complexity associated with the high spatial\\ndimensions of HSIs (1024×1024×20), instead of resizing, each\\nimage is divided into 16 smaller patches of size 256×256×20\\nto ensure that no critical spatial-spectral information is lost. The\\n\\nConvNeXt model is adapted for 20 spectral channels and has\\nits classification head modified for multi-class prediction. This\\npatch-based approach, coupled with the model’s spectral-spatial\\nlearning capabilities, allows for accurate classification with\\nminimal misclassifications, as shown by the confusion matrix.\\nThe proposed framework underlines the efficacy of DL in HSI\\ndata analysis, offering significant advantages for environmental\\nmonitoring and rapid hydrocarbon spill identification.\\nA. Motivation\\n1) Oil spills or hydrocarbon spills can severely damage\\nmarine ecosystems or agricultural lands sometimes. Early\\ndetection using HSI is important for quick response and\\nminimal environmental damage.\\n2) Traditional detection methods do not have the accuracy\\nand spectral resolution required in changing conditions.\\nHSI provides superior accuracy in the identification and\\nquantification of oil spills.\\n3) Recent developments in satellite and HSI sensor technol-\\nogy improve remote sensing ability, covering huge areas\\nwith oil spill monitoring very efficiently, so it finds usage\\nin remedial actions.\\nB. Research Contributions\\nThis paper has the following research contributions.\\n1) We utilise the HSHD dataset, which provides detailed\\nHSI data to enhance the model’s ability to accurately\\nclassify oil spills.\\n2) We propose a DL-based approach for detecting oil and\\nhydrocarbon spills by fine-tuning various state-of-art\\nCNN architectures, leveraging their strengths in feature\\nextraction from HSI.\\n3) We demonstrate the model’s predictive accuracy using\\nvarious performance metrics, such as accuracy, precision,\\nrecall and F1-score, to evaluate its results .\\nC. Paper Organization\\nThe paper is further organized as follows: Section II proposes\\nthe system model and the problem formulation, Section III\\nexplains the proposed framework, Section IV discusses the\\nresults, and Section V provides the conclusion and future work.\\nII. SYSTEM MODEL AND PROBLEM FORMULATION\\nA. System Model\\nThe system model of the proposed framework is illustrated\\nin Fig. 1. The detection of oil spills using HSI data leverages\\nadvanced remote sensing and DL technologies. The system\\noperates through a multi-step pipeline, starting with data ac-\\nquisition via HSI sensors mounted on satellites and drones. Let\\nX ⊆RH×W ×C represent the set of HSIs captured, where H\\nand W denote the height and width of the images, respectively,\\nand C is the number of spectral channels. Each image is\\nsegmented into smaller patches {Pi}, where Pi ∈Rh×w×C,\\nwith h and w representing the height and width of the patches,\\nrespectively. This segmentation facilitates localized analysis and\\nreduces the computational overhead associated with processing\\nlarge images. After data acquisition, the HSI data is transmitted\\nto centralized data centers via secure communication channels\\nfor preprocessing and analysis. Let X ′ = {P ′\\ni} denote the\\nset of preprocessed patches. Each patch is then fed into the\\nDL model for predictive analysis. The processed insights are\\nsubsequently communicated to relevant stakeholders, such as\\ndisaster management teams and local rescue operators, en-\\nabling timely remedial actions. The primary objective of the\\nsystem is to classify each patch {Pi} into its respective class\\nyi ∈Y. A DL-based approach is employed, fine-tuning various\\nCNN architectures for feature extraction and classification.\\nThe models are optimized using the AdamW optimizer and\\nCrossEntropyLoss function, with performance metrics such as\\naccuracy, precision, recall, and F1-score used to evaluate their\\neffectiveness.\\nFig. 1: System model.\\nB. Problem Formulation\\nLet the hyperspectral dataset be as follows:\\nD = {(P ′\\ni, yi)}N\\ni=1,\\n(1)\\nwhere N is the total number of patches, P ′\\ni\\n∈Rh×w×C\\nrepresents the ith preprocessed patch of spatial dimensions\\nh×w with C spectral channels, and yi ∈Y is its corresponding\\nclass label. The dataset comprises patches derived from high-\\nresolution HSIs, where each patch inherits the label of its parent\\nimage, ensuring the preservation of spatial and spectral infor-\\nmation. The objective is to train a DL model fθ, parameterized\\nby θ, that maps an input patch P ′\\ni to its predicted label ˆyi.\\nMathematically, this can be expressed as:\\nfθ(P ′\\ni) = ˆyi,\\n∀i ∈{1, 2, . . . , N},\\n(2)\\nwhere ˆyi ∈Y is the predicted label for the ith patch. The\\ntraining process involves minimizing the CrossEntropyLoss L,\\nwhich measures the discrepancy between the true labels yi and\\nthe predicted probabilities for each class. The loss function is\\ndefined as:\\nL(θ) = −1\\nN\\nN\\nX\\ni=1\\nK\\nX\\nk=1\\n1(yi = k) log(pθ(yi = k | P ′\\ni))\\n(3)\\nwhere:\\n\\n• K is the total number of classes,\\n• pθ(yi = k | P ′\\ni) represents the predicted probability for\\nclass k,\\n• 1(·) is an indicator function that evaluates to 1 if yi = k,\\notherwise 0.\\nThe primary objective is to optimize the model parameters θ\\nsuch that the loss function L(θ) is minimized. This optimization\\nproblem can be formally expressed as:\\nθ∗= arg min\\nθ\\nL(θ),\\n(4)\\nwhere θ∗represents the set of parameters that minimizes the\\naverage loss across all training samples.\\nBy minimizing L(θ), the model learns to accurately cap-\\nture the spatial and spectral patterns within the HSI data,\\nenabling robust classification across all classes. The patch-\\nbased approach ensures that the large spatial dimensions of the\\noriginal images do not impose excessive computational burdens\\nwhile preserving critical spatial-spectral details necessary for\\naccurate classification. Additionally, the use of CrossEntropy-\\nLoss ensures the model effectively handles imbalances in class\\ndistributions, promoting stable and efficient learning.\\nIII. THE PROPOSED FRAMEWORK\\nThe proposed framework, as demonstrated in Fig. 2. It is\\na three-layered approach containing the data collection layer,\\nintelligent layer, and application layer.\\nFig. 2: Proposed framework.\\nA. Data Collection Layer\\nThe data collection layer forms the base of the framework,\\nusing satellite and drone platforms equipped with HSI cameras\\nto collect raw data in a number of electromagnetic spectra.\\nHSI is fundamental in oil spill detection, since it provides\\ndetailed spectral information that differentiates oil from water,\\nvegetation, and other substances due to its high spectral res-\\nolution. Recent satellite missions, such as ESA’s Copernicus\\nprogram with Sentinel satellites and the upcoming EnMAP,\\nprovide high-resolution HSI data over large ocean areas. The\\ndrones complement these satellites, being flexible and of higher\\nspatial resolution, thus enabling fast, localized data acquisition\\nwith advanced HSI cameras. These are then transmitted to the\\ncentralized data center via satellite links or through 4G/5G\\nnetworks, depending on the location of the platform. The in-\\nfrastructure will provide real-time data ingestion, preprocessing,\\nand storage of data, readying the data for further analysis by\\nthe intelligence layer for effective oil spill classification.\\nB. Intelligence Layer\\n1) Data Preprocessing: The HSHD [8] is a specific set of\\n124 HSIs to classify four different classes: clean samples (un-\\ncontaminated), and samples contaminated with gasoline, motor\\noil, or thinner. Every HSI has a resolution of 1024×1024×20,\\nwhere the first two dimensions are the spatial resolution, and\\nthe third dimension is the spectral channels. The dataset is\\nstored in standard ENVI format, where each sample will have\\ntwo accompanying files, such as the header metadata (.hdr)\\nthat provides wavelength information as well as the spatial\\nresolution; the .dat file will carry the HSI data cube. To\\nload images, Spectral Python (SPy) [9] was used: it enables\\ntransformation of ENVI files into appropriate numpy arrays\\nwhich are more tractable for calculation.\\nOnce loaded, the images undergo normalization along the\\nspatial dimensions to ensure consistent scaling and facilitate\\nconvergence during model training. Due to the large spatial\\nresolution of 1024×1024, training on full-sized images is com-\\nputationally expensive and risks exceeding available hardware\\nmemory. However, directly reducing the spatial dimensions\\nthrough downsampling can degrade performance by discarding\\ncritical spatial information, which is essential for accurate\\nclassification. To address this, each HSI is split into 16 smaller\\npatches of 256 × 256 × 20, with each patch inheriting the\\nsame label as the original image as shown in Fig. 3. This\\nmethod does not lose any spatial information but increases the\\nsize of the training dataset, which helps mitigate overfitting by\\nexposing the model to more variations during training. Lastly,\\nthese processed images are converted to tensors so that they\\ncan be used to train the models implemented in the PyTorch\\ndeeplearning framework . This preprocessing pipeline balances\\ncomputational feasibility, meanwhile retaining rich spatial and\\nspectral details required for optimal training of the ConvNeXt\\nmodel.\\n2) Model Architecture and Training: The ConvNeXt model\\n[10] was adapted and fine-tuned to classify HSIs into four\\nclasses: clean, gasoline, motor oil, and thinner. HSIs with\\n20 spectral channels were input to the model, necessitating\\nmodifications to the initial convolution layer to handle the 20-\\nchannel input. This was accomplished by replacing the stem\\ncell with a 4 × 4 convolution with a stride of 4, which means\\nit downsamples the input dimensions by a factor of 4 × 4 and\\nprojects the 20 channels into 96 feature maps. The stages in\\nConvNeXt consist of depthwise convolutions for spatial feature\\nextraction, inverted bottleneck blocks with an expansion ratio\\nof 4, and 1 × 1 convolutions for channel mixing. It represents\\n\\nFig. 3: A sample image of type clean (no contamination) from\\nthe dataset. The 11th, 8th and 4th band were selected as the\\nred, green and blue components respectively, to display the HSI\\nas RGB image. The image is divided into 16 patches.\\nthe spatial feature extraction at every stage as follows:\\nYi,h,w =\\nkh\\nX\\nu=1\\nkw\\nX\\nv=1\\nWi,u,vXi,h+u,w+v + bi,\\n(5)\\nwhere kh and kw denote the kernel dimensions, W and b are the\\nlearnable weights and biases, and h, w index spatial dimensions.\\nDepthwise convolutions separately process each input channel,\\nreducing computational overhead.\\nConvNeXt incorporates inverted bottleneck blocks, where the\\nhidden dimension of the multi-layer perceptron (MLP) block is\\nexpanded by a factor of 4:\\nH′ = GELU(W1H + b1),\\nY = W2H′ + b2,\\n(6)\\nwhere W1 ∈Rd′×d, W2 ∈Rd×d′, and d′ is the expanded\\ndimension. Layer normalization (LN) replaces batch normal-\\nization (BN) to stabilize training:\\nLN(X) = X −µ\\n√\\nσ2 + ϵ\\n· γ + β,\\n(7)\\nwhere µ and σ2 are the mean and variance of the input features,\\nand γ, β are learnable parameters. Gaussian error linear unit\\n(GELU) activation further smooths nonlinear transformations:\\nGELU(x) = x · Φ(x),\\nΦ(x) = 1\\n2\\n\\x14\\n1 + erf\\n\\x12 x\\n√\\n2\\n\\x13\\x15\\n.\\n(8)\\nThe classification head was adjusted to output logits for the\\nfour target classes via a fully connected layer, minimizing the\\ncross-entropy loss:\\nL = −1\\nN\\nN\\nX\\ni=1\\n4\\nX\\nc=1\\nI[yi = c] log p(c|xi),\\n(9)\\nwhere p(c|xi) denotes the predicted probability for class c, and\\nI[yi = c] is an indicator function. The model was trained for\\n20 epochs using the AdamW optimizer with a weight decay\\nof 10−2. The learning rate was initialized at 10−4 and reduced\\nby a factor of 0.1 via a ReduceLROnPlateau scheduler if the\\ntest loss did not improve for three consecutive epochs. Early\\nstopping with a patience of 5 epochs was employed to prevent\\noverfitting.\\nAlgorithm 1 Oil Spill Detection using HSI and ConvNeXt\\n1: Input: Hyperspectral Dataset\\n2: procedure OIL SPILL DETECTOR(Hyperspectral Dataset,\\nConvNeXt)\\n3: Data preprocessing:\\n4: Create HyperspectralDataset class with following parame-\\nters:\\n5: patch size = 512\\n6: image size = 1024 × 1024 × 20 channels\\n7: for each Image ∈Hyperspectral Dataset do\\n8:\\npatches ←ExtractPatches(Image, patch size)\\n9:\\nnormalized patches ←NormalizeSpectrum(patches)\\n10: end for\\n11: train dataset, test dataset ←Split(dataset, [0.7, 0.3])\\n12: dataloaders ←CreateDataLoader(batch size = 16)\\n13: define ConvNeXt model:\\n14: model ←ConvNeXt Small()\\n15: input layer ←Conv2d(in channels=20, out channels=96)\\n16: output layer ←Linear(out features=4)\\n17: Training Configuration:\\n18: optimizer ←AdamW(lr=0.0001, weight decay=0.01)\\n19: scheduler ←ReduceLROnPlateau(patience=3, factor=0.1)\\n20: loss function ←CrossEntropyLoss()\\n21: for epoch in range(max epochs) do\\n22:\\nTraining Phase:\\n23:\\nfor each batch ∈train dataloader do\\n24:\\npredictions ←model(batch)\\n25:\\nloss ←loss function(predictions, labels)\\n26:\\nBackward propagation and optimize\\n27:\\nend for\\n28:\\nEvaluation Phase:\\n29:\\nfor each batch ∈test dataloader do\\n30:\\npredictions ←model(batch)\\n31:\\nCalculate metrics (F1, Precision, Recall, AUROC)\\n32:\\nend for\\n33:\\nUpdate learning rate based on validation loss\\n34:\\nCheck early stopping condition (patience = 5)\\n35: end for\\n36: Return trained model for oil spill classification\\nC. Application Layer\\nThe application layer communicates processed data from the\\nintelligence layer to disaster management authorities, translat-\\ning HSI insights into actionable responses. Upon identifying oil\\nspills with high precision, real-time information is communi-\\ncated for the quick coordination of containment and remediation\\n\\nactions. Modern disaster management systems rely on real-time\\ndata feeds from environmental monitoring platforms to support\\ndynamic decision-making. Advanced tools, such as geographic\\ninformation systems (GIS) and decision support systems (DSS),\\nhelp in visualizing the impacts of oil spills, forecasting en-\\nvironmental and economic damage. From this classification\\ndata, disaster management teams can initiate a unified response:\\ndeployment of containment booms, skimmers, and dispersants,\\nmobilization of cleanup crews and notify affected stakeholders.\\nThis integrated approach, using HSI data and advanced tools,\\nenhances the ability to mitigate environmental damage, protect\\necosystems, and reduce economic losses.\\nIV. RESULTS AND DISCUSSION\\nA. Experimental Setup\\nThe proposed framework is implemented on Kaggle’s code\\neditor. For computational purposes, the Tesla T4 TPU is used.\\nIt is equipped with 2,560 CUDA cores and 16GB of GDDR6\\nmemory, thus allowing for faster and more efficient processing.\\nWe implemented the model using Python version 3.10.14,\\nalong with essential libraries such as NumPy version 1.26.4\\nfor numerical computations, Pandas version 2.2.3 for data\\nmanipulation and preprocessing, Matplotlib version 3.7.5 for\\ndata visualization, PyTorch version 2.4.0 for implementing the\\nDL model, and Scikit-learn version 1.2.2 for additional ML\\nfunctionalities.\\nFig. 4: Test time accuracy and F1-score comparison between\\ndifferent CNN models.\\nB. Performance Analysis and Discussion\\nFig. 4 shows the comparative analysis of the four models\\ndemonstrating the best performance of ConvNeXt for the given\\ntask of classifying the oil spills from the dataset with a test\\naccuracy of 96.93 % and an F1 score of 0.97, which leads\\nto good generalization with balanced precision and recall, thus\\nperforming particularly well in identifying spill and non-spill\\nregions correctly. AlexNet performed well, with a test accuracy\\nof 92.82 % and an F1 score of 0.92, which indicates its\\ncapability as a simpler yet reliable architecture. EfficientNet\\nv2 achieved moderate results, with a test accuracy of 90.6 %\\nand an F1 score of 0.91, while ResNet50 showed the lowest\\nperformance among the models, with a test accuracy of 89.22%\\nand an F1 score of 0.89. These results point to the superiority\\nof ConvNeXt in the ability to tackle complex patterns in\\nhydrocarbon spill detection, thereby suggesting suitability for\\ndeployment in real-world applications.\\nThe superior performance of ConvNeXt can be attributed\\nto its modern architecture that has the advancements from\\nvision transformers (ViT) [11] with simplicity from CNNs.\\nThis hybrid design makes it possible for ConvNeXt to capture\\nboth global and local features, which are very important to\\ndifferentiate between hydrocarbon spills and the background\\nnoise in high-resolution images. Moreover, the hierarchical\\nfeature extraction in the architecture of ConvNeXt is suitable\\nfor data with variable spill patterns and sizes. It should be\\nnoticed that all models were trained with the same batch size,\\nwhich was 16 and patch size, which was 256, and other training\\nparameters, as mentioned in III-B2. Thus, comparison should\\nbe fair. ConvNeXt’s architectural improvements and effective\\nfeature representation would have been key factors to justify\\nthe superiority by this model.\\nThen, Fig. 5 describes the training and test performance of\\n(a)\\n(b)\\nFig. 5: Training vs test for (a) model accuracy and (b) loss over\\nthe epochs for oil spill detection.\\nthe proposed ConvNext model on the given task in terms of\\naccuracy and loss. The reduction in both train and test loss\\nover the epochs (Fig. 5b) shows that the model was able to\\nlearn well from the dataset, and the overall training process is\\nsmooth. This can be credited to the normalized input images,\\nand weight-regularized AdamW optimizer. The test loss is\\n\\nslightly higher than train loss, indicating that the model is\\nslightly overfitting as the epochs increases. one can deduce the\\nrobustness of the model from the increasing accuracy of the\\nmodel over the epochs (Fig. 5a).\\nFig. 6 shows the confusion matrix for the classification\\nFig. 6: Confusion matrix for ConvNeXt.\\nperformance of the proposed fine-tuned ConvNeXt model on\\nthe HSHD. The model shows an overall high accuracy in\\nall four classes: clean, gasoline, motor oil, and thinner, with\\nstrong diagonal values. In particular, the model shows a perfect\\nclassification rate for the clean class (class 0) with an accuracy\\nof 100%. Moreover, class 2 with motor oil shows a very\\ngood correctness level of 99%, making almost all samples\\nget well-classified. Class 1 with gasoline indicates a clear-cut\\nclassification with 93% accuracy, though there is a small portion\\nof misclassification as thinner oil (class 3). Class 3 or thinner\\nstands robustly at 95%, causing some minor misclassifications\\ninto the gasoline and motor oil classes, respectively.\\nAs depcted from Fig. 6 the model is very efficient in distin-\\nguishing the clean class (class 0) from contaminated samples,\\nwhich is crucial in practical scenarios for rapid detection of\\nuncontaminated areas. Second, the high accuracy for motor\\noil (class 2) indicates that the spectral features of this class\\nare distinct enough for the model to classify them with near-\\nperfect precision. However, the slight misclassifications ob-\\nserved between gasoline (class 1) and thinner (class 3) suggest\\nthat these two hydrocarbons share some overlapping spectral\\ncharacteristics, which might make them harder to differentiate.\\nThis overlap might be due to similarities in their chemical\\ncompositions.\\nV. CONCLUSION\\nIn this work, we proposed a deep DL-based framework for\\nhydrocarbon spill detection using HSI data. We leveraged the\\nConvNeXt CNN adapted to process 20 spectral channels, and\\nclassifying four classes.We divided large HSIs into smaller\\npatches instead of resizing them. This approach ensured that\\ncritical spatial-spectral features were preserved and computa-\\ntional constraints could be efficiently managed. This patch-\\nbased method reduces the probability of information loss while\\nenhancing model generalization performance across complex\\nHSI datasets. The proposed approach attained high accuracy\\nclassification performance with four classes of hydrocarbon\\ncontamination, indicating high robustness and reliability. We\\nshowed the robustness of the ConvNext model by comparing\\nit with other state-of-the-art CNN models like EfficientNet-\\nV2, AlexNet, and Resnet50. Thus, the overall findings have\\nhighlighted the applicability of fine-tuned CNNs to HSI data\\nanalysis with many practical implications to environmental\\nmonitoring and disaster response systems. Future work could\\ninclude integrating additional datasets, domain-specific aug-\\nmentation techniques, and real-world deployment to enhance\\nthe model’s applicability and performance.\\nREFERENCES\\n[1] A. Bhargava, A. Sachdeva, K. Sharma, M. H. Alsharif, P. Uthansakul, and\\nM. Uthansakul, “Hyperspectral imaging and its applications: A review,”\\nHeliyon, vol. 10, no. 12, p. e33208, 2024.\\n[2] S. S, R. R, S. M, and V. S, “Detection of oil spill events at sea using\\nmachine learning,” in 2023 5th International Conference on Inventive\\nResearch in Computing Applications (ICIRCA), pp. 53–58, 2023.\\n[3] K. Sherif, F. H. Rizk, A. M. Zaki, M. M. Eid, N. Khodadadi, A. Ibrahim,\\nA. A. Abdelhamid, L. Abualigah, and E.-S. M. El-Kenawy, “Revolution-\\nizing oil spill detection: A machine learning approach for satellite image\\nclassification,” in 2024 International Telecommunications Conference\\n(ITC-Egypt), pp. 245–250, 2024.\\n[4] N. A. Bui, Y. Oh, and I. Lee, “Oil spill detection and classification through\\ndeep learning and tailored data augmentation,” International Journal of\\nApplied Earth Observation and Geoinformation, vol. 129, p. 103845,\\n2024.\\n[5] J. Yang, Y. Hu, J. Zhang, Y. Ma, Z. Li, and Z. Jiang, “Identification\\nof marine oil spill pollution using hyperspectral combined with thermal\\ninfrared remote sensing,” Frontiers in Marine Science, vol. 10, 2023.\\n[6] J. M. Haut, S. Moreno-Alvarez, R. Pastor-Vargas, A. Perez-Garcia, and\\nM. E. Paoletti, “Cloud-based analysis of large-scale hyperspectral imagery\\nfor oil spill detection,” IEEE Journal of Selected Topics in Applied Earth\\nObservations and Remote Sensing, vol. 17, pp. 2461–2474, 2024.\\n[7] S. Jia, S. Jiang, Z. Lin, N. Li, M. Xu, and S. Yu, “A survey: Deep\\nlearning for hyperspectral image classification with few labeled samples,”\\nNeurocomputing, vol. 448, pp. 179–204, 2021.\\n[8] D. Rivas-Lalaleo and C. Hernandez, “Hydrocarbon spill hyperspectral\\ndataset (hshd,” 2024.\\n[9] Spectral Python Development Team, “Spectral python (spy).” Accessed:\\n2025-01-19.\\n[10] Z. Liu, H. Mao, C.-Y. Wu, C. Feichtenhofer, T. Darrell, and S. Xie, “A\\nconvnet for the 2020s,” 2022.\\n[11] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Un-\\nterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit,\\nand N. Houlsby, “An image is worth 16x16 words: Transformers for image\\nrecognition at scale,” 2021.\\n'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_output[\"extracted_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(state_output[\"ppt_object\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'Methods', 'bullet_points': ['Dataset: Hydrocarbon Spill Hyperspectral Dataset (HSHD) with 124 HSIs (1024x1024x20) categorized into four classes: clean, gasoline, motor oil, and thinner.', 'Methodology: Patch-based approach dividing each HSI into 16 smaller patches (256x256x20) to manage computational complexity while preserving spatial-spectral information.', 'Model: Fine-tuned ConvNeXt CNN architecture adapted for 20 spectral channels and multi-class classification.', 'Training: AdamW optimizer, CrossEntropyLoss function, 20 epochs, learning rate scheduler, and early stopping to prevent overfitting.', 'Additional Context:  The patch-based approach is a common technique to handle large hyperspectral images, balancing computational efficiency and information preservation.  Other CNN architectures were also considered and compared.'], 'notes': None, 'images': None}\n"
     ]
    }
   ],
   "source": [
    "ppt_content = state_output[\"ppt_object\"]\n",
    "# print(ppt_content)\n",
    "print(ppt_content[\"slides\"][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Fig1': 'extracted_images/page_2_img_1.png',\n",
       " 'Fig2': 'extracted_images/page_3_img_1.png',\n",
       " 'Fig3': 'extracted_images/page_4_img_1.png',\n",
       " 'Fig4': 'extracted_images/page_5_img_1.png',\n",
       " 'Fig5': 'extracted_images/page_5_img_2.png',\n",
       " 'Fig6': 'extracted_images/page_5_img_3.png',\n",
       " 'Fig7': 'extracted_images/page_6_img_1.png'}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_output[\"extracted_images\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Introduction',\n",
       "  'bullet_points': ['Problem: Inefficient and inaccurate hydrocarbon spill detection using traditional methods.',\n",
       "   'Objectives: Develop a deep learning-based approach for accurate and rapid hydrocarbon spill classification using hyperspectral imagery (HSI).',\n",
       "   'Motivation:  HSI offers superior spectral and spatial resolution for oil spill identification, enabling quicker response and minimizing environmental damage. Traditional methods lack the accuracy and speed needed for effective response.',\n",
       "   'Additional Context: The increasing frequency and severity of oil spills necessitate the development of advanced detection technologies for effective environmental protection and mitigation of economic losses.'],\n",
       "  'notes': None,\n",
       "  'images': None},\n",
       " {'title': 'Methods',\n",
       "  'bullet_points': ['Dataset: Hydrocarbon Spill Hyperspectral Dataset (HSHD) with 124 HSIs (1024x1024x20) categorized into four classes: clean, gasoline, motor oil, and thinner.',\n",
       "   'Methodology: Patch-based approach dividing each HSI into 16 smaller patches (256x256x20) to manage computational complexity while preserving spatial-spectral information.',\n",
       "   'Model: Fine-tuned ConvNeXt CNN architecture adapted for 20 spectral channels and multi-class classification.',\n",
       "   'Training: AdamW optimizer, CrossEntropyLoss function, 20 epochs, learning rate scheduler, and early stopping to prevent overfitting.',\n",
       "   'Additional Context:  The patch-based approach is a common technique to handle large hyperspectral images, balancing computational efficiency and information preservation.  Other CNN architectures were also considered and compared.'],\n",
       "  'notes': None,\n",
       "  'images': None},\n",
       " {'title': 'Results',\n",
       "  'bullet_points': ['ConvNeXt achieved a test accuracy of 96.93% and an F1-score of 0.97.',\n",
       "   \"Figure 4: Comparative analysis of ConvNeXt, AlexNet, EfficientNetV2, and ResNet50, showcasing ConvNeXt's superior performance.\",\n",
       "   'Figure 5: Training and testing accuracy and loss curves over epochs, demonstrating model convergence and slight overfitting.',\n",
       "   \"Figure 6: Confusion matrix illustrating the model's classification performance for each class, highlighting high accuracy and minimal misclassifications.\",\n",
       "   \"Additional Context:  The performance metrics (accuracy, F1-score, precision, recall) are standard evaluation measures for classification tasks.  The confusion matrix provides a detailed breakdown of the model's performance across different classes.\"],\n",
       "  'notes': None,\n",
       "  'images': None},\n",
       " {'title': 'Graphs Slide',\n",
       "  'bullet_points': [],\n",
       "  'notes': None,\n",
       "  'images': ['Fig. 4', 'Fig. 5', 'Fig. 6']},\n",
       " {'title': 'Discussion',\n",
       "  'bullet_points': [\"ConvNeXt's superior performance is attributed to its hybrid CNN-Transformer architecture, effectively capturing both local and global features in HSIs.\",\n",
       "   \"Comparison with other CNN models (AlexNet, EfficientNetV2, ResNet50) highlights ConvNeXt's advantage in handling complex spatial-spectral patterns in hydrocarbon spill detection.\",\n",
       "   'Slight overfitting observed, suggesting potential improvements through regularization techniques or data augmentation.',\n",
       "   'Confusion matrix analysis reveals high accuracy in classifying clean samples and motor oil, with minor misclassifications between gasoline and thinner, potentially due to overlapping spectral characteristics.',\n",
       "   'Additional Context: The choice of ConvNeXt was motivated by its recent success in image classification tasks.  Future work could explore different hyperparameter settings or architectural modifications to further improve performance.'],\n",
       "  'notes': None,\n",
       "  'images': None},\n",
       " {'title': 'Conclusion',\n",
       "  'bullet_points': ['Proposed a robust deep learning framework for multi-class hydrocarbon spill classification using HSI and a fine-tuned ConvNeXt model.',\n",
       "   'Achieved high accuracy (96.93%) and F1-score (0.97) demonstrating the effectiveness of the patch-based approach and ConvNeXt architecture.',\n",
       "   'Future work: Explore larger datasets, incorporate advanced data augmentation techniques, and investigate real-world deployment for practical applications.',\n",
       "   'Additional Context: The research contributes to the field of environmental monitoring and disaster response by providing a more accurate and efficient method for detecting hydrocarbon spills.'],\n",
       "  'notes': None,\n",
       "  'images': None},\n",
       " {'title': 'References',\n",
       "  'bullet_points': ['[1] A. Bhargava, A. Sachdeva, K. Sharma, M. H. Alsharif, P. Uthansakul, and M. Uthansakul, “Hyperspectral imaging and its applications: A review,” Heliyon, vol. 10, no. 12, p. e33208, 2024.',\n",
       "   '[2] S. S, R. R, S. M, and V. S, “Detection of oil spill events at sea using machine learning,” in 2023 5th International Conference on Inventive Research in Computing Applications (ICIRCA), pp. 53–58, 2023.',\n",
       "   '[3] K. Sherif, F. H. Rizk, A. M. Zaki, M. M. Eid, N. Khodadadi, A. Ibrahim, A. A. Abdelhamid, L. Abualigah, and E.-S. M. El-Kenawy, “Revolutionizing oil spill detection: A machine learning approach for satellite image classification,” in 2024 International Telecommunications Conference (ITC-Egypt), pp. 245–250, 2024.',\n",
       "   '[4] N. A. Bui, Y. Oh, and I. Lee, “Oil spill detection and classification through deep learning and tailored data augmentation,” International Journal of Applied Earth Observation and Geoinformation, vol. 129, p. 103845, 2024.',\n",
       "   '[5] J. Yang, Y. Hu, J. Zhang, Y. Ma, Z. Li, and Z. Jiang, “Identification of marine oil spill pollution using hyperspectral combined with thermal infrared remote sensing,” Frontiers in Marine Science, vol. 10, 2023.',\n",
       "   '[6] J. M. Haut, S. Moreno-Alvarez, R. Pastor-Vargas, A. Perez-Garcia, and M. E. Paoletti, “Cloud-based analysis of large-scale hyperspectral imagery for oil spill detection,” IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, vol. 17, pp. 2461–2474, 2024.',\n",
       "   '[7] S. Jia, S. Jiang, Z. Lin, N. Li, M. Xu, and S. Yu, “A survey: Deep learning for hyperspectral image classification with few labeled samples,” Neurocomputing, vol. 448, pp. 179–204, 2021.',\n",
       "   '[8] D. Rivas-Lalaleo and C. Hernandez, “Hydrocarbon spill hyperspectral dataset (hshd,” 2024.',\n",
       "   '[9] Spectral Python Development Team, “Spectral python (spy).” Accessed: 2025-01-19.',\n",
       "   '[10] Z. Liu, H. Mao, C.-Y. Wu, C. Feichtenhofer, T. Darrell, and S. Xie, “A convnet for the 2020s,” 2022.',\n",
       "   '[11] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby, “An image is worth 16x16 words: Transformers for image recognition at scale,” 2021.'],\n",
       "  'notes': None,\n",
       "  'images': None}]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppt_content[\"slides\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PowerPoint presentation saved as hydrocarbon_2.pptx\n"
     ]
    }
   ],
   "source": [
    "from pptx import Presentation\n",
    "from pptx.util import Pt, Inches\n",
    "from pptx.enum.text import PP_ALIGN, MSO_ANCHOR, MSO_AUTO_SIZE\n",
    "from pptx.dml.color import RGBColor\n",
    "\n",
    "class ThemeConfig:\n",
    "    def __init__(self, name=\"modern\"):\n",
    "        themes = {\n",
    "            \"modern\": {\n",
    "                \"background\": RGBColor(30, 30, 30),  # Dark gray background\n",
    "                \"title\": RGBColor(255, 215, 0),  # Gold title text\n",
    "                \"body\": RGBColor(200, 200, 200),  # Light gray body text\n",
    "                \"title_font\": \"Montserrat\",\n",
    "                \"body_font\": \"Lato\",\n",
    "            },\n",
    "            \"vintage\": {\n",
    "                \"background\": RGBColor(245, 222, 179),  # Wheat background\n",
    "                \"title\": RGBColor(139, 69, 19),  # Saddle brown title\n",
    "                \"body\": RGBColor(105, 105, 105),  # Dim gray text\n",
    "                \"title_font\": \"Georgia\",\n",
    "                \"body_font\": \"Times New Roman\",\n",
    "            },\n",
    "            \"corporate\": {\n",
    "                \"background\": RGBColor(255, 255, 255),  # White background\n",
    "                \"title\": RGBColor(0, 51, 102),  # Navy blue title\n",
    "                \"body\": RGBColor(51, 51, 51),  # Dark gray body text\n",
    "                \"title_font\": \"Arial\",\n",
    "                \"body_font\": \"Verdana\",\n",
    "            },\n",
    "            \"minimal\": {\n",
    "                \"background\": RGBColor(240, 240, 240),  # Light gray background\n",
    "                \"title\": RGBColor(50, 50, 50),  # Dark gray title\n",
    "                \"body\": RGBColor(80, 80, 80),  # Slightly lighter gray for body\n",
    "                \"title_font\": \"Helvetica\",\n",
    "                \"body_font\": \"Sans-Serif\",\n",
    "            },\n",
    "            \"bold\": {\n",
    "                \"background\": RGBColor(0, 0, 0),  # Black background\n",
    "                \"title\": RGBColor(255, 0, 0),  # Red title text\n",
    "                \"body\": RGBColor(255, 255, 255),  # White body text\n",
    "                \"title_font\": \"Impact\",\n",
    "                \"body_font\": \"Arial Black\",\n",
    "            }\n",
    "        }\n",
    "        self.theme = themes.get(name, themes[\"minimal\"])\n",
    "\n",
    "def apply_background(slide, color):\n",
    "    \"\"\"Apply background color to a slide\"\"\"\n",
    "    background = slide.background\n",
    "    fill = background.fill\n",
    "    fill.solid()\n",
    "    fill.fore_color.rgb = color\n",
    "\n",
    "def create_ppt_from_dict(ppt_data: dict, image_mapping: dict, theme_name: str=\"default\", output_file: str = \"presentation.pptx\"):\n",
    "    prs = Presentation()\n",
    "    theme = ThemeConfig(theme_name)\n",
    "\n",
    "    slide_width = prs.slide_width\n",
    "    slide_height = prs.slide_height\n",
    "\n",
    "    # Title Slide Fix\n",
    "    title_slide_layout = prs.slide_layouts[0]  # Title slide layout\n",
    "    title_slide = prs.slides.add_slide(title_slide_layout)\n",
    "    apply_background(title_slide, theme.theme[\"background\"])\n",
    "    # Set title\n",
    "    title = title_slide.shapes.title\n",
    "    title.text = ppt_data['title']\n",
    "    title_para = title.text_frame.paragraphs[0]\n",
    "    title_para.font.size = Pt(40)\n",
    "    title_para.font.name = theme.theme[\"title_font\"]\n",
    "    title_para.font.color.rgb = theme.theme[\"title\"]\n",
    "    title_para.alignment = PP_ALIGN.CENTER\n",
    "\n",
    "    # Set subtitle (authors and institutions)\n",
    "    subtitle = title_slide.placeholders[1]\n",
    "    subtitle.text_frame.clear()  # Clear default placeholder text\n",
    "\n",
    "    # Add authors as one paragraph\n",
    "    authors_para = subtitle.text_frame.add_paragraph()\n",
    "    authors_para.text = \", \".join(ppt_data['authors'])\n",
    "    authors_para.font.size = Pt(18)\n",
    "    authors_para.font.name = theme.theme[\"body_font\"]\n",
    "    authors_para.font.color.rgb = theme.theme[\"body\"]\n",
    "    authors_para.alignment = PP_ALIGN.CENTER\n",
    "\n",
    "    # Add institution as a separate paragraph\n",
    "    institution_para = subtitle.text_frame.add_paragraph()\n",
    "    institution_para.text = \"\".join(ppt_data['institution'])\n",
    "    institution_para.font.size = Pt(16)  # Slightly smaller font\n",
    "    institution_para.font.name = theme.theme[\"body_font\"]\n",
    "    institution_para.font.color.rgb = theme.theme[\"body\"]\n",
    "    institution_para.alignment = PP_ALIGN.CENTER\n",
    "\n",
    "    # Ensure the text fits within the shape\n",
    "    subtitle.text_frame.auto_size = MSO_AUTO_SIZE.SHAPE_TO_FIT_TEXT\n",
    "    subtitle.text_frame.word_wrap = True\n",
    "\n",
    "    # Add content slides\n",
    "    for i in range(1, len(ppt_data[\"slides\"])):\n",
    "        slide_data = ppt_data[\"slides\"][i]\n",
    "        title_text = slide_data.get(\"title\", \"\")\n",
    "\n",
    "        # Detect Graphics/Graphs Slide\n",
    "        is_graphics_slide = \"graphics\" in title_text.lower() or \"graphs slide\" in title_text.lower()\n",
    "\n",
    "        # Use a blank layout for Graphics slides\n",
    "        slide_layout = prs.slide_layouts[6] if is_graphics_slide else prs.slide_layouts[1]\n",
    "        slide = prs.slides.add_slide(slide_layout)\n",
    "        apply_background(slide, theme.theme[\"background\"])\n",
    "\n",
    "        if not is_graphics_slide:\n",
    "            title = slide.shapes.title\n",
    "            title.text = title_text\n",
    "            title_para = title.text_frame.paragraphs[0]\n",
    "            title_para.font.size = Pt(32)\n",
    "            title_para.font.name = theme.theme[\"title_font\"]\n",
    "            title_para.font.color.rgb = theme.theme[\"title\"]\n",
    "        # Handling Graphics/Graphs Slide\n",
    "        \n",
    "        if is_graphics_slide and \"images\" in slide_data:\n",
    "            image_filenames = slide_data[\"images\"]\n",
    "            image_paths = [image_mapping.get(fig.replace(\".\", \"\").replace(\" \", \"\")) for fig in image_filenames]\n",
    "            image_paths = [img for img in image_paths if img and os.path.exists(img)]  # Remove missing files\n",
    "    \n",
    "            num_images = len(image_paths)\n",
    "    \n",
    "            # Get theme colors\n",
    "            caption_font = theme.theme[\"body_font\"]\n",
    "            caption_color = theme.theme[\"body\"]\n",
    "    \n",
    "            # Define positioning based on number of images\n",
    "            if num_images == 1:\n",
    "                left, top, width, height = Inches(1.5), Inches(1.5), Inches(7), Inches(5)\n",
    "                img_shape = slide.shapes.add_picture(image_paths[0], left, top, width=width, height=height)\n",
    "                caption_left = left + width / 2 - Inches(0.5)\n",
    "                caption_top = top + height + Inches(0.2)\n",
    "    \n",
    "                # Add caption\n",
    "                caption = slide.shapes.add_textbox(caption_left, caption_top, Inches(1), Inches(0.5))\n",
    "                text_frame = caption.text_frame\n",
    "                text_frame.text = image_filenames[0]\n",
    "                para = text_frame.paragraphs[0]\n",
    "                para.font.size = Pt(14)\n",
    "                para.font.name = caption_font\n",
    "                para.font.color.rgb = caption_color\n",
    "                para.alignment = PP_ALIGN.CENTER\n",
    "    \n",
    "            elif num_images == 2:\n",
    "                positions = [(Inches(1), Inches(2)), (Inches(5.5), Inches(2))]\n",
    "                size = (Inches(4), Inches(3))\n",
    "    \n",
    "                for i, img_path in enumerate(image_paths[:2]):\n",
    "                    img_left, img_top = positions[i]\n",
    "                    img_shape = slide.shapes.add_picture(img_path, img_left, img_top, *size)\n",
    "    \n",
    "                    # Add caption\n",
    "                    caption_left = img_left + size[0] / 2 - Inches(0.5)\n",
    "                    caption_top = img_top + size[1] + Inches(0.2)\n",
    "                    caption = slide.shapes.add_textbox(caption_left, caption_top, Inches(1), Inches(0.5))\n",
    "                    text_frame = caption.text_frame\n",
    "                    text_frame.text = image_filenames[i]\n",
    "                    para = text_frame.paragraphs[0]\n",
    "                    para.font.size = Pt(14)\n",
    "                    para.font.name = caption_font\n",
    "                    para.font.color.rgb = caption_color\n",
    "                    para.alignment = PP_ALIGN.CENTER\n",
    "    \n",
    "            elif num_images >= 3:\n",
    "                positions = [\n",
    "                    (Inches(1), Inches(1.5)), (Inches(5), Inches(1.5)),\n",
    "                    (Inches(3), Inches(4))\n",
    "                ]\n",
    "                size = (Inches(3.5), Inches(2.5))\n",
    "    \n",
    "                for i, img_path in enumerate(image_paths[:3]):\n",
    "                    img_left, img_top = positions[i]\n",
    "                    img_shape = slide.shapes.add_picture(img_path, img_left, img_top, *size)\n",
    "    \n",
    "                    # Add caption\n",
    "                    caption_left = img_left + size[0] / 2 - Inches(0.5)\n",
    "                    caption_top = img_top + size[1] + Inches(0.2)\n",
    "                    caption = slide.shapes.add_textbox(caption_left, caption_top, Inches(1), Inches(0.5))\n",
    "                    text_frame = caption.text_frame\n",
    "                    text_frame.text = image_filenames[i]\n",
    "                    para = text_frame.paragraphs[0]\n",
    "                    para.font.size = Pt(14)\n",
    "                    para.font.name = caption_font\n",
    "                    para.font.color.rgb = caption_color\n",
    "                    para.alignment = PP_ALIGN.CENTER\n",
    "\n",
    "        else: \n",
    "            bullet_points = slide_data.get(\"bullet_points\", [])\n",
    "            content_placeholder = slide.placeholders[1]\n",
    "            text_frame = content_placeholder.text_frame\n",
    "            text_frame.clear()\n",
    "            if bullet_points:\n",
    "                text_frame = content_placeholder.text_frame\n",
    "                text_frame.clear()  # Remove default placeholder text\n",
    "                text_frame.word_wrap = True  # Enable text wrapping\n",
    "                text_frame.auto_size = MSO_AUTO_SIZE.SHAPE_TO_FIT_TEXT  # Enable auto size for content\n",
    "\n",
    "                # Set default font size based on slide type\n",
    "                is_references = \"references\" in slide_data.get(\"title\", \"\").lower()\n",
    "                DEFAULT_FONT_SIZE = 12 if is_references else 20\n",
    "\n",
    "            for point in slide_data['bullet_points']:\n",
    "                paragraph = text_frame.add_paragraph()\n",
    "                paragraph.text = point\n",
    "                paragraph.font.size = Pt(DEFAULT_FONT_SIZE)\n",
    "                paragraph.font.name = theme.theme[\"body_font\"]\n",
    "                paragraph.font.color.rgb = theme.theme[\"body\"]\n",
    "                \n",
    "\n",
    "    # Save PowerPoint file\n",
    "    prs.save(output_file)\n",
    "    print(f\"PowerPoint presentation saved as {output_file}\")\n",
    "\n",
    "create_ppt_from_dict(ppt_content, state_output[\"extracted_images\"], \"modern\", \"hydrocarbon_2.pptx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PowerPoint presentation saved as hydrocarbon_2.pptx\n"
     ]
    }
   ],
   "source": [
    "from pptx import Presentation\n",
    "from pptx.util import Inches\n",
    "import os\n",
    "\n",
    "def create_ppt_from_dict(ppt_data: dict, image_mapping: dict, theme_name: str=\"default\", output_file: str = \"presentation.pptx\"):\n",
    "    prs = Presentation()\n",
    "    theme = ThemeConfig(theme_name)\n",
    "\n",
    "    for slide_data in ppt_data[\"slides\"]:\n",
    "        title_text = slide_data.get(\"title\", \"\")\n",
    "\n",
    "        # Detect Graphics/Graphs Slide\n",
    "        is_graphics_slide = \"graphics\" in title_text.lower() or \"graphs slide\" in title_text.lower()\n",
    "\n",
    "        # Use a blank layout for Graphics slides\n",
    "        slide_layout = prs.slide_layouts[6] if is_graphics_slide else prs.slide_layouts[1]\n",
    "        slide = prs.slides.add_slide(slide_layout)\n",
    "        apply_background(slide, theme.theme[\"background\"])\n",
    "\n",
    "        # Title\n",
    "        if not is_graphics_slide:\n",
    "            title = slide.shapes.title\n",
    "            title.text = title_text\n",
    "\n",
    "        # Handling Graphics/Graphs Slide\n",
    "        if is_graphics_slide and \"images\" in slide_data:\n",
    "            image_filenames = slide_data[\"images\"]\n",
    "            image_paths = [image_mapping.get(fig.replace(\".\", \"\").replace(\" \", \"\")) for fig in image_filenames]\n",
    "            image_paths = [img for img in image_paths if img and os.path.exists(img)]  # Remove missing files\n",
    "\n",
    "            num_images = len(image_paths)\n",
    "\n",
    "            # Define positioning based on number of images\n",
    "            if num_images == 1:\n",
    "                left, top, width, height = Inches(1.5), Inches(1.5), Inches(7), Inches(5)\n",
    "                slide.shapes.add_picture(image_paths[0], left, top, width=width, height=height)\n",
    "\n",
    "            elif num_images == 2:\n",
    "                positions = [(Inches(1), Inches(2)), (Inches(5.5), Inches(2))]\n",
    "                size = (Inches(4), Inches(3))\n",
    "                for i, img_path in enumerate(image_paths[:2]):\n",
    "                    slide.shapes.add_picture(img_path, positions[i][0], positions[i][1], *size)\n",
    "\n",
    "            elif num_images >= 3:\n",
    "                positions = [\n",
    "                    (Inches(1), Inches(1.5)), (Inches(5), Inches(1.5)),\n",
    "                    (Inches(3), Inches(4))\n",
    "                ]\n",
    "                size = (Inches(3.5), Inches(2.5))\n",
    "                for i, img_path in enumerate(image_paths[:3]):\n",
    "                    slide.shapes.add_picture(img_path, positions[i][0], positions[i][1], *size)\n",
    "\n",
    "        else:  # Standard text slide\n",
    "            content = slide.placeholders[1]\n",
    "            bullet_points = slide_data.get(\"bullet_points\", [])\n",
    "            if bullet_points:\n",
    "                text_frame = content.text_frame\n",
    "                text_frame.clear()\n",
    "                for point in bullet_points:\n",
    "                    p = text_frame.add_paragraph()\n",
    "                    p.text = point\n",
    "\n",
    "    prs.save(output_file)\n",
    "    print(f\"PowerPoint presentation saved as {output_file}\")\n",
    "\n",
    "# Example Call\n",
    "create_ppt_from_dict(ppt_content, state_output[\"extracted_images\"], \"modern\", \"hydrocarbon_2.pptx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
