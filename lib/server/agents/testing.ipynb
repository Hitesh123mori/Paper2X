{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "from typing import List, Dict, Any\n",
    "from typing_extensions import TypedDict\n",
    "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from IPython.display import Image, display\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "import os\n",
    "from dotenv import load_dotenv # type: ignore\n",
    "from typing import List, Dict, Any, Optional\n",
    "import fitz # type: ignore\n",
    "from pydantic import BaseModel, Field # type: ignore\n",
    "import regex as re\n",
    "import string\n",
    "from gtts import gTTS\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"LANGSMITH_PROJECT\"] = f\"MineD 2025\"\n",
    "os.environ[\"GOOGLE_API_KEY\"] = f\"AIzaSyBQU_iCwA34u1XbnJcekcNEOkgFb3PHJZM\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash\",\n",
    "    temperature=0.1,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResPaperText(BaseModel):\n",
    "    authors: str = Field(..., description=\"List of authors of the research paper\")\n",
    "    title: str = Field(..., description=\"Title of the research paper\")\n",
    "    submission_date: str = Field(..., description=\"Submission date of the research paper\")\n",
    "    keywords: List[str] = Field(..., description=\"List of keywords associated with the research paper\")\n",
    "    references: List[str] = Field(..., description=\"List of references cited in the research paper\")\n",
    "    abstract: str = Field(..., description=\"Abstract of the research paper\")\n",
    "    conclusion: str = Field(..., description=\"Conclusion of the research paper\")\n",
    "    summary: str = Field(..., description=\"Summary of the research paper\")\n",
    "\n",
    "class SlideContent(BaseModel):\n",
    "    title: str = Field(..., description=\"Title of the particular slide\")\n",
    "    bullet_points: Optional[List[str]] = Field(None, description=\"Content in bullet points form for the slide\")\n",
    "    notes: Optional[str] = Field(None, description=\"Additional notes for the slide\")\n",
    "    images: Optional[List[str]] = Field(None, description=\"List of relevant image paths for the slide\")\n",
    "\n",
    "class PPTPresentation(BaseModel):\n",
    "    title: str = Field(..., description=\"Title of the presentation\")\n",
    "    authors: List[str] = Field(..., description=\"List of authors of the presentation\")\n",
    "    institution: str = Field(..., description=\"Institution associated with the presentation\")\n",
    "    slides: List[SlideContent] = Field(..., description=\"List of slides, in the presentation,which are SlideContent schemas.\")\n",
    "    \n",
    "\n",
    "class Dialogue(BaseModel):\n",
    "    text: str = Field(..., description=\"The text of dialogue\")\n",
    "\n",
    "class Conversation(BaseModel):\n",
    "    katherine: List[Dialogue] = Field(..., description=\"Katherine's dialogues\")\n",
    "    clay: List[Dialogue] = Field(..., description=\"Clay's dialogues\")\n",
    "    order: List[str] = Field(..., description=\"The order of dialogues denoted by the names of the speaker\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResPaperExtractState(TypedDict):\n",
    "    pdf_path: Optional[str] = None  # Path to the PDF file\n",
    "    extracted_text: Optional[str] = None  # Full extracted text from the PDF\n",
    "    extracted_images: Optional[List[str]] = None  # Paths to extracted images\n",
    "    slides_content: Optional[List[Dict[str, str]]] = None  # Prepared content for PowerPoint slides\n",
    "    metadata: str\n",
    "    ppt_object: PPTPresentation\n",
    "    convo: Conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pdf(state: ResPaperExtractState):\n",
    "    pdf_path = state[\"pdf_path\"]\n",
    "    doc = fitz.open(pdf_path)  # Load the PDF only once\n",
    "    \n",
    "    extracted_text = []\n",
    "    extracted_images = []\n",
    "    output_folder = \"extracted_images\"\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Iterate through each page\n",
    "    for page_number, page in enumerate(doc):\n",
    "        # Extract text\n",
    "        text = page.get_text(\"text\")\n",
    "        extracted_text.append(text)\n",
    "\n",
    "        # Extract images\n",
    "        for img_index, img in enumerate(page.get_images(full=True)):\n",
    "            xref = img[0]\n",
    "            base_image = doc.extract_image(xref)\n",
    "            image_bytes = base_image[\"image\"]\n",
    "            image_ext = base_image[\"ext\"]\n",
    "            img_filename = f\"{output_folder}/page_{page_number+1}_img_{img_index+1}.{image_ext}\"\n",
    "            \n",
    "            with open(img_filename, \"wb\") as img_file:\n",
    "                img_file.write(image_bytes)\n",
    "            \n",
    "            extracted_images.append(img_filename)\n",
    "\n",
    "    # Combine text from all pages\n",
    "    full_text = \"\\n\".join(extracted_text)\n",
    "\n",
    "    # Update state\n",
    "    return {\"extracted_text\": full_text, \"extracted_images\": extracted_images}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message_condensation = SystemMessagePromptTemplate.from_template(\n",
    "    \"\"\"You are an expert AI based researcher, your task is to find out key innovations and the overall summary from a given research paper, you should include the brief information about the authors of the paper,\n",
    "    The type of paper, the domain of the research paper,title, author, submission date, summary, literature review, methods used, results, discussion, conclusion and references of the given paper.\n",
    "    \n",
    "    Try to quantify the summary wherever necessary, also include number of result sections in the conclusion\n",
    "    \n",
    "    The summary should contain the simplified summary of the research paper, not the actual abstract, make sure it is lengthy enough to cover all the ideas discussed in the paper, under 2000 words. \n",
    "    \n",
    "    Additional information: \n",
    "        - Dont leave out any ideas from the paper\n",
    "        - Use your knowledge to connects the dots for very hard concepts\n",
    "        - Give a summary in under 2000 words\n",
    "        - Also present any innovative ideas to carry out this work\n",
    "        - Give output in json format\n",
    "    \n",
    "    The format for the extraction is as follows: {format_instructions}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Human Message: Supplies extracted text from the research paper\n",
    "human_message_condensation = HumanMessagePromptTemplate.from_template(\"Here is the extracted text:\\n\\n{extracted_text}\")\n",
    "\n",
    "parser_metadata = JsonOutputParser(pydantic_object=ResPaperText)\n",
    "# Combine into a structured chat prompt\n",
    "chat_prompt_metadata = ChatPromptTemplate(\n",
    "    messages=[system_message_condensation, human_message_condensation],\n",
    "    partial_variables={\"format_instructions\": parser_metadata.get_format_instructions()}\n",
    ")\n",
    "\n",
    "def condense_data(state):\n",
    "    extracted_text = state[\"extracted_text\"]\n",
    "    \n",
    "    prompt = chat_prompt_metadata.invoke({\"extracted_text\":extracted_text})\n",
    "    llm_out = llm.invoke(prompt)\n",
    "    llm_out.content = llm_out.content.replace(\"```json\", \"```\")\n",
    "    parsed = parser_metadata.invoke(llm_out)\n",
    "    \n",
    "    return {\"metadata\": parsed}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message_ppt = SystemMessagePromptTemplate.from_template(\n",
    "    \"\"\"You are an expert in creating PowerPoint presentations. Generate a structured PowerPoint (PPT) presentation \n",
    "    that summarizes a research paper based on the provided extracted text. Follow these instructions:\n",
    "    \n",
    "    Remember that the objective of this PPT is for a third party to understand the key points of the research paper, and \n",
    "    give them a gist of the research paper.\n",
    "\n",
    "    - Title Slide: Include the research paper title, authors, and institution.\n",
    "    - Introduction Slide: Summarize the problem, objectives, and motivation.\n",
    "    - Methods Slide: Briefly explain the methodology, datasets, and experimental setup.\n",
    "    - Results Slide: Summarize key findings with bullet points. Mention any visuals (graphs, tables) found from the extracted text. You should definetly mention in the presentation any figures related to a performance metric or tables that are mentioned in the extracted text.\n",
    "    - Discussion Slide: Explain the significance of results and compare with prior work.\n",
    "    - Conclusion Slide: Summarize key takeaways and potential future work.\n",
    "    - References Slide: Include citations if available.\n",
    "\n",
    "    Additional Guidelines:\n",
    "    - Keep slides concise (use bullet points).\n",
    "    - Maintain a professional and visually appealing slide design.\n",
    "    - Give the text in markdown format.\n",
    "    - Each slide should have rich information content, summarizing the information related to the particular slide heading, \n",
    "    and also include some content that is related to the slide heading but not directly mentioned in the extracted text.\n",
    "    - Also keep in mind that the text for each slide should not be too lengthy, and should be concise and to the point.\n",
    "\n",
    "    {format_instructions}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Human Message: Supplies extracted text from the research paper\n",
    "human_message_ppt = HumanMessagePromptTemplate.from_template(\"Here is the summary of the research paper:\\n\\n{metadata}\")\n",
    "\n",
    "parser_ppt = JsonOutputParser(pydantic_object=PPTPresentation)\n",
    "# Combine into a structured chat prompt\n",
    "chat_prompt_ppt = ChatPromptTemplate(\n",
    "    messages=[system_message_ppt, human_message_ppt],\n",
    "    partial_variables={\"format_instructions\": parser_ppt.get_format_instructions()}\n",
    ")\n",
    "\n",
    "def get_ppt_data(state):\n",
    "    metadata = state[\"metadata\"]\n",
    "    prompt = chat_prompt_ppt.invoke({\"metadata\": metadata, \"tone\": \"formal\"})\n",
    "    llm_out = llm.invoke(prompt)\n",
    "    parsed = parser_ppt.invoke(llm_out)\n",
    "    return {\"ppt_object\": parsed}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message_podcast = SystemMessagePromptTemplate.from_template(\n",
    "    \"\"\"You are an expert in creating/writing scripts for podcast, consider the given scenario, Two people one girl and one boy who are completing their B.Tech degree this year are discussing the given research paper to create an podcast of this research paper\n",
    "    \n",
    "    Boy's Name: Clay\n",
    "    Girl's Name: Katherine\n",
    "    \n",
    "    The Girl has complete knowledge about this paper, while the boy doesn't know anything about the paper.\n",
    "    \n",
    "    Write a script for a podcast, wherein firstly the girl introduces the paper, but the boy seems clueless, so the boy ask the girl many questions about the paper.\n",
    "    \n",
    "    The boy's question should cover all the possible doubt that one can have regarding the paper, and the girl should answer that questions correctly.\n",
    "\n",
    "    General Guideline:\n",
    "    - Intro must include the name, application and the authors (and their institution)\n",
    "    - Consider the audience to be technically sound, so you can ue jargons\n",
    "    - The boys questions should cover all the aspects from methodology, results, literature review, etc\n",
    "    - Dont make it too obvious that they are discussing about the paper\n",
    "    - Make the order such that the question asked by clay in previous dialogue is answered by katherine in this dialogue.\n",
    "\n",
    "    Additional Guidelines:\n",
    "    - Output in JSON format, this JSON should have two keys, names of boys and girls, in lower case.\n",
    "    - Each key corresponds to a list, their dialogues in sequential manner\n",
    "    - Consider that the girl always starts first\n",
    "    - Also give the order of dialogues, that are to be taken in a sequence\n",
    "    - Make sure that the number of dialogues in the order and in the lists add up.\n",
    "    - Both of them dont have to speak alternatively, they can heave continuous dialogues\n",
    "    - Each and every question asked by clay has to be answered by katherine\n",
    "\n",
    "    {format_instructions}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Human Message: Supplies extracted text from the research paper\n",
    "human_message_podcast = HumanMessagePromptTemplate.from_template(\"Here is the summary of research paper:\\n\\n{metadata}. \\nMake sure the tone is {tone}\")\n",
    "\n",
    "parser_podcast = JsonOutputParser(pydantic_object=Conversation)\n",
    "# Combine into a structured chat prompt\n",
    "chat_prompt_podcast = ChatPromptTemplate(\n",
    "    messages=[system_message_podcast, human_message_podcast],\n",
    "    partial_variables={\"format_instructions\": parser_podcast.get_format_instructions()}\n",
    ")\n",
    "\n",
    "def get_data_podcast(state):\n",
    "    metadata = state[\"metadata\"]\n",
    "    prompt = chat_prompt_podcast.invoke({\"metadata\": metadata, \"tone\": \"informative\"})\n",
    "    llm_out = llm.invoke(prompt)\n",
    "    parsed = parser_podcast.invoke(llm_out)\n",
    "    \n",
    "    return {\"convo\": parsed}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = StateGraph(ResPaperExtractState)\n",
    "\n",
    "builder.add_node(\"pdf-2-text\", load_pdf)\n",
    "builder.add_node(\"text-condensation\", condense_data)\n",
    "# builder.add_node(\"make-ppt-text\", get_ppt_data)\n",
    "builder.add_node(\"make-podcast-text\", get_data_podcast)\n",
    "\n",
    "builder.add_edge(START, \"pdf-2-text\")\n",
    "builder.add_edge(\"pdf-2-text\", \"text-condensation\")\n",
    "# builder.add_edge(\"text-condensation\", \"make-ppt-text\")\n",
    "builder.add_edge(\"text-condensation\", \"make-podcast-text\")\n",
    "builder.add_edge(\"make-podcast-text\", END)\n",
    "# builder.add_edge( \"make-ppt-text\", END)\n",
    "\n",
    "graph = builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"C:\\\\Users\\\\Mihir Patel\\\\Downloads\\\\1706.03762v7.pdf\"\n",
    "state_output = graph.invoke({\"pdf_path\": path})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The paper introduces the Transformer, a novel neural network architecture based solely on attention mechanisms, eliminating recurrence and convolutions.  Evaluated on machine translation tasks, the Transformer surpasses existing models in quality, parallelization, and training speed, achieving state-of-the-art BLEU scores on WMT 2014 English-to-German and English-to-French translation tasks.  Its effectiveness extends to other tasks like English constituency parsing.\n",
      "This research paper introduces the Transformer, a groundbreaking neural network architecture designed for sequence transduction tasks like machine translation.  Unlike dominant models that rely on recurrent or convolutional neural networks (RNNs or CNNs), the Transformer uses only attention mechanisms. This key innovation allows for significantly greater parallelization during training, leading to faster training times and improved performance.  \n",
      "\n",
      "The authors, a team of researchers primarily from Google Brain and Google Research, including Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin, detail the architecture of the Transformer, which consists of an encoder and a decoder, both built using stacked layers of self-attention and point-wise fully connected feed-forward networks.  The self-attention mechanism allows the model to weigh the importance of different parts of the input sequence when generating the output, capturing long-range dependencies more effectively than RNNs or CNNs.  A crucial component is the scaled dot-product attention, which addresses the problem of vanishing gradients in dot-product attention by scaling the dot products.  Multi-head attention further enhances the model's ability to attend to information from different representation subspaces.  Positional encoding is incorporated to provide information about the order of the sequence, as the model lacks recurrence.\n",
      "\n",
      "The paper presents extensive experimental results on two machine translation tasks: WMT 2014 English-to-German and WMT 2014 English-to-French.  The Transformer achieves state-of-the-art results, significantly outperforming existing models, including ensembles, by over 2 BLEU points on the English-to-German task (achieving 28.4 BLEU) and establishing a new single-model state-of-the-art on the English-to-French task (41.8 BLEU).  Importantly, these results are achieved with significantly less training time.  The authors also demonstrate the Transformer's generalizability by applying it to English constituency parsing, where it achieves competitive results, even with limited training data.\n",
      "\n",
      "The paper provides a detailed comparison of self-attention with recurrent and convolutional layers, highlighting self-attention's advantages in terms of computational complexity, parallelization, and path length for learning long-range dependencies.  The authors also explore various model variations, analyzing the impact of different architectural choices on performance.  The training process is described in detail, including data preparation, hardware used, optimization techniques (Adam optimizer with a specific learning rate schedule), and regularization methods (dropout and label smoothing).  The paper concludes by discussing future research directions, such as extending the Transformer to other modalities and exploring local attention mechanisms for efficient handling of large inputs and outputs.\n"
     ]
    }
   ],
   "source": [
    "print(state_output[\"metadata\"]['abstract'])\n",
    "print(state_output[\"metadata\"]['summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "7\n",
      "['katherine', 'clay', 'katherine', 'clay', 'katherine', 'clay', 'katherine', 'clay', 'katherine', 'clay', 'katherine', 'clay', 'katherine']\n"
     ]
    }
   ],
   "source": [
    "convo = state_output[\"convo\"]\n",
    "print(convo['katherine'].__len__())\n",
    "print(convo['clay'].__len__())\n",
    "print(convo['order'])\n",
    "\n",
    "kat_index = 0\n",
    "clay_index = 0\n",
    "\n",
    "dialogues = []\n",
    "\n",
    "for speaker in convo['order']:\n",
    "    try:\n",
    "        dialogue = None\n",
    "        if speaker == 'katherine':\n",
    "            dialogue = convo['katherine'][kat_index]\n",
    "            kat_index += 1\n",
    "        else:\n",
    "            dialogue = convo['clay'][kat_index]\n",
    "            clay_index += 1\n",
    "        dialogues.append({speaker: dialogue['text']})\n",
    "    except:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'katherine': \"Hey Clay, ready to dive into this week's podcast topic?  It's a pretty groundbreaking paper, 'Attention is All You Need,' by Vaswani et al. from Google Brain and Google Research. It's all about a new neural network architecture called the Transformer, designed for sequence transduction tasks like machine translation.\"}, {'clay': \"Okay, so it's all about attention. But how exactly does this 'scaled dot-product attention' and 'multi-head attention' work?  Can you break that down for me?\"}, {'katherine': \"The core idea is revolutionary: they've completely ditched recurrence and convolutions, relying solely on attention mechanisms. This allows for incredible parallelization during training, leading to much faster training times compared to traditional RNN or CNN-based models.\"}, {'clay': \"You mentioned state-of-the-art results.  What specific metrics did they use to evaluate the model's performance?  And how significant were these improvements compared to previous best results?\"}, {'katherine': 'The Transformer uses a novel self-attention mechanism, specifically scaled dot-product attention and multi-head attention.  This lets the model weigh the importance of different parts of the input sequence when generating the output, capturing long-range dependencies much more effectively than RNNs.'}, {'clay': 'The encoder-decoder structure sounds familiar.  How does this differ from the encoder-decoder architectures used in previous sequence-to-sequence models?'}, {'katherine': 'Their results are stunning. They achieved state-of-the-art results on the WMT 2014 English-to-German and English-to-French translation tasks, significantly outperforming existing models in both quality and speed.  They also showed promising results on English constituency parsing.'}, {'clay': \"You mentioned parallelization.  Can you explain how the Transformer's architecture enables this parallel processing, and what are the implications for training time and efficiency?\"}, {'katherine': \"The architecture itself is quite elegant. It's based on an encoder-decoder structure, with each layer containing multi-head self-attention and a position-wise feed-forward network.  Residual connections and layer normalization are used to stabilize training.\"}, {'clay': 'This paper seems to cover a lot of ground.  What were some of the key challenges they faced during the research, and how did they address them?'}, {'katherine': \"They also discuss the computational advantages of self-attention over recurrence and convolutions, showing how it reduces computational complexity and path length for learning long-range dependencies.  The paper is very thorough, covering everything from the architecture's details to the training process and hyperparameter choices.\"}, {'clay': 'What are the limitations of this Transformer architecture, and what are some of the potential future research directions mentioned in the paper?'}, {'katherine': 'The authors suggest future work could focus on extending the Transformer to other modalities and exploring more efficient attention mechanisms for handling extremely long sequences.'}]\n"
     ]
    }
   ],
   "source": [
    "print(dialogues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mined_2025-9loJUqGE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
