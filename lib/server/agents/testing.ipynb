{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "from typing import List, Dict, Any\n",
    "from typing_extensions import TypedDict\n",
    "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from IPython.display import Image, display\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "# from langchain.document_loaders import PyMuPDFLoader\n",
    "from typing import List, Dict, Any, Optional\n",
    "import fitz\n",
    "from pydantic import BaseModel, Field\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"LANGSMITH_PROJECT\"] = f\"MineD 2025\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    # other params...\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_huggingface import HuggingFaceEndpoint\n",
    "\n",
    "# llm = HuggingFaceEndpoint(\n",
    "#     # repo_id=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
    "#     repo_id=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "#     task=\"text-generation\", \n",
    "#     do_sample=False,\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm = ChatGroq(model=\"gemma2-9b-it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "#base model to hold the metadata, and slide summeries that the llm will extract\n",
    "class ResPaperText(BaseModel):\n",
    "    # authors: str = Field(..., description=\"List of authors of the research paper\")\n",
    "    # title: str = Field(..., description=\"Title of the research paper\")\n",
    "    # submission_date: str = Field(..., description=\"Submission date of the research paper\")\n",
    "    # keywords: List[str] = Field(..., description=\"List of keywords associated with the research paper\")\n",
    "    # references: List[str] = Field(..., description=\"List of references cited in the research paper\")\n",
    "    # abstract: str = Field(..., description=\"Abstract of the research paper\")\n",
    "    conclusion: str = Field(..., description=\"Conclusion of the research paper\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Pydantic Model for PPT slides\n",
    "class SlideContent(BaseModel):\n",
    "    title: str = Field(..., description=\"Title of the particular slide\")\n",
    "    bullet_points: Optional[List[str]] = Field(None, description=\"Content in bullet points form for the slide\")\n",
    "    notes: Optional[str] = Field(None, description=\"Additional notes for the slide\")\n",
    "    images: Optional[List[str]] = Field(None, description=\"List of relevant image paths for the slide\")\n",
    "\n",
    "class PPTPresentation(BaseModel):\n",
    "    title: str = Field(..., description=\"Title of the presentation\")\n",
    "    authors: List[str] = Field(..., description=\"List of authors of the presentation\")\n",
    "    institution: str = Field(..., description=\"Institution associated with the presentation\")\n",
    "    slides: List[SlideContent] = Field(..., description=\"List of slides, in the presentation,which are SlideContent schemas.\")\n",
    "\n",
    "class Dialogue(BaseModel):\n",
    "    text: str = Field(..., description=\"The text of dialogue\")\n",
    "\n",
    "class Conversation(BaseModel):\n",
    "    katherine: List[Dialogue] = Field(..., description=\"Katherine's dialogues\")\n",
    "    clay: List[Dialogue] = Field(..., description=\"Clay's dialogues\")\n",
    "    order: List[str] = Field(..., description=\"The order of dialogues denoted by the names of the speaker\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResPaperExtractState(TypedDict):\n",
    "    pdf_path: Optional[str] = None  # Path to the PDF file\n",
    "    extracted_text: Optional[str] = None  # Full extracted text from the PDF\n",
    "    extracted_images: Optional[Dict[str,str]] = None  # Paths to extracted images\n",
    "    slides_content: Optional[List[Dict[str, str]]] = None  # Prepared content for PowerPoint slides\n",
    "    ppt_object: PPTPresentation\n",
    "    summary_text: str\n",
    "    convo: Conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pdf(state: ResPaperExtractState):\n",
    "    pdf_path = state[\"pdf_path\"]\n",
    "    doc = fitz.open(pdf_path)  # Load the PDF only once\n",
    "    \n",
    "    extracted_text = []\n",
    "    extracted_images = dict()\n",
    "    output_folder = \"extracted_images\"\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Iterate through each page\n",
    "    img_cntr=1\n",
    "    for page_number, page in enumerate(doc):\n",
    "        # Extract text\n",
    "        text = page.get_text(\"text\")\n",
    "        extracted_text.append(text)\n",
    "\n",
    "        # Extract images\n",
    "        for img_index, img in enumerate(page.get_images(full=True)):\n",
    "            xref = img[0]\n",
    "            base_image = doc.extract_image(xref)\n",
    "            image_bytes = base_image[\"image\"]\n",
    "            image_ext = base_image[\"ext\"]\n",
    "            img_filename = f\"{output_folder}/page_{page_number+1}_img_{img_index+1}.{image_ext}\"\n",
    "            \n",
    "            with open(img_filename, \"wb\") as img_file:\n",
    "                img_file.write(image_bytes)\n",
    "            \n",
    "            extracted_images[f\"Fig{img_cntr}\"] = img_filename\n",
    "            img_cntr+=1\n",
    "\n",
    "    # Combine text from all pages\n",
    "    full_text = \"\\n\".join(extracted_text)\n",
    "\n",
    "    # Update state\n",
    "    return {\"extracted_text\": full_text, \"extracted_images\": extracted_images}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = SystemMessagePromptTemplate.from_template(\n",
    "    \"\"\"You are an expert in creating PowerPoint presentations. Generate a structured PowerPoint (PPT) presentation \n",
    "    that summarizes a research paper based on the provided extracted text. Follow these instructions:\n",
    "    \n",
    "    Remember that the objective of this PPT is for a third party to understand the key points of the research paper, and \n",
    "    give them a gist of the research paper.\n",
    "\n",
    "    - Title Slide: Include the research paper title, authors, and institution.\n",
    "    - Introduction Slide: Summarize the problem, objectives, and motivation.\n",
    "    - Methods Slide: Briefly explain the methodology, datasets, and experimental setup.\n",
    "    - Results Slide: Summarize key findings with bullet points. Mention any visuals (graphs, tables) found from the extracted text. You should definetly mention in the presentation any figures related to a performance metric or tables that are mentioned in the extracted text.\n",
    "    - Graphics: Include any images of graphs or charts or other images, relevant to the results,or images depicting a performance metric,\n",
    "      that are mentioned in the extracted text. You can find such images by looking for any captions that mention figures or tables. \n",
    "      It is necessary to name this slide as Graphics.\n",
    "      Note that you should only mention the image number, like Fig1, Fig2, etc...\n",
    "      Include only relevant image names.\n",
    "    - Discussion Slide: Explain the significance of results and compare with prior work.\n",
    "    - Conclusion Slide: Summarize key takeaways and potential future work.\n",
    "    - References Slide: Include citations if available.\n",
    "\n",
    "    Additional Guidelines:\n",
    "    - Keep slides concise (use bullet points).\n",
    "    - Maintain a professional and visually appealing slide design.\n",
    "    - Give the text in markdown format.\n",
    "    - Each slide should have rich information content, summarizing the information related to the particular slide heading, \n",
    "    and also include some content that is related to the slide heading but not directly mentioned in the extracted text.\n",
    "    - Also keep in mind that the text for each slide should not be too lengthy, and should be concise and to the point.\n",
    "\n",
    "    {format_instructions}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Human Message: Supplies extracted text from the research paper\n",
    "human_message = HumanMessagePromptTemplate.from_template(\"Here is the extracted text:\\n\\n{extracted_text}\")\n",
    "\n",
    "parser = JsonOutputParser(pydantic_object=PPTPresentation)\n",
    "# Combine into a structured chat prompt\n",
    "chat_prompt = ChatPromptTemplate(\n",
    "    messages=[system_message, human_message],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "def get_data(state):\n",
    "    extracted_text = state[\"extracted_text\"]\n",
    "    \n",
    "    # Format prompt with extracted text\n",
    "    \n",
    "    # Invoke LLM with structured output\n",
    "    chain = chat_prompt | llm | parser\n",
    "\n",
    "    # Parse structured output into Pydantic model\n",
    "    ppt_object = chain.invoke({\"extracted_text\":extracted_text})\n",
    "    \n",
    "    return {\"ppt_object\": ppt_object}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary(state: ResPaperExtractState):\n",
    "    extracted_text = state[\"extracted_text\"]\n",
    "    \n",
    "    summary_template_string = \"\"\"\n",
    "You are an expert science communicator who specializes in breaking down complex research papers into engaging, conversational summaries. Your goal is to generate a summary that serves the purposes: \n",
    "\n",
    "**Conversational Podcast** – The summary should be structured in a way that makes it engaging for a podcast discussion. \n",
    "Imagine two hosts discussing the paper in an insightful yet accessible way. \n",
    "Include thought-provoking questions and key discussion points that make the findings compelling to a general audience.\n",
    "\n",
    "### **Instructions:**\n",
    "- Start with an **intriguing hook** that captures the essence of the paper in an engaging way. Example: *\"What if we could predict network congestion before it even happens? A recent study explores exactly that in the context of V2X communication.\"*\n",
    "- Clearly state the **research problem** and why it matters.\n",
    "- Summarize the **key findings** and their implications, but in a way that sparks curiosity.\n",
    "- **Use an engaging tone** that makes it feel like a conversation rather than a dry summary.\n",
    "- Include at least **three discussion-worthy questions** that podcast hosts could debate.\n",
    "- Highlight any **visual elements** that could be useful for a graphical abstract, such as relationships between variables, experimental results, or unexpected insights.\n",
    "\n",
    "### **Important Guidelines:**\n",
    "- Keep it insightful yet engaging—avoid overly technical jargon unless necessary.  \n",
    "- Don’t make the summary too short; ensure all important elements of the research are covered.  \n",
    "- Aim for a summary length of **300-500 words** to balance depth with readability.  \n",
    "- If applicable, include **real-world analogies** or examples to make the findings more relatable. \n",
    "- Remember, the goal is to make the research accessible and interesting to a broad audience.\n",
    "- Return a single string with the summary text, acheiving the above objectives. \n",
    "\n",
    "Now, using these guidelines, generate a well-structured summary of the following research paper: {text}  \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    summary_template_string_2 = \"\"\"\n",
    "You are an expert science communicator who specializes in breaking down complex research papers into engaging, conversational summaries. Your goal is to generate a summary that will be used to generate text for conversational podcast.\n",
    "The summary should be structured in a way that makes it engaging for a podcast discussion. \n",
    "Include thought-provoking questions and key discussion points that make the findings compelling to a general audience.\n",
    "\n",
    "### **Instructions:**\n",
    "- Start with an **intriguing hook** that captures the essence of the paper in an engaging way. \n",
    "- Clearly state the **research problem** and why it matters.\n",
    "- Summarize the **key findings** and their implications, but in a way that sparks curiosity.\n",
    "- **Use an engaging tone** that makes it feel like a conversation rather than a dry summary.\n",
    "- Include at least **three discussion-worthy questions** that podcast hosts could debate.\n",
    "- Highlight any **visual elements** that could be useful for a graphical abstract, such as relationships between variables, experimental results, or unexpected insights.\n",
    "\n",
    "### **Important Guidelines:**\n",
    "- Keep it insightful yet engaging—avoid overly technical jargon unless necessary.  \n",
    "- Don’t make the summary too short; ensure all important elements of the research are covered.  \n",
    "- Aim for a summary length of **300-500 words** to balance depth with readability.  \n",
    "- If applicable, include **real-world analogies** or examples to make the findings more relatable. \n",
    "- Remember, the goal is to make the research accessible and interesting to a broad audience.\n",
    "- Return a single string with the summary text, acheiving the above objectives. \n",
    "Now, using these guidelines, generate a well-structured summary of the following research paper: {text}  \n",
    "\n",
    "\"\"\"\n",
    "    summary_prompt = PromptTemplate.from_template(summary_template_string_2)\n",
    "    # Generate summary with LLM\n",
    "    summary_text = llm.invoke(summary_prompt.format(text=extracted_text))  # No chunking, single LLM call\n",
    "    \n",
    "    return {\"summary_text\": summary_text}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message_podcast = SystemMessagePromptTemplate.from_template(\n",
    "    \"\"\"You are an expert in creating/writing scripts for podcasts. \n",
    "    Consider the given scenario: Two people one girl and one boy who are in final year of their B.Tech, are discussing the given research paper to create an podcast of this research paper\n",
    "    \n",
    "    Boy's Name: Clay\n",
    "    Girl's Name: Katherine\n",
    "    \n",
    "    The Girl has complete knowledge about this paper, while the boy doesn't know anything about the paper.\n",
    "    \n",
    "    Write a script for a podcast, wherein firstly the girl introduces the paper, but the boy seems clueless.So the boy ask the girl many questions about the paper, to understand the paper and learn more about the keyowrds and topics involved.\n",
    "    \n",
    "    The boy's question should cover all the possible doubt that one can have regarding the paper, and the girl should answer that questions correctly.\n",
    "\n",
    "    General Guideline:\n",
    "    - Intro must include the name, application and the authors (and their institution)\n",
    "    - Consider the audience to be technically sound, so you can ue jargons\n",
    "    - The boys questions should cover all the aspects from methodology, results, literature review, etc\n",
    "    - Dont make it too obvious that they are discussing about the paper\n",
    "    - Make the order such that the question asked by clay in previous dialogue is answered by katherine in this dialogue.\n",
    "\n",
    "    Additional Guidelines:\n",
    "    - Output in JSON format, this JSON should have two keys, names of boys and girls, in lower case.\n",
    "    - Each key corresponds to a list, their dialogues in sequential manner\n",
    "    - Consider that the girl always starts first\n",
    "    - Also give the order of dialogues, that are to be taken in a sequence\n",
    "    - Make sure that the number of dialogues in the order and in the lists add up.\n",
    "    - Both of them dont have to speak alternatively, they can heave continuous dialogues\n",
    "    - Each and every question asked by clay has to be answered by katherine\n",
    "\n",
    "    {format_instructions}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Human Message: Supplies extracted text from the research paper\n",
    "human_message_podcast = HumanMessagePromptTemplate.from_template(\"Here is the summary of research paper:\\n\\n{summary_text}. \\nMake sure the tone is {tone}\")\n",
    "\n",
    "parser_podcast = JsonOutputParser(pydantic_object=Conversation)\n",
    "# Combine into a structured chat prompt\n",
    "chat_prompt_podcast = ChatPromptTemplate(\n",
    "    messages=[system_message_podcast, human_message_podcast],\n",
    "    partial_variables={\"format_instructions\": parser_podcast.get_format_instructions()}\n",
    ")\n",
    "\n",
    "def generate_conversation(state: ResPaperExtractState):\n",
    "    summary_text = state[\"summary_text\"]\n",
    "    prompt = chat_prompt_podcast.invoke({\"summary_text\": summary_text, \"tone\": \"informative\"})\n",
    "    llm_out = llm.invoke(prompt)\n",
    "    parsed = parser_podcast.invoke(llm_out)\n",
    "    \n",
    "    return {\"convo\":parsed}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# builder = StateGraph(ResPaperExtractState)\n",
    "\n",
    "# builder.add_node(\"pdf-2-text\", load_pdf)\n",
    "# builder.add_node(\"text-condensation\", get_data)\n",
    "\n",
    "# builder.add_edge(START, \"pdf-2-text\")\n",
    "# builder.add_edge(\"pdf-2-text\", \"text-condensation\")\n",
    "# builder.add_edge(\"text-condensation\", END)\n",
    "\n",
    "# graph = builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "builder = StateGraph(ResPaperExtractState)\n",
    "\n",
    "builder.add_node(\"pdf-2-text\", load_pdf)\n",
    "builder.add_node(\"ppt-extract\", get_data)\n",
    "builder.add_node(\"summary-text\", generate_summary)\n",
    "builder.add_node(\"conversation\", generate_conversation)\n",
    "\n",
    "builder.add_edge(START, \"pdf-2-text\")\n",
    "builder.add_edge(\"pdf-2-text\", \"ppt-extract\")\n",
    "builder.add_edge(\"pdf-2-text\", \"summary-text\")\n",
    "builder.add_edge(\"summary-text\", \"conversation\")\n",
    "builder.add_edge(\"ppt-extract\", END)\n",
    "builder.add_edge(\"conversation\", END)\n",
    "graph = builder.compile()\n",
    "\n",
    "display(Image(graph.get_graph(xray=1).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "path1 = r\"C:\\Users\\milap\\OneDrive\\Desktop\\CLG\\3rd YR\\SEM VI\\mined_2025\\lib\\server\\Milap_Tathya_ICC_June_2025.pdf\"\n",
    "path2 = r\"C:\\Users\\milap\\OneDrive\\Desktop\\CLG\\3rd YR\\SEM VI\\mined_2025\\lib\\server\\STORM.pdf\"\n",
    "path3 = r\"C:\\Users\\milap\\OneDrive\\Desktop\\CLG\\3rd YR\\SEM VI\\mined_2025\\lib\\server\\SuFIA.pdf\"\n",
    "path4 = r\"C:\\Users\\milap\\OneDrive\\Desktop\\CLG\\3rd YR\\SEM VI\\mined_2025\\lib\\server\\ankit review.pdf\"\n",
    "state_output = graph.invoke({\"pdf_path\":path3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SUFIA: Language-Guided Augmented Dexterity\\nfor Robotic Surgical Assistants\\nMasoud Moghani1, Lars Doorenbos2, William Chung-Ho Panitch3,\\nSean Huver4, Mahdi Azizian4, Ken Goldberg3, Animesh Garg1,4,5\\nAbstract— In this work, we present SUFIA, the first frame-\\nwork for natural language-guided augmented dexterity for\\nrobotic surgical assistants. SUFIA incorporates the strong\\nreasoning capabilities of large language models (LLMs) with\\nperception modules to implement high-level planning and low-\\nlevel control of a robot for surgical sub-task execution. This\\nenables a learning-free approach to surgical augmented dexterity\\nwithout any in-context examples or motion primitives. SUFIA\\nuses a human-in-the-loop paradigm by restoring control to\\nthe surgeon in the case of insufficient information, mitigating\\nunexpected errors for mission-critical tasks. We evaluate SUFIA\\non four surgical sub-tasks in a simulation environment and two\\nsub-tasks on a physical surgical robotic platform in the lab,\\ndemonstrating its ability to perform common surgical sub-tasks\\nthrough supervised autonomous operation under challenging\\nphysical and workspace conditions.\\nProject website: orbit-surgical.github.io/sufia\\nI. INTRODUCTION\\nRecently, one prominent trend in surgery has been the\\nincreasing adoption of robotic surgical assistants (RSAs) in\\noperating rooms. These RSAs are often controlled via local\\nor remote teleoperation through a console by a trained human\\nsurgeon using hand controllers or other input peripherals,\\nthereby enabling the surgeon to perform tasks with enhanced\\nprecision, dexterity, and control during an operation [1]. The\\nteleoperated surgical procedures often involve tedious, repeti-\\ntive, or time-consuming sub-tasks. Augmented dexterity in\\nsurgery holds the potential to simplify the surgical workflow,\\nreduce surgeon fatigue, and improve patient outcomes [2],\\n[3].\\nLearning-based approaches such as reinforcement and\\nimitation learning learn policies to solve specific surgical sub-\\ntasks [4], [5]. However, complex, long-horizon surgical sub-\\ntasks are often computationally expensive, require extensive\\ndomain knowledge and reward engineering, and involve\\ntime-consuming dataset curation. Furthermore, the lack of\\ngeneralizability limits the utility of learning-based models in\\nsafety-critical applications where unseen, in-domain variations\\nare prevalent. As a result, most surgical robotic platforms\\nstill lack any level of autonomous capabilities [6].\\nIn recent years, Large Language Models (LLMs) have\\nreceived considerable attention for their ability to respond\\nnaturally to textual prompts and have been integrated into var-\\nious domains, including the field of robotics and autonomous\\nagents [7]. Language and vision models have demonstrated\\nconsiderable promise in long-horizon robot planning and\\n1University of Toronto, 2University of Bern, 3University of California,\\nBerkeley, 4NVIDIA, 5Georgia Institute of Technology\\nmoghani@cs.toronto.edu, animesh.garg@gatech.edu\\nHigh-level\\nPlanning\\nLow-level\\nCode\\nFeedback\\nLLM Generation\\nRobot\\nAction\\nPerception\\nSurgeon: \\n“Please lift the needle”\\nAssistant:\\n“The needle has been successfully \\nlifted to the correct position”\\nRe-planning\\nAutonomy \\nDelegation\\nneedle\\nFig. 1: An overview of SUFIA automating the lifting of a\\nsuture needle from a surgical site. SUFIA receives commands\\nfrom a surgeon in natural language and converts them to high-\\nlevel planning and low-level control code. If a task requires object\\ninteraction, SUFIA queries a perception module for object state\\ninformation and generates low-level trajectories and robot actions\\naccordingly. SUFIA can assist a surgeon with open-ended tasks,\\nsuch as moving the robot in a desired motion to help complete a\\nsurgical task. In times of inefficient information, SUFIA delegates\\nfull control back to the surgeon.\\ncontrol [8], [9], [10]. While these efforts still require pre-\\ntrained skills and motion primitives, they have demonstrated\\nthe potential of unified many-modality models for addressing\\na variety of complex tasks involving improved generalization\\nto novel objects and unseen tasks.\\nIn surgical settings, LLMs have the additional potential\\nto aid interaction between a human surgeon and a robot via\\nnatural language teleoperation. This empowers the surgeon\\nwith the ability to use both fine-grained manual control\\nand autonomous natural language conversational control in\\ncommanding the RSA to perform a sub-task. This approach\\npromises both more natural human-robot coordination and\\nthe potential for developing general-purpose models for\\nautonomous surgery beyond the capability of current task-by-\\narXiv:2405.05226v1  [cs.RO]  8 May 2024\\n\\ntask automation approaches.\\nIn this work, we present SUFIA (Surgical First Interactive\\nAutonomy Assistant), a framework for natural interaction\\nbetween a human surgeon and a surgical robot to provide\\ninteractive surgical autonomy. As shown in Fig. 1, SUFIA\\ntakes in sub-task commands from a surgeon and outputs\\na high-level natural language task plan, as well as low-\\nlevel Python code snippets for execution, if requested. A\\nperception module grounds perceived surgical objects in the\\nscene regardless of variations in shape, size, and pose and\\naccounts for the characteristics of their often small, slender\\nshapes. SUFIA also incorporates re-planning and human-in-\\nthe-loop control as safety measures. Our primary contributions\\nare as follows:\\n• A general formulation for natural language interaction\\nbetween a surgeon and a robot.\\n• A language-based control approach to facilitate surgical\\nsub-task implementations.\\n• A systematic evaluation of the generalization of our\\napproach to various surgical sub-tasks, showing its per-\\nformance and robustness for challenging workspace condi-\\ntions.\\nII. RELATED WORK\\nA. Large Language Models for Robotics\\nLarge Language Models (LLMs) are state-of-the-art nat-\\nural language processing systems built on the transformer\\narchitecture [11]. LLMs are pre-trained with self-supervised\\nobjectives on vast amounts of text corpora, enabling these\\nmodels to exhibit impressive language understanding and\\ngeneration capabilities and perform a wide range of tasks.\\nThey are typically further fine-tuned with labeled data and\\nRLHF to create general-purpose assistants [12], [13] or more\\nspecialized models for use cases such as coding [14], [15]\\nor report generation [16].\\nIn robotics, LLMs have been recently employed to address\\nthe high-level planning aspect of robotic control [17], [18].\\nThese models still require trajectory generators through cost\\nor reward functions to compute the trajectory. Other works\\nfocused on leveraging LLMs to design reward functions [19],\\n[20] to acquire complex skills via reinforcement learning.\\nHowever, most of these research works perform well on\\npredefined tasks and still require expensive training time\\nto generalize. Building on a recent work [21] that revealed\\nthe potential of LLMs to directly reason trajectory paths for\\nrobot arms, SUFIA incorporates LLMs to directly control\\nthe gripper poses to perform surgical sub-tasks. This enables\\nthe surgeon to naturally interact with the robot by asking\\nfor a complete task (e.g., pick the needle, insert the soft\\ntube) or an open-ended task (e.g., move the needle in semi-\\ncircular motion) to help complete a sub-task. Our work\\ndiffers from [21] in that we do not rely on a separate object\\ndetector for validation, incorporate further safety mechanisms\\nby delegation, and show results for surgical scenes, where we\\nadditionally study domain-relevant axes such as variations in\\nneedle shape.\\nB. Surgical Augmented Dexterity\\nAugmented dexterity has been attempted for several sub-\\ntasks with varying levels of autonomy [22], [23], [6] such as\\ndexterous needle picking and handling [24], [3], suturing [2],\\n[25], and tissue manipulation [26], [27], [28]. In particular,\\nin contrast to full automation, an often-explored paradigm\\nin surgical robotics is augmented dexterity [29], in which\\nminimal surgical sub-tasks are automated under human\\nsupervision, enabling more precise actuation with less effort\\nexpended.\\nHowever, these works largely rely on access to expensive,\\ntask-specific surgical hardware and software. In order to\\nenable wider exploration of robotic automation in surgery,\\nprior work has often focused on reducing the hardware barrier\\nby adapting traditional robotic arm geometries for medical\\nsub-tasks [30], [31], or designing novel, lower-cost, multi-\\npurpose medical robotic systems [32], [33], [34]. Additionally,\\nlearning robust perception and control models for surgical\\ntasks often requires gathering very large and expensive in-vivo\\ndatasets to avoid safety-critical failure cases [35], [36]. In\\nthis work, we propose an alternate approach to this software\\nbarrier by relying on a general-purpose, natural language-\\nguided framework for surgical augmented dexterity across\\nmultiple tasks.\\nIII. PROBLEM FORMULATION\\nWe focus on a novel approach to surgical augmented\\ndexterity. In contrast to previous methods, we are investigating\\nthe potential of a generalist framework using large language\\nmodels to address surgical augmented dexterity rather than\\ntraining individual models for isolated tasks. We now briefly\\ndetail the assumptions with respect to the environment and\\navailable tools in our work. We do not provide any policies,\\ntrajectory optimizers, or in-context examples to the LLM [21].\\nInstead, we expect the LLM to reason over automating a\\nbenchmark simulated surgical sub-task with their internal\\nknowledge and access to limited environment information\\nthrough pre-defined function calls available in an API. All\\nof our experiments are carried out on the da Vinci Research\\nKit (dVRK) robot platform [33]. The initial position and\\norientation of the dVRK grippers are available from the robot\\ncontroller.\\nWe assume access to a single RGB-D camera with a known\\nintrinsic matrix, allowing for transformation between the\\ncamera’s perspective and the world coordinate space. With\\nthis, we design a perception module for the LLM to interact\\nwith and query object information. This module identifies\\nand retrieves the pose information of objects present in the\\nscene. For this, in simulation, we assume access to an instance\\nsegmentation model that, given an object name, outputs the\\nsegmentation maps of all instances of the queried object. In\\nthe physical experiments, we train a segmentation network\\nbased on the architecture from [37] for a needle segmentation\\nmodel.\\nIV. SUFIA\\nWe propose SUFIA, a framework for natural interaction\\nbetween surgeons and robots. SUFIA uses a human-in-the-\\n\\nLLM Generation\\nHigh-level Planning\\nTask Descriptions\\nTask Breakdown\\nTrajectory planning\\nPerception\\nObject Detection\\nPose Estimation\\nGripper Pose\\nRobot Action and Execution\\nTask Completion\\nAutonomy\\nDelegation\\nRobot Controller\\nSystem Prompt\\nSurgeon’s \\nCommand\\n+\\nLow-level Control\\nAPI Function Calls\\nPython Code\\nRe-planning\\nSuccess\\nFig. 2: SUFIA architecture and work-\\nflows. SUFIA enables a surgeon to nat-\\nurally interact with the robot by either\\nasking for a complete sub-task (e.g. “pick\\nup the needle and hand it over to the\\nother arm”) or generating a trajectory to\\nhelp with performing a task (e.g. “move\\nthe needle 1 cm to the left”). SUFIA uses\\nlimited environmental knowledge in nat-\\nural language (i.e. System Prompt) and\\nscene understanding from a perception\\nmodule to directly generate high-level\\nplans and low-level sequences of gripper\\nposes to interact with small-scale surgical\\nobjects. If SUFIA encounters difficulty\\nin querying for an object or executing\\na necessary step to solve the surgical\\nsub-task, it hands the control back to the\\nsurgeon for teleoperation.\\nloop approach, allowing either complete sub-task autonomy\\nor assistance in open-ended tasks to help surgeons achieve\\ntheir desired goals. The architectural framework, workflow,\\nand primary elements of SUFIA are shown in Fig. 2. The\\nfollowing sections elaborate on the specifics of SUFIA.\\nA. LLM Generation and Planning\\nCrucial to the effectiveness of any LLM-based system is\\nthe design of the prompt, as only changing the prompt format\\ncan already lead to large differences in performance [38]. We\\nbuild upon [21] who developed a single task agnostic prompt\\nfor performing low-level robot control for object grasping.\\nWe adapt it for surgical augmented dexterity with a four-part\\nprompt, which consists of a role description and three core\\nparts: the first part contains the API library available to the\\nLLM; the second provides limited environment information\\n(e.g. the state of the robot(s) to control and the orientation of\\nthe coordinate system); the third provides general instructions\\non how the LLM should generate the code, including the\\nformat of the desired output; the fourth describes prompt\\noptimizations such as doing step-by-step reasoning [39].\\nB. API Library\\nThe LLM has access to a library of functions that are\\navailable through an API. This API is documented in the main\\nprompt, where for each function, its signature is given along\\nwith a brief description of its functionality [21], [40]. The\\nAPI library mainly manages interaction with the robot control\\nand perception modules. The modular approach with the API\\nprovides SUFIA the flexibility to adapt its respective modules\\nindependently, enabling integration into new embodiments and\\nenvironments, such as switching from simulation to physical\\nexperiments.\\nSpecifically, the API library includes robot control func-\\ntionalities to execute a trajectory, rotate or open/close the\\ngripper of the specified robot arm, and return the control\\nback to the surgeon. Furthermore, perception functions detect\\nthe world poses of objects within the environment and can\\nvalidate whether an object is at the expected position.\\nC. Perception\\nWhile LLMs lack the capability to ground physical\\nworlds [41], they can still reason over the required steps\\nto interact with objects and plan for task execution. To do so,\\nwe design a perception module that enables the processing\\nof observations of the environment obtained from a single\\nRGB-D camera to provide the object states to the LLM\\ngenerator. This workflow is enabled by the API function\\ndetect_object, through which the LLM queries and interacts\\nwith the perception module to retrieve object information.\\ndetect_object takes as input the name of the object to\\ndetect. After obtaining a segmentation of the named object\\nand projecting it to world coordinates with the camera intrinsic\\nmatrix, we compute the 3D bounding cube and obtain the\\nlocation and orientation. Moreover, for circular objects such\\nas needles, we fit RANSAC to provide the object parameters\\nand compute candidates for the location and orientation of a\\nsuitable interaction point.\\nD. Safety\\nA critical issue in surgical robotics is the reliability and\\nsafety of the robot control. To this end, we implement two\\ncomponents tailored to improve these aspects:\\n1) Re-planning. The original plan could become inappro-\\npriate due to, for example, mistakes in the planning or\\nunforeseen circumstances, such as the gripper losing grip\\non the needle, in which case a new plan has to be devised.\\nWe encourage SUFIA to repeatedly use the verify_object\\nfunction to check whether the observed position of a given\\nobject matches the position expected by the framework. If\\nthe object being manipulated is not in the expected place,\\nSUFIA re-plans the steps to complete the desired task\\ngiven the updated knowledge of the environment.\\n2) Human-in-the-loop approach. In some cases, the per-\\nception module cannot find the desired object. In this\\ncase, rather than continuing blindly, SUFIA proceeds by\\nhanding control back to the surgeon for teleoperation\\nwith the API function transfer_control. SUFIA is also\\ninstructed to call this function when it does not know\\nhow to solve a certain (sub-)task rather than operating on\\n\\ninsufficient information. Fig. 6 illustrates an instance when\\nthe system is unable to execute a command properly and\\nreturns the control to the surgeon to adjust the environment\\nor provide further instruction.\\nTogether, these two components enhance the safety and\\nreliability of the assistant, which is crucial in the domain\\nof surgical robotics. Note that the surgeon can also directly\\ninstruct SUFIA to re-take control of the robot, ensuring a\\nsmooth interplay between surgeon and robot.\\nV. EXPERIMENTAL RESULTS\\nTo empirically measure the efficacy of SUFIA, we perform\\nexperiments both in ORBIT-Surgical, a high-fidelity surgical\\nsimulation framework, and on a dVRK platform in the lab.\\nA. Experimental Setup\\nWe conduct our simulation experiments in ORBIT-\\nSurgical [42], which accurately imitates joint articulation\\nand low-level controllers of the real dVRK platform, sup-\\nports contact-rich physical interactions between rigid and\\ndeformable objects, and provides high-fidelity rendering.\\nFurthermore, ORBIT-Surgical provides an interface for tele-\\noperation, which enables the user to work together with\\nSUFIA to solve a sub-task if needed. We use a camera\\nsensor in NVIDIA Omniverse to acquire 512 × 512 rendered\\nRGB-D images and ground-truth semantic segmentation\\nmasks. In section V-E, we will discuss the utility of general\\nsegmentation models to adapt the workflow to objects with\\nvarious configuration in simulation.\\nPhysical experiments are performed on a da Vinci Research\\nKit (dVRK) [33] robot surgical assistant, using an Allied\\nVision Prosilica GC 1290 stereo camera pair for visual input.\\nThese cameras are capable of producing paired stereo frames\\nat a resolution of 1280 × 960 at 33 fps. Real-world depth\\nimages are then subsequently obtained by passing image pairs\\nthrough RAFT-Stereo RVC [43], a state-of-the-art network for\\npredicting image correspondences using optical flow, and then\\nusing the camera’s calculated intrinsic matrix to retrieve depth\\nfrom these point discrepancies. We find that this approach\\nprovides better empirical results than traditional depth cameras\\nin our use case due to the small, reflective objects and short\\nfocal lengths involved in the surgical setting. To emulate the\\nreal-world conditions encountered in a surgical setting, our\\nworkspace consists of a 3-D Med suturing tissue phantom\\non a red background. The phantom is then wrapped in blue\\ncloth to imitate the use of a surgical cover during operation.\\nThese physical experiments introduce additional challenges,\\nincluding a more challenging perception task, estimation and\\ncontrol noise, and more complex physics.\\nThroughout this section, we use GPT-4 Turbo [12] unless\\nstated otherwise.\\nB. Tasks and Evaluation Metrics\\nWe demonstrate the generalizability of SUFIA by evaluat-\\ning it across four distinct simulated surgical sub-tasks derived\\nfrom ORBIT-Surgical, as shown in Fig. 3. We additionally\\nselect two of the subtasks (Needle Lift and Needle Handover)\\nfor evaluation using our physical setup. Each sub-task poses\\n(a)\\n(b)\\n(c)\\n(d)\\nFig. 3: Surgical sub-tasks. (a) Needle Lift: lift a suture needle\\nto a desired height, (b) Needle handover: pick and handover\\na suture needle, (c) Vessel Dilation: grip the vessel rim and\\ndilate by pulling, (d) Shunt Insertion: insert a soft tube into\\nlarger vessel phantom. Best viewed in color.\\nunique challenges to show the robustness of the proposed\\nworkflow, described as follows:\\nNeedle Lift – In this task, the needle (N1 in Fig. 5)\\nis initialized at a random position and orientation within the\\nreach area from a single dVRK arm. The task is successful\\nif the robot grasps and lifts the needle to a specified height\\nabove the table.\\nNeedle Handover – This task involves transferring a\\nneedle using a dual-arm dVRK setup. The needle is initially\\npositioned randomly. The arm closest to the needle first grasps\\nand lifts it to a specified handover location. Subsequently,\\nthe second arm reaches for the needle, grasps it, and takes it\\nto a desired position. The task is successful if the needle is\\neffectively transferred from the initial to the second arm.\\nVessel Dilation – In this task, a spring clamp\\nassembly holds a soft vessel phantom from two points. The\\ndVRK arm is required to grip the vessel rim from a third point\\nfacing the robot and dilate the vessel by pulling backward. A\\nsuccessful trial is defined if the robot fully dilates the vessel.\\nShunt Insertion – This task requires using a dVRK\\ngripper to insert a shunt into a vessel phantom. The arm grasps\\nthe shunt from the middle, lifts it slightly, and then inserts\\nit into a vessel phantom. The task is considered successful\\nif, upon release by the grippers, the shunt remains inside the\\nvessel phantom.\\nC. SUFIA Evaluation\\nWe now discuss the effectiveness of SUFIA on solving\\nthe proposed surgical sub-tasks. SUFIA utilizes a perception\\nmodule to localize the objects and proposes a sequence of\\nsub-trajectories to perform the required task. We present\\nthe success rate for each sub-task for 10 trials in TABLE I.\\nOverall, SUFIA is able to solve all proposed surgical sub-\\ntasks requiring precise grasping of small surgical objects in\\nsimulation. Each task poses a unique challenge for automation,\\nincluding object-gripper alignment and executing many steps\\nto achieve successful results. In the Vessel Dilation\\n\\n1\\n2\\n3\\n4\\nFig. 4: Physical Needle Handover task. (1) Starting workspace configuration. The needle is placed in a fixed position within the\\nworkspace, and the gripper positions are randomized. In this stage, the SUFIA LLM planner queries for and identifies the pose of the\\nsuture needle, determines which robot arm is closest to it, and plans a trajectory for that robot arm to reach the suture needle. (2) The\\nclosest robot arm approaches and grasps the suture needle. (3) The suture needle is lifted to a neutral handover position. At this stage, the\\nSUFIA LLM planner detects the pose of the suture needle at the handover position and plans a trajectory for the second robot arm to\\napproach the needle. (4) The second robot arm descends and grasps the needle, then the first robot arm releases the needle after the second\\nrobot arm has grasped it. We provide task videos at orbit-surgical.github.io/sufia\\nFailure Modes\\nExperiment\\nSuccess Rate\\nPlanning Steps\\n(P)\\n(E)\\nSim Experiments\\nNeedle Lift\\n100 %\\n6\\n0\\n0\\nNeedle Handover\\n90 %\\n14 - 16\\n1\\n0\\nVessel Dilation\\n60 %\\n6 - 8\\n3\\n1\\nShunt Insertion\\n70 %\\n8 - 9\\n3\\n0\\nPhysical Experiments\\nNeedle Lift\\n100 %\\n6\\n0\\n0\\nNeedle Handover\\n50 %\\n14 - 18\\n2\\n3\\nTABLE I: Evaluation Success rate and planning steps required\\nfor surgical sub-tasks automation (10 trials for each experiment).\\nFailure modes: (P) denotes planning and (E) denotes execution\\nfailures. Sim experiments are carried out in ORBIT-Surgical, a high-\\nfidelity surgical simulation framework. Physical Experiments are\\nperformed on a dVRK surgical platform.\\ntask, all planning failures were due to not rotating the\\ngrippers to grasp the vessel’s rim correctly. In the Shunt\\nInsertion task, the planning failures were from incorrect\\nlift height calculations before insertion.\\nWe observed that the performance of SUFIA was relatively\\nrobust to the more complex physics and observation spaces\\nof the physical environment, with 0 and 2 planning fail-\\nures encountered during the Needle Lift and Needle\\nHandover experiments, respectively. This aligns closely\\nwith the framework’s performance in simulation. However, we\\nfound that hysteresis and encoder mismatch within the cable-\\ndriven dVRK resulted in variation between the commanded\\nand actual gripper positions. Although SUFIA was often\\nable to recover from the failures induced by this mismatch\\nthrough its re-planning behavior, the lack of explicit servoing\\ncan result in dropping the needle during more complicated\\nhandovers.\\nD. Task Prompt Analysis\\nSimple tasks such as Needle Lift require a simple\\nprompt to function properly. The surgeon can specify a\\nposition to transfer the needle to or allow the LLM to\\ndetermine a specific lift height above the table.\\nMore sophisticated prompts are needed for tasks that re-\\nquire several steps for successful completion. In the Needle\\nHandover task, the surgeon can provide additional notes\\nfor SUFIA to consider (e.g., \"please note that for a handover,\\neach robot should grasp the needle from the side closest to\\nit.\"). The sequence in which the robot arms grasp and hand\\nover to each other, as well as the location of the handover, can\\neither be specified directly or left for the SUFIA to decide\\nbased on the distance to the needle or other environmental\\nstates.\\nThe SUFIA planner may suggest unnecessary steps that\\nmay not be required for task completion and may potentially\\nelongate task execution time. For instance, in the Vessel\\nDilation task, the vanilla prompt for dilating a vessel can\\nsometimes lead to an additional step of \"Lift the vessel slightly\\nby moving the end-effector upwards to provide clearance from\\nthe table.\" The surgeon can provide additional information\\nabout the fact that the clamps are holding the vessel vertically\\nto eliminate the suggestion of lifting steps in dilating the\\nvessel. Similarly, in the Shunt Insertion task, additional\\ninformation such as \"please lift the small tube by a specific\\namount off of the table and horizontally insert it\" helps to\\nachieve better planning and execution.\\nVision language models (VLMs) can also be incorporated\\nin SUFIA to enhance the general visual understanding of the\\nLLM planner. For instance, in the Vessel Dilation task,\\nGPT4-Vision [12] can provide the planner with environmental\\ncontext regarding the orientation of the vessel phantom. In\\nthis example, the VLM response can complement the user\\nprompt: I see a vertical yellow tube on the\\nright side of the image. It appears to\\nbe standing upright on one of its ends\\non a flat surface.\\nWhile\\nuseful\\nfor\\nproviding\\ngeneral visual context, similar to [44], we find GPT4-Vision\\nunreliable as a standalone perception module for detecting\\n(small) objects’ spatial states and omit it for the remainder\\nof our experiments.\\nThe prompts used for the tasks are as follows:\\nNeedle Lift – \"Pick up the needle and lift it.\"\\n\\n(N1)\\n(N2)\\n(N3)\\n(N4)\\n(N5)\\nFig. 5: Needle variations in simulation. We consider five instances\\nof simulated suture needles (N1 - N5) with various sizes and shapes\\nto conduct the generalizability experiment in ORBIT-Surgical.\\nNeedle Handover – \"Pick up the needle with the arm\\nclosest to it, move it directly to the handover location between\\nthe two arms, and keep holding the needle. Grasp the right\\nside of the needle with the other robot arm, then right after\\nthat, release the needle from the first robot and stay put.\"\\nVessel Dilation – \"Grasp the vessel from its leftmost\\nside with robot 0 and pull it backward to the left by 5\\nmillimeters while holding on to it to dilate. When grasping\\nthe vessel, grasp it 15 millimeters below the left point.\"\\nShunt Insertion – \"Lift the small shunt from the middle\\nand insert it into the left opening of the large tube. Approach\\nthe large tube from the left. Only lift the tube by 8 millimeters\\nand move horizontally to insert.\"\\nE. Perception Adaptation and Domain Variation\\nWhile we envision domain-specific perception models for\\napplications of our framework in a given surgical environment\\n(e.g., a needle segmentation network in our real-world\\nphysical experimentation), here we investigate whether a\\ngeneral-purpose segmentation model, LangSAM [45], can\\nbe utilized by the perception module in simulated surgical\\nenvironments to enhance the generalizability of the SUFIA\\nframework to various object shapes.\\nWe found that due to the slender shape of dVRK\\narms, the use of LangSAM in the perception module\\nsometimes returned both the dVRK arm and the suture\\nneedle when prompted to find the \"needle.\" However,\\ndescriptive adjectives (e.g. \"round\" or \"small white needle\")\\nenable LangSAM to correctly identify and segment the\\nsuture needle well enough for SUFIA to generate the\\nrequired steps to grasp and lift it. GPT-4 was also able to\\nreason over the sizes of the segmented objects and properly\\ndetermine the object of interest; here is an example of the\\nLLM planner: \"there are two objects detected\\nas \"white needle,\" but only one of them\\nhas dimensions that match a needle\\n(Width: 0.011, Length: 0.032, Height:\\n0.002). The second object’s dimensions\\nare too large to be the needle we are\\ninterested in. Therefore, we will focus\\non the first object with the correct\\ndimensions.\"\\nTo study the generalizability of the perception module\\nacross various needle sizes and shapes, we conduct a study\\nwith five different needles, three needles in different sizes (N1\\nPerception Module\\nN1\\nN2\\nN3\\nN4\\nN5\\nIsaac Sim Camera\\n5 / 5\\n4 / 5\\n5 / 5\\n5 / 5\\n4 / 5\\nLangSAM\\n4 / 5\\n5 / 5\\n4 / 5\\n3 / 5\\n3 / 5\\nTABLE II: Domain variation evaluation in simulation. We report\\nthe success rates for lifting suture needles with varied sizes and\\nirregular shapes (suture needles N1 - N5) over 5 trial runs with two\\nvariations of the perception module.\\n- N3; Fig. 5) and two irregular shapes (N4 and N5; Fig. 5).\\nTABLE II shows the performance of SUFIA to lift various\\nneedles, which is robust to their shape and size.\\nF. LLMs Investigation\\nHere, we investigate the effect of different LLMs on the\\nperformance of the needle lift task. We use the same prompt\\nand needle locations for all LLMs. As the error handling in\\nSUFIA would, in principle, allow an LLM to keep trying\\nendlessly until it generates code where no exceptions are\\nraised, we limit the number of errors to five before terminating\\nthe program.\\nNone of the open-source LLMs can perform the simple\\ntask of needle picking and have a hard time following\\nthe instructions in the prompt. All models struggle with\\nunderstanding that detect_object() will print its result\\nrather than return it as a variable in a Python script. When\\nfaced with errors, Mixtral [46] typically only outputs updated\\ncode snippets when asked to improve a code block rather\\nthan the whole code. CodeLlama [15] calls many undefined\\nfunctions, such as get_end_effector_pose(), despite\\nthe end-effector pose being given in the prompt. Llama 2 [47]\\nhas a variety of mistakes related to understanding the steps\\nin the task, such as forgetting to close the gripper or moving\\nit down before lifting the needle.\\nGPT3.5\\nTurbo\\nsimilarly\\nmisunderstands\\ndetect_object(),\\noften\\nassigning\\nits\\nvalue\\nto\\na\\nvariable called needle_position, despite the prompt\\nstating the function does not return anything. Beyond that,\\nGPT3.5 Turbo does consistently define a proper plan to\\nlift the needle, but even when it calls detect_object()\\ncorrectly, the information is not incorporated successfully.\\nAll in all, in our experiments, only GPT-4 Turbo could\\nfollow all instructions and appropriately plan and execute the\\nrelatively simple task of lifting a suture needle.\\nG. Re-planning\\nTo illustrate the benefits of our safety modules, we provide\\nan example in the Needle Lift environment in Fig. 7.\\nIn the first row, SUFIA executes the plan it came up with\\nto perform the task desired by the user, i.e., orienting its\\ngripper with the needle, moving to a position where it can\\ngrab it, and picking it up. While picking it up, we move the\\nneedle to a different position. Because SUFIA validates the\\nexpected and observed position of the objects it manipulates,\\nit correctly identifies the needle is not where it should be.\\nBased on the newly observed state, SUFIA devises a new\\nplan to proceed with the user instruction, finally lifting it to\\nthe desired height.\\n\\nSuFIA LLM Planner: To begin the task of finding the suturing site, I will first need to detect \\nthe location of the suturing site within the environment. I will call the `detect_object` function \\nwith the parameter \"suturing site\" to obtain its position.\\n```python\\ndetect_object(\"suturing site\")\\n```\\nSystem execution output: Object not found!\\nSuFIA LLM Planner: Since the \"suturing site\" object was not found in the environment, I \\nwill defer control to the surgeon to provide further instructions or to adjust the environment so \\nthat the suturing site can be detected.\\n```python\\ntransfer_control()\\n```\\nControl of the robot is transferred to the Surgeon.\\n(a)\\n(b)\\n(c)\\nSurgeon: Please find the suturing site.\\nFig. 6: Interactive human-in-the-loop approach. (a) An overview of the environment showing the dVRK robotic arm and endoscope\\ncamera as well as a needle and a suturing pad in ORBIT-Surgical, (b) RGB image from the endoscope camera focused on the needle as\\ninput to the perception module, (c) delegation of control back to the surgeon due to the inability of the system to identify a suturing site.\\nThe uncertainty and occlusion in a surgical scene might lead to undesired object localization and retrieval. A human-in-the-loop approach\\nenables a fail-safe paradigm for interactive robotic surgical assistants.\\nLLM Function Call for Verification:\\n# Verify that the needle has been picked up and \\nlifted\\nverify_object(\"needle\", lifted_position, 0.015)\\nFunction Output:\\nverify_object output: False\\nLLM Planner:\\nThe verification step has returned `False`, indicating that \\nthe needle has not been successfully picked up and lifted \\nto the specified position. Therefore, I will need to re-open \\nthe gripper and plan a new trajectory to complete the task.\\nLLM Function Call for Verification:\\n# Verify that the needle has been picked up and \\nlifted\\nverify_object(\"needle\", lifted_position, 0.015)\\nFunction Output:\\nverify_object output: True\\n(a) Failed execution\\n(b) Re-planning and successful \\nexecution\\nFig. 7: Re-planning. (a) A failed execution as a result of not\\nfinding a proper grasping point on the needle. The LLM verification\\nstep indicates that the task was not executed successfully. (b) LLM\\nplanner proposes a new plan to re-identify and lift the needle by\\nthe robot.\\nH. Limitations\\nThe current best results are obtained with API calls to\\nGPT-4 Turbo. Due to the generation speed of OpenAI’s\\nAPI, SUFIA does not operate in real-time; for real-world\\nneedle handover trials, the API calls invoked during planning\\n(including sensing and replanning) took an average of 25.5\\nseconds to complete, out of an average total trial time of\\n61.4 seconds. However, with open-source models constantly\\nimproving, running a quantized open-source LLM on-device\\nwill soon be a viable way to improve inference time greatly.\\nMoreover, while we incorporate two measures specifically\\ndesigned to improve safety and reliability, deploying au-\\ntonomous or semi-autonomous RSAs in real-world scenarios\\nstill has the potential to bring risks from unexpected circum-\\nstances an AI system might not be able to handle.\\nVI. CONCLUSION\\nWe present SUFIA, a modular framework for natural\\nsurgeon-robot interaction. We show that our training-free\\napproach, which uses pre-trained LLMs to provide low-level\\ncontrol of surgical robots, can successfully interact with\\nsmall surgical objects and execute surgeon commands for\\nautomating surgical sub-tasks. Safety is bolstered through\\nre-planning capabilities and a human-in-the-loop approach.\\nWe evaluate the efficacy of SUFIA for common surgical\\nsub-tasks in simulated and physical experiments in the lab\\nand show that the proposed method succeeds across different\\nsub-tasks with various difficulty levels. These results suggest\\nthat language-guided autonomy has the potential to enhance\\nsurgeon’s efficiency in surgical procedures.\\nIn future work, we plan to test the viability of quantized\\nopen-source LLMs on-device to improve inference time.\\nThis will also address any privacy concerns stemming from\\ntransmitting highly sensitive medical information to off-site\\nservers. Furthermore, we intend to explore the usefulness of\\nfine-tuned large language and vision models in SUFIA.\\nREFERENCES\\n[1] M. Hwang, D. Seita, B. Thananjeyan, J. Ichnowski, S. Paradis, D. Fer,\\nT. Low, and K. Goldberg, “Applying depth-sensing to automated\\nsurgical manipulation with a da vinci robot,” in 2020 international\\nsymposium on medical robotics (ISMR).\\nIEEE, 2020, pp. 22–29.\\n[2] S. Sen, A. Garg, D. V. Gealy, S. McKinley, Y. Jen, and K. Goldberg,\\n“Automating multi-throw multilateral surgical suturing with a mechani-\\ncal needle guide and sequential convex optimization,” in 2016 IEEE\\ninternational conference on robotics and automation (ICRA).\\nIEEE,\\n2016, pp. 4178–4185.\\n[3] H. Lin, B. Li, X. Chu, Q. Dou, Y. Liu, and K. W. S. Au, “End-to-\\nend learning of deep visuomotor policy for needle picking,” in 2023\\nIEEE/RSJ International Conference on Intelligent Robots and Systems\\n(IROS).\\nIEEE, 2023, pp. 8487–8494.\\n[4] P. M. Scheikl, B. Gyenes, R. Younis, C. Haas, G. Neumann, M. Wagner,\\nand F. Mathis-Ullrich, “LapGym - An open source framework for\\nreinforcement learning in robot-assisted laparoscopic surgery,” Journal\\nof Machine Learning Research, vol. 24, no. 368, pp. 1–42, 2023.\\n[5] J. Xu, B. Li, B. Lu, Y.-H. Liu, Q. Dou, and P.-A. Heng, “Surrol:\\nAn open-source reinforcement learning centered and dvrk compatible\\nplatform for surgical robot learning,” in 2021 IEEE/RSJ International\\nConference on Intelligent Robots and Systems (IROS).\\nIEEE, 2021,\\npp. 1821–1828.\\n[6] A. Attanasio, B. Scaglioni, E. De Momi, P. Fiorini, and P. Valdastri,\\n“Autonomy in surgical robotics,” Annual Review of Control, Robotics,\\nand Autonomous Systems, vol. 4, pp. 651–679, 2021.\\n[7] L. Wang, C. Ma, X. Feng, Z. Zhang, H. Yang, J. Zhang, Z. Chen,\\nJ. Tang, X. Chen, Y. Lin et al., “A survey on large language model\\nbased autonomous agents,” arXiv preprint arXiv:2308.11432, 2023.\\n[8] D. Driess, F. Xia, M. S. Sajjadi, C. Lynch, A. Chowdhery, B. Ichter,\\n\\nA. Wahid, J. Tompson, Q. Vuong, T. Yu et al., “Palm-e: An embodied\\nmultimodal language model,” arXiv preprint arXiv:2303.03378, 2023.\\n[9] A. Brohan, N. Brown, J. Carbajal, Y. Chebotar, J. Dabis, C. Finn,\\nK. Gopalakrishnan, K. Hausman, A. Herzog, J. Hsu et al., “RT-1:\\nRobotics transformer for real-world control at scale,” arXiv preprint\\narXiv:2212.06817, 2022.\\n[10] A. Brohan, N. Brown, J. Carbajal, Y. Chebotar, X. Chen, K. Choro-\\nmanski, T. Ding, D. Driess, A. Dubey, C. Finn et al., “RT-2: Vision-\\nlanguage-action models transfer web knowledge to robotic control,”\\narXiv preprint arXiv:2307.15818, 2023.\\n[11] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\\nŁ. Kaiser, and I. Polosukhin, “Attention is all you need,” Advances in\\nneural information processing systems, vol. 30, 2017.\\n[12] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman,\\nD. Almeida, J. Altenschmidt, S. Altman, S. Anadkat et al., “GPT-4\\ntechnical report,” arXiv preprint arXiv:2303.08774, 2023.\\n[13] H. Liu, C. Li, Q. Wu, and Y. J. Lee, “Visual instruction tuning,”\\nAdvances in neural information processing systems, vol. 36, 2024.\\n[14] Z. Luo, C. Xu, P. Zhao, Q. Sun, X. Geng, W. Hu, C. Tao, J. Ma,\\nQ. Lin, and D. Jiang, “Wizardcoder: Empowering code large language\\nmodels with evol-instruct,” arXiv preprint arXiv:2306.08568, 2023.\\n[15] B. Roziere, J. Gehring, F. Gloeckle, S. Sootla, I. Gat, X. E. Tan, Y. Adi,\\nJ. Liu, T. Remez, J. Rapin et al., “Code llama: Open foundation models\\nfor code,” arXiv preprint arXiv:2308.12950, 2023.\\n[16] Z. Wang, L. Liu, L. Wang, and L. Zhou, “R2GenGPT: Radiology\\nreport generation with frozen LLMs,” Meta-Radiology, vol. 1, no. 3, p.\\n100033, 2023.\\n[17] J. Liang, W. Huang, F. Xia, P. Xu, K. Hausman, B. Ichter, P. Florence,\\nand A. Zeng, “Code as policies: Language model programs for\\nembodied control,” in 2023 IEEE International Conference on Robotics\\nand Automation (ICRA).\\nIEEE, 2023, pp. 9493–9500.\\n[18] S. Vemprala, R. Bonatti, A. Bucker, and A. Kapoor, “ChatGPT for\\nrobotics: Design principles and model abilities,” Microsoft Auton. Syst.\\nRobot. Res, vol. 2, p. 20, 2023.\\n[19] T. Xie, S. Zhao, C. H. Wu, Y. Liu, Q. Luo, V. Zhong, Y. Yang, and\\nT. Yu, “Text2Reward: Automated dense reward function generation for\\nreinforcement learning,” arXiv preprint arXiv:2309.11489, 2023.\\n[20] Y. J. Ma, W. Liang, G. Wang, D.-A. Huang, O. Bastani, D. Jayara-\\nman, Y. Zhu, L. Fan, and A. Anandkumar, “Eureka: Human-level\\nreward design via coding large language models,” arXiv preprint\\narXiv:2310.12931, 2023.\\n[21] T. Kwon, N. Di Palo, and E. Johns, “Language models as zero-shot\\ntrajectory generators,” arXiv preprint arXiv:2310.11604, 2023.\\n[22] T. D. Nagy and T. Haidegger, “Autonomous surgical robotics at task\\nand subtask levels,” in Advanced Robotics and Intelligent Automation\\nin Manufacturing.\\nIGI global, 2020, pp. 296–319.\\n[23] F. Ficuciello, G. Tamburrini, A. Arezzo, L. Villani, and B. Siciliano,\\n“Autonomy in surgical robots and its meaningful human control,”\\nPaladyn, Journal of Behavioral Robotics, vol. 10, no. 1, pp. 30–43,\\n2019.\\n[24] A. Wilcox, J. Kerr, B. Thananjeyan, J. Ichnowski, M. Hwang, S. Paradis,\\nD. Fer, and K. Goldberg, “Learning to localize, grasp, and hand over\\nunmodified surgical needles,” in 2022 International Conference on\\nRobotics and Automation (ICRA).\\nIEEE, 2022, pp. 9637–9643.\\n[25] S. Krishnan, A. Garg, S. Patil, C. Lea, G. Hager, P. Abbeel, and\\nK. Goldberg, “Transition state clustering: Unsupervised surgical\\ntrajectory segmentation for robot learning,” International Journal of\\nRobotics Research (IJRR), 2017.\\n[26] C. Shin, P. W. Ferguson, S. A. Pedram, J. Ma, E. P. Dutson, and J. Rosen,\\n“Autonomous tissue manipulation via surgical robot using learning based\\nmodel predictive control,” in 2019 International conference on robotics\\nand automation (ICRA).\\nIEEE, 2019, pp. 3875–3881.\\n[27] N. D. Nguyen, T. Nguyen, S. Nahavandi, A. Bhatti, and G. Guest, “Ma-\\nnipulating soft tissues by deep reinforcement learning for autonomous\\nrobotic surgery,” in 2019 IEEE International Systems Conference\\n(SysCon).\\nIEEE, 2019, pp. 1–7.\\n[28] Y. Li, F. Richter, J. Lu, E. K. Funk, R. K. Orosco, J. Zhu, and M. C.\\nYip, “Super: A surgical perception framework for endoscopic tissue\\nmanipulation with surgical robotics,” IEEE Robotics and Automation\\nLetters, vol. 5, no. 2, pp. 2294–2301, 2020.\\n[29] K. Goldberg. (2023) Augmented Dexterity: How Robots Can\\nEnhance\\nSurgeon\\nDexterity.\\n[Online].\\nAvailable:\\nhttps://bit.ly/\\nAugmented-Dexterity-S24\\n[30] A. Shademan, R. S. Decker, J. D. Opfermann, S. Leonard, A. Krieger,\\nand P. C. Kim, “Supervised autonomous robotic soft tissue surgery,”\\nScience translational medicine, vol. 8, no. 337, pp. 337ra64–337ra64,\\n2016.\\n[31] H. Saeidi, J. D. Opfermann, M. Kam, S. Wei, S. Léonard, M. H. Hsieh,\\nJ. U. Kang, and A. Krieger, “Autonomous robotic laparoscopic surgery\\nfor intestinal anastomosis,” Science robotics, vol. 7, no. 62, p. eabj2908,\\n2022.\\n[32] B. Hannaford, J. Rosen, D. W. Friedman, H. King, P. Roan, L. Cheng,\\nD. Glozman, J. Ma, S. N. Kosari, and L. White, “Raven-ii: an\\nopen platform for surgical robotics research,” IEEE Transactions on\\nBiomedical Engineering, vol. 60, no. 4, pp. 954–959, 2012.\\n[33] P. Kazanzides, Z. Chen, A. Deguet, G. S. Fischer, R. H. Taylor, and S. P.\\nDiMaio, “An open-source research kit for the da vinci® surgical system,”\\nin 2014 IEEE international conference on robotics and automation\\n(ICRA).\\nIEEE, 2014, pp. 6434–6439.\\n[34] T. Ranzani, G. Gerboni, M. Cianchetti, and A. Menciassi, “A bioinspired\\nsoft manipulator for minimally invasive surgery,” Bioinspiration &\\nbiomimetics, vol. 10, no. 3, p. 035008, 2015.\\n[35] K. B. Ozyoruk, G. I. Gokceler, T. L. Bobrow, G. Coskun, K. Incetan,\\nY. Almalioglu, F. Mahmood, E. Curto, L. Perdigoto, M. Oliveira et al.,\\n“Endoslam dataset and an unsupervised monocular visual odometry\\nand depth estimation approach for endoscopic videos,” Medical image\\nanalysis, vol. 71, p. 102058, 2021.\\n[36] M. S. Yasar and H. Alemzadeh, “Real-time context-aware detection of\\nunsafe events in robot-assisted surgery,” in 2020 50th Annual IEEE/IFIP\\nInternational Conference on Dependable Systems and Networks (DSN),\\n2020, pp. 385–397.\\n[37] K. Dharmarajan, W. Panitch, B. Shi, H. Huang, L. Y. Chen, M. Moghani,\\nQ. Yu, K. Hari, T. Low, D. Fer et al., “Robot-assisted vascular shunt\\ninsertion with the dvrk surgical robot,” Journal of Medical Robotics\\nResearch, vol. 8, no. 03n04, p. 2340006, 2023.\\n[38] Z. Zhao, E. Wallace, S. Feng, D. Klein, and S. Singh, “Calibrate\\nbefore use: Improving few-shot performance of language models,” in\\nInternational Conference on Machine Learning.\\nPMLR, 2021, pp.\\n12 697–12 706.\\n[39] T. Kojima, S. S. Gu, M. Reid, Y. Matsuo, and Y. Iwasawa, “Large lan-\\nguage models are zero-shot reasoners,” Advances in neural information\\nprocessing systems, vol. 35, pp. 22 199–22 213, 2022.\\n[40] I. Singh, V. Blukis, A. Mousavian, A. Goyal, D. Xu, J. Tremblay, D. Fox,\\nJ. Thomason, and A. Garg, “Progprompt: program generation for\\nsituated robot task planning using large language models,” Autonomous\\nRobots, pp. 1–14, 2023.\\n[41] R. Liu, J. Wei, S. S. Gu, T.-Y. Wu, S. Vosoughi, C. Cui, D. Zhou, and\\nA. M. Dai, “Mind’s eye: Grounded language model reasoning through\\nsimulation,” arXiv preprint arXiv:2210.05359, 2022.\\n[42] Q. Yu, M. Moghani, K. Dharmarajan, V. Schorp, W. C.-H. Panitch,\\nJ. Liu, K. Hari, H. Huang, M. Mittal, K. Goldberg, and A. Garg,\\n“ORBIT-Surgical: An open-simulation framework for learning surgical\\naugmented dexterity,” arXiv preprint arXiv:2404.16027, 2024.\\n[43] H. Jiang, R. Xu, and W. Jiang, “An improved raftstereo trained with\\na mixed dataset for the robust vision challenge 2022,” arXiv preprint\\narXiv:2210.12785, 2022.\\n[44] M. Skreta, Z. Zhou, J. L. Yuan, K. Darvish, A. Aspuru-Guzik, and\\nA. Garg, “Replan: Robotic replanning with perception and language\\nmodels,” arXiv preprint arXiv:2401.04157, 2024.\\n[45] L. Medeiros. (2023) LangSam: Language Segment-Anything. [Online].\\nAvailable: https://github.com/luca-medeiros/lang-segment-anything\\n[46] A. Q. Jiang, A. Sablayrolles, A. Roux, A. Mensch, B. Savary,\\nC. Bamford, D. S. Chaplot, D. d. l. Casas, E. B. Hanna, F. Bressand\\net al., “Mixtral of experts,” arXiv preprint arXiv:2401.04088, 2024.\\n[47] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei,\\nN. Bashlykov, S. Batra, P. Bhargava, S. Bhosale et al., “Llama\\n2: Open foundation and fine-tuned chat models,” arXiv preprint\\narXiv:2307.09288, 2023.\\n'"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_output[\"extracted_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Imagine a future where surgeons can guide robotic surgical assistants with simple voice commands, like \"Lift the needle,\" and the robot flawlessly executes the task.  That future is closer than you think, thanks to a groundbreaking new framework called SUFIA.\\n\\nThe research problem tackled by this paper is the current limitations of robotic surgical assistants (RSAs).  While RSAs offer enhanced precision and dexterity, they often require tedious, repetitive programming for each surgical sub-task.  Learning-based approaches are computationally expensive, lack generalizability, and pose safety concerns in the high-stakes environment of surgery.  This matters because it slows down procedures, increases surgeon fatigue, and potentially impacts patient outcomes.\\n\\nSUFIA changes the game by using Large Language Models (LLMs), the same technology behind conversational AI like ChatGPT, to control the robot.  Instead of pre-programmed instructions, the surgeon provides natural language commands.  SUFIA\\'s key finding is that it can successfully translate these commands into precise robotic actions, performing various surgical sub-tasks in both simulated and real-world settings.  This is a learning-free approach, meaning it doesn\\'t require extensive training data for each task, making it incredibly versatile.  The system also incorporates safety features:  if the robot encounters a problem or lacks sufficient information, it hands control back to the surgeon.\\n\\nThe implications are huge.  SUFIA could significantly streamline surgical workflows, reducing surgeon fatigue and potentially improving surgical precision.  The ability to adapt to different tasks without extensive reprogramming opens up possibilities for wider adoption of robotic assistance in surgery.\\n\\nHere are some discussion points for your podcast:\\n\\n1. **The ethical implications of delegating surgical tasks to AI:** How much autonomy should we give to AI in surgery? What are the safety protocols needed to ensure patient well-being?  Should there always be a human surgeon in the loop, or could fully autonomous procedures be possible in the future?\\n\\n2. **The generalizability of SUFIA:** The study shows success across several sub-tasks.  But how well would it perform with completely novel tasks or unexpected situations?  What are the limitations of relying on an LLM\\'s understanding of the world?  Could this approach be applied beyond surgery to other complex robotic tasks?\\n\\n3. **The role of human-in-the-loop control:**  SUFIA\\'s safety features are crucial.  But how seamless is the transition between autonomous operation and human control?  Could this handoff process be improved to minimize delays or disruptions during surgery?\\n\\nFor a graphical abstract, consider visualizing the SUFIA architecture (Figure 2 from the paper) showing the flow of information between the surgeon, the LLM, the perception module, and the robot.  Another compelling visual would be a comparison of success rates across different surgical sub-tasks (Table I), highlighting the robustness of the system.  Finally, a visual showing the different needle variations (Figure 5) and the success rates in handling them would demonstrate the generalizability of the perception module.  The contrast between the success of GPT-4 and the struggles of other LLMs (Section V-F) would also make a strong visual point about the current state of the technology.'"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_output[\"summary_text\"].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(state_output[\"convo\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(state_output[\"ppt_object\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'Introduction', 'bullet_points': ['Problem: Current robotic surgical assistants (RSAs) often require tedious teleoperation, leading to surgeon fatigue.', 'Objective: Develop SUFIA, a framework for natural language-guided augmented dexterity in RSAs.', 'Motivation:  Improve surgical workflow, reduce surgeon fatigue, and enhance patient outcomes by enabling a learning-free, generalizable approach to surgical task automation.', 'Existing approaches using reinforcement learning are computationally expensive, require extensive data, and lack generalizability.'], 'notes': 'This slide highlights the problem addressed by the research, the specific objective of creating SUFIA, and the reasons behind this research. It also briefly discusses limitations of existing learning-based methods.', 'images': None}\n"
     ]
    }
   ],
   "source": [
    "ppt_content = state_output[\"ppt_object\"]\n",
    "# print(ppt_content)\n",
    "print(ppt_content[\"slides\"][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Fig1': 'extracted_images/page_1_img_1.jpeg',\n",
       " 'Fig2': 'extracted_images/page_1_img_2.jpeg',\n",
       " 'Fig3': 'extracted_images/page_4_img_1.jpeg',\n",
       " 'Fig4': 'extracted_images/page_4_img_2.jpeg',\n",
       " 'Fig5': 'extracted_images/page_4_img_3.jpeg',\n",
       " 'Fig6': 'extracted_images/page_4_img_4.jpeg',\n",
       " 'Fig7': 'extracted_images/page_5_img_1.jpeg',\n",
       " 'Fig8': 'extracted_images/page_5_img_2.jpeg',\n",
       " 'Fig9': 'extracted_images/page_5_img_3.jpeg',\n",
       " 'Fig10': 'extracted_images/page_5_img_4.jpeg',\n",
       " 'Fig11': 'extracted_images/page_6_img_1.jpeg',\n",
       " 'Fig12': 'extracted_images/page_7_img_1.jpeg',\n",
       " 'Fig13': 'extracted_images/page_7_img_2.jpeg',\n",
       " 'Fig14': 'extracted_images/page_7_img_3.jpeg',\n",
       " 'Fig15': 'extracted_images/page_7_img_4.jpeg'}"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_output[\"extracted_images\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Title Slide',\n",
       "  'bullet_points': ['SUFIA: Language-Guided Augmented Dexterity for Robotic Surgical Assistants',\n",
       "   'Masoud Moghani, Lars Doorenbos, William Chung-Ho Panitch, Sean Huver, Mahdi Azizian, Ken Goldberg, Animesh Garg',\n",
       "   'University of Toronto, University of Bern, University of California, Berkeley, NVIDIA, Georgia Institute of Technology'],\n",
       "  'notes': \"This slide introduces the research paper's title, authors, and their affiliations.  It sets the stage for the presentation.\",\n",
       "  'images': None},\n",
       " {'title': 'Introduction',\n",
       "  'bullet_points': ['Problem: Current robotic surgical assistants (RSAs) often require tedious teleoperation, leading to surgeon fatigue.',\n",
       "   'Objective: Develop SUFIA, a framework for natural language-guided augmented dexterity in RSAs.',\n",
       "   'Motivation:  Improve surgical workflow, reduce surgeon fatigue, and enhance patient outcomes by enabling a learning-free, generalizable approach to surgical task automation.',\n",
       "   'Existing approaches using reinforcement learning are computationally expensive, require extensive data, and lack generalizability.'],\n",
       "  'notes': 'This slide highlights the problem addressed by the research, the specific objective of creating SUFIA, and the reasons behind this research. It also briefly discusses limitations of existing learning-based methods.',\n",
       "  'images': None},\n",
       " {'title': 'Methods',\n",
       "  'bullet_points': ['Methodology:  A human-in-the-loop framework combining Large Language Models (LLMs) with perception modules for high-level planning and low-level robot control.',\n",
       "   'Datasets:  Simulation environment (ORBIT-Surgical) and a physical dVRK surgical robotic platform.',\n",
       "   'Experimental Setup:  Four simulated surgical sub-tasks and two physical sub-tasks (Needle Lift, Needle Handover) were evaluated.  Used GPT-4 Turbo unless otherwise specified.',\n",
       "   'Perception Module: Uses RGB-D camera input and object segmentation (instance segmentation in simulation, trained network for physical experiments).'],\n",
       "  'notes': 'This slide details the research methodology, including the use of LLMs and perception modules, the datasets used, and the experimental setup.  It emphasizes the use of both simulation and real-world data.',\n",
       "  'images': None},\n",
       " {'title': 'Results',\n",
       "  'bullet_points': ['SUFIA successfully performed all four simulated surgical sub-tasks and two physical sub-tasks.',\n",
       "   'Table I shows success rates and planning steps for each task (simulation and physical).  Higher success rates observed in simulation.',\n",
       "   'Figure 3 shows the four simulated surgical sub-tasks: Needle Lift, Needle Handover, Vessel Dilation, and Shunt Insertion.',\n",
       "   'Figure 4 illustrates the physical Needle Handover task.',\n",
       "   'Figure 5 shows variations in simulated needles used to test generalizability.',\n",
       "   'Table II shows the success rates for lifting needles with varied shapes and sizes using LangSAM.',\n",
       "   'Physical experiments showed robustness but also highlighted challenges due to dVRK limitations (hysteresis, encoder mismatch).'],\n",
       "  'notes': 'This slide summarizes the key findings of the research, referencing relevant figures and tables from the paper. It also points out limitations observed in the physical experiments.',\n",
       "  'images': None},\n",
       " {'title': 'Graphics',\n",
       "  'bullet_points': ['Fig. 1: Overview of SUFIA automating needle lifting.',\n",
       "   'Fig. 2: SUFIA architecture and workflows.',\n",
       "   'Fig. 3: Simulated surgical sub-tasks.',\n",
       "   'Fig. 4: Physical Needle Handover task.',\n",
       "   'Fig. 5: Needle variations in simulation.',\n",
       "   'Fig. 6: Interactive human-in-the-loop approach.',\n",
       "   'Fig. 7: Re-planning example.'],\n",
       "  'notes': 'This slide lists all figures mentioned in the extracted text.  Each figure is crucial for understanding different aspects of the research.',\n",
       "  'images': ['Fig. 1',\n",
       "   'Fig. 2',\n",
       "   'Fig. 3',\n",
       "   'Fig. 4',\n",
       "   'Fig. 5',\n",
       "   'Fig. 6',\n",
       "   'Fig. 7']},\n",
       " {'title': 'Discussion',\n",
       "  'bullet_points': ['SUFIA demonstrates the potential of a general-purpose, language-guided approach to surgical augmented dexterity.',\n",
       "   'Comparison with prior work:  Unlike task-specific learning-based methods, SUFIA is learning-free and generalizes across multiple tasks.',\n",
       "   'Significance:  Reduces reliance on extensive training data and complex reward engineering.',\n",
       "   \"Limitations:  Current implementation is not real-time due to GPT-4 Turbo's API speed.  Potential risks remain from unexpected circumstances.\"],\n",
       "  'notes': 'This slide discusses the significance of the results, comparing SUFIA to previous approaches and highlighting its advantages and limitations.  It emphasizes the generalizability and learning-free nature of the approach.',\n",
       "  'images': None},\n",
       " {'title': 'Conclusion',\n",
       "  'bullet_points': ['SUFIA is a modular, natural language-guided framework for surgeon-robot interaction.',\n",
       "   'Successfully automates surgical sub-tasks using pre-trained LLMs and a human-in-the-loop approach.',\n",
       "   'Future work: Explore on-device, quantized open-source LLMs for real-time operation and privacy enhancement. Investigate fine-tuned LLMs and VLMs.',\n",
       "   'Language-guided autonomy shows promise for improving surgical efficiency.'],\n",
       "  'notes': 'This slide summarizes the key takeaways from the research and outlines potential future directions for the work.  It emphasizes the overall success and potential impact of SUFIA.',\n",
       "  'images': None},\n",
       " {'title': 'References',\n",
       "  'bullet_points': ['[1]–[47] (See extracted text for full citations)'],\n",
       "  'notes': 'This slide provides a placeholder for the full list of references cited in the research paper.',\n",
       "  'images': None}]"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppt_content[\"slides\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PowerPoint presentation saved as sufia_2.pptx\n"
     ]
    }
   ],
   "source": [
    "from pptx import Presentation\n",
    "from pptx.util import Pt, Inches\n",
    "from pptx.enum.text import PP_ALIGN, MSO_ANCHOR, MSO_AUTO_SIZE\n",
    "from pptx.dml.color import RGBColor\n",
    "\n",
    "class ThemeConfig:\n",
    "    def __init__(self, name=\"modern\"):\n",
    "        themes = {\n",
    "            \"modern\": {\n",
    "                \"background\": RGBColor(30, 30, 30),  # Dark gray background\n",
    "                \"title\": RGBColor(255, 215, 0),  # Gold title text\n",
    "                \"body\": RGBColor(200, 200, 200),  # Light gray body text\n",
    "                \"title_font\": \"Montserrat\",\n",
    "                \"body_font\": \"Lato\",\n",
    "            },\n",
    "            \"vintage\": {\n",
    "                \"background\": RGBColor(245, 222, 179),  # Wheat background\n",
    "                \"title\": RGBColor(139, 69, 19),  # Saddle brown title\n",
    "                \"body\": RGBColor(105, 105, 105),  # Dim gray text\n",
    "                \"title_font\": \"Georgia\",\n",
    "                \"body_font\": \"Times New Roman\",\n",
    "            },\n",
    "            \"corporate\": {\n",
    "                \"background\": RGBColor(255, 255, 255),  # White background\n",
    "                \"title\": RGBColor(0, 51, 102),  # Navy blue title\n",
    "                \"body\": RGBColor(51, 51, 51),  # Dark gray body text\n",
    "                \"title_font\": \"Arial\",\n",
    "                \"body_font\": \"Verdana\",\n",
    "            },\n",
    "            \"minimal\": {\n",
    "                \"background\": RGBColor(240, 240, 240),  # Light gray background\n",
    "                \"title\": RGBColor(50, 50, 50),  # Dark gray title\n",
    "                \"body\": RGBColor(80, 80, 80),  # Slightly lighter gray for body\n",
    "                \"title_font\": \"Helvetica\",\n",
    "                \"body_font\": \"Sans-Serif\",\n",
    "            },\n",
    "            \"bold\": {\n",
    "                \"background\": RGBColor(0, 0, 0),  # Black background\n",
    "                \"title\": RGBColor(255, 0, 0),  # Red title text\n",
    "                \"body\": RGBColor(255, 255, 255),  # White body text\n",
    "                \"title_font\": \"Impact\",\n",
    "                \"body_font\": \"Arial Black\",\n",
    "            }\n",
    "        }\n",
    "        self.theme = themes.get(name, themes[\"minimal\"])\n",
    "\n",
    "def apply_background(slide, color):\n",
    "    \"\"\"Apply background color to a slide\"\"\"\n",
    "    background = slide.background\n",
    "    fill = background.fill\n",
    "    fill.solid()\n",
    "    fill.fore_color.rgb = color\n",
    "\n",
    "def create_ppt_from_dict(ppt_data: dict, image_mapping: dict, theme_name: str=\"default\", output_file: str = \"presentation.pptx\"):\n",
    "    prs = Presentation()\n",
    "    theme = ThemeConfig(theme_name)\n",
    "\n",
    "    slide_width = prs.slide_width\n",
    "    slide_height = prs.slide_height\n",
    "\n",
    "    # Title Slide Fix\n",
    "    title_slide_layout = prs.slide_layouts[0]  # Title slide layout\n",
    "    title_slide = prs.slides.add_slide(title_slide_layout)\n",
    "    apply_background(title_slide, theme.theme[\"background\"])\n",
    "    # Set title\n",
    "    title = title_slide.shapes.title\n",
    "    title.text = ppt_data['title']\n",
    "    title_para = title.text_frame.paragraphs[0]\n",
    "    title_para.font.size = Pt(40)\n",
    "    title_para.font.name = theme.theme[\"title_font\"]\n",
    "    title_para.font.color.rgb = theme.theme[\"title\"]\n",
    "    title_para.alignment = PP_ALIGN.CENTER\n",
    "\n",
    "    # Set subtitle (authors and institutions)\n",
    "    subtitle = title_slide.placeholders[1]\n",
    "    subtitle.text_frame.clear()  # Clear default placeholder text\n",
    "\n",
    "    # Add authors as one paragraph\n",
    "    authors_para = subtitle.text_frame.add_paragraph()\n",
    "    authors_para.text = \", \".join(ppt_data['authors'])\n",
    "    authors_para.font.size = Pt(18)\n",
    "    authors_para.font.name = theme.theme[\"body_font\"]\n",
    "    authors_para.font.color.rgb = theme.theme[\"body\"]\n",
    "    authors_para.alignment = PP_ALIGN.CENTER\n",
    "\n",
    "    # Add institution as a separate paragraph\n",
    "    institution_para = subtitle.text_frame.add_paragraph()\n",
    "    institution_para.text = \"\".join(ppt_data['institution'])\n",
    "    institution_para.font.size = Pt(16)  # Slightly smaller font\n",
    "    institution_para.font.name = theme.theme[\"body_font\"]\n",
    "    institution_para.font.color.rgb = theme.theme[\"body\"]\n",
    "    institution_para.alignment = PP_ALIGN.CENTER\n",
    "\n",
    "    # Ensure the text fits within the shape\n",
    "    subtitle.text_frame.auto_size = MSO_AUTO_SIZE.SHAPE_TO_FIT_TEXT\n",
    "    subtitle.text_frame.word_wrap = True\n",
    "\n",
    "    # Add content slides\n",
    "    for i in range(1, len(ppt_data[\"slides\"])):\n",
    "        slide_data = ppt_data[\"slides\"][i]\n",
    "        title_text = slide_data.get(\"title\", \"\")\n",
    "\n",
    "        # Detect Graphics/Graphs Slide\n",
    "        is_graphics_slide = \"graphics\" in title_text.lower() or \"graphs slide\" in title_text.lower()\n",
    "\n",
    "        # Use a blank layout for Graphics slides\n",
    "        slide_layout = prs.slide_layouts[6] if is_graphics_slide else prs.slide_layouts[1]\n",
    "        slide = prs.slides.add_slide(slide_layout)\n",
    "        apply_background(slide, theme.theme[\"background\"])\n",
    "\n",
    "        if not is_graphics_slide:\n",
    "            title = slide.shapes.title\n",
    "            title.text = title_text\n",
    "            title_para = title.text_frame.paragraphs[0]\n",
    "            title_para.font.size = Pt(32)\n",
    "            title_para.font.name = theme.theme[\"title_font\"]\n",
    "            title_para.font.color.rgb = theme.theme[\"title\"]\n",
    "        # Handling Graphics/Graphs Slide\n",
    "        \n",
    "        if is_graphics_slide and \"images\" in slide_data:\n",
    "            image_filenames = slide_data[\"images\"]\n",
    "            image_paths = [image_mapping.get(fig.replace(\".\", \"\").replace(\" \", \"\")) for fig in image_filenames]\n",
    "            image_paths = [img for img in image_paths if img and os.path.exists(img)]  # Remove missing files\n",
    "    \n",
    "            num_images = len(image_paths)\n",
    "    \n",
    "            # Get theme colors\n",
    "            caption_font = theme.theme[\"body_font\"]\n",
    "            caption_color = theme.theme[\"body\"]\n",
    "    \n",
    "            # Define positioning based on number of images\n",
    "            if num_images == 1:\n",
    "                left, top, width, height = Inches(1.5), Inches(1.5), Inches(7), Inches(5)\n",
    "                img_shape = slide.shapes.add_picture(image_paths[0], left, top, width=width, height=height)\n",
    "                caption_left = left + width / 2 - Inches(0.5)\n",
    "                caption_top = top + height + Inches(0.2)\n",
    "    \n",
    "                # Add caption\n",
    "                caption = slide.shapes.add_textbox(caption_left, caption_top, Inches(1), Inches(0.5))\n",
    "                text_frame = caption.text_frame\n",
    "                text_frame.text = image_filenames[0]\n",
    "                para = text_frame.paragraphs[0]\n",
    "                para.font.size = Pt(14)\n",
    "                para.font.name = caption_font\n",
    "                para.font.color.rgb = caption_color\n",
    "                para.alignment = PP_ALIGN.CENTER\n",
    "    \n",
    "            elif num_images == 2:\n",
    "                positions = [(Inches(1), Inches(2)), (Inches(5.5), Inches(2))]\n",
    "                size = (Inches(4), Inches(3))\n",
    "    \n",
    "                for i, img_path in enumerate(image_paths[:2]):\n",
    "                    img_left, img_top = positions[i]\n",
    "                    img_shape = slide.shapes.add_picture(img_path, img_left, img_top, *size)\n",
    "    \n",
    "                    # Add caption\n",
    "                    caption_left = img_left + size[0] / 2 - Inches(0.5)\n",
    "                    caption_top = img_top + size[1] + Inches(0.2)\n",
    "                    caption = slide.shapes.add_textbox(caption_left, caption_top, Inches(1), Inches(0.5))\n",
    "                    text_frame = caption.text_frame\n",
    "                    text_frame.text = image_filenames[i]\n",
    "                    para = text_frame.paragraphs[0]\n",
    "                    para.font.size = Pt(14)\n",
    "                    para.font.name = caption_font\n",
    "                    para.font.color.rgb = caption_color\n",
    "                    para.alignment = PP_ALIGN.CENTER\n",
    "    \n",
    "            elif num_images >= 3:\n",
    "                positions = [\n",
    "                    (Inches(1), Inches(1.5)), (Inches(5), Inches(1.5)),\n",
    "                    (Inches(3), Inches(4))\n",
    "                ]\n",
    "                size = (Inches(3.5), Inches(2.5))\n",
    "    \n",
    "                for i, img_path in enumerate(image_paths[:3]):\n",
    "                    img_left, img_top = positions[i]\n",
    "                    img_shape = slide.shapes.add_picture(img_path, img_left, img_top, *size)\n",
    "    \n",
    "                    # Add caption\n",
    "                    caption_left = img_left + size[0] / 2 - Inches(0.5)\n",
    "                    caption_top = img_top + size[1] + Inches(0.2)\n",
    "                    caption = slide.shapes.add_textbox(caption_left, caption_top, Inches(1), Inches(0.5))\n",
    "                    text_frame = caption.text_frame\n",
    "                    text_frame.text = image_filenames[i]\n",
    "                    para = text_frame.paragraphs[0]\n",
    "                    para.font.size = Pt(14)\n",
    "                    para.font.name = caption_font\n",
    "                    para.font.color.rgb = caption_color\n",
    "                    para.alignment = PP_ALIGN.CENTER\n",
    "\n",
    "        else: \n",
    "            bullet_points = slide_data.get(\"bullet_points\", [])\n",
    "            content_placeholder = slide.placeholders[1]\n",
    "            text_frame = content_placeholder.text_frame\n",
    "            text_frame.clear()\n",
    "            if bullet_points:\n",
    "                text_frame = content_placeholder.text_frame\n",
    "                text_frame.clear()  # Remove default placeholder text\n",
    "                text_frame.word_wrap = True  # Enable text wrapping\n",
    "                text_frame.auto_size = MSO_AUTO_SIZE.SHAPE_TO_FIT_TEXT  # Enable auto size for content\n",
    "\n",
    "                # Set default font size based on slide type\n",
    "                is_references = \"references\" in slide_data.get(\"title\", \"\").lower()\n",
    "                DEFAULT_FONT_SIZE = 12 if is_references else 20\n",
    "\n",
    "            for point in slide_data['bullet_points']:\n",
    "                paragraph = text_frame.add_paragraph()\n",
    "                paragraph.text = point\n",
    "                paragraph.font.size = Pt(DEFAULT_FONT_SIZE)\n",
    "                paragraph.font.name = theme.theme[\"body_font\"]\n",
    "                paragraph.font.color.rgb = theme.theme[\"body\"]\n",
    "                \n",
    "\n",
    "    # Save PowerPoint file\n",
    "    prs.save(output_file)\n",
    "    print(f\"PowerPoint presentation saved as {output_file}\")\n",
    "\n",
    "create_ppt_from_dict(ppt_content, state_output[\"extracted_images\"], \"modern\", \"sufia_2.pptx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pptx import Presentation\n",
    "# from pptx.util import Inches\n",
    "# import os\n",
    "\n",
    "# def create_ppt_from_dict(ppt_data: dict, image_mapping: dict, theme_name: str=\"default\", output_file: str = \"presentation.pptx\"):\n",
    "#     prs = Presentation()\n",
    "#     theme = ThemeConfig(theme_name)\n",
    "\n",
    "#     for slide_data in ppt_data[\"slides\"]:\n",
    "#         title_text = slide_data.get(\"title\", \"\")\n",
    "\n",
    "#         # Detect Graphics/Graphs Slide\n",
    "#         is_graphics_slide = \"graphics\" in title_text.lower() or \"graphs slide\" in title_text.lower()\n",
    "\n",
    "#         # Use a blank layout for Graphics slides\n",
    "#         slide_layout = prs.slide_layouts[6] if is_graphics_slide else prs.slide_layouts[1]\n",
    "#         slide = prs.slides.add_slide(slide_layout)\n",
    "#         apply_background(slide, theme.theme[\"background\"])\n",
    "\n",
    "#         # Title\n",
    "#         if not is_graphics_slide:\n",
    "#             title = slide.shapes.title\n",
    "#             title.text = title_text\n",
    "\n",
    "#         # Handling Graphics/Graphs Slide\n",
    "#         if is_graphics_slide and \"images\" in slide_data:\n",
    "#             image_filenames = slide_data[\"images\"]\n",
    "#             image_paths = [image_mapping.get(fig.replace(\".\", \"\").replace(\" \", \"\")) for fig in image_filenames]\n",
    "#             image_paths = [img for img in image_paths if img and os.path.exists(img)]  # Remove missing files\n",
    "\n",
    "#             num_images = len(image_paths)\n",
    "\n",
    "#             # Define positioning based on number of images\n",
    "#             if num_images == 1:\n",
    "#                 left, top, width, height = Inches(1.5), Inches(1.5), Inches(7), Inches(5)\n",
    "#                 slide.shapes.add_picture(image_paths[0], left, top, width=width, height=height)\n",
    "\n",
    "#             elif num_images == 2:\n",
    "#                 positions = [(Inches(1), Inches(2)), (Inches(5.5), Inches(2))]\n",
    "#                 size = (Inches(4), Inches(3))\n",
    "#                 for i, img_path in enumerate(image_paths[:2]):\n",
    "#                     slide.shapes.add_picture(img_path, positions[i][0], positions[i][1], *size)\n",
    "\n",
    "#             elif num_images >= 3:\n",
    "#                 positions = [\n",
    "#                     (Inches(1), Inches(1.5)), (Inches(5), Inches(1.5)),\n",
    "#                     (Inches(3), Inches(4))\n",
    "#                 ]\n",
    "#                 size = (Inches(3.5), Inches(2.5))\n",
    "#                 for i, img_path in enumerate(image_paths[:3]):\n",
    "#                     slide.shapes.add_picture(img_path, positions[i][0], positions[i][1], *size)\n",
    "\n",
    "#         else:  # Standard text slide\n",
    "#             content = slide.placeholders[1]\n",
    "#             bullet_points = slide_data.get(\"bullet_points\", [])\n",
    "#             if bullet_points:\n",
    "#                 text_frame = content.text_frame\n",
    "#                 text_frame.clear()\n",
    "#                 for point in bullet_points:\n",
    "#                     p = text_frame.add_paragraph()\n",
    "#                     p.text = point\n",
    "\n",
    "#     prs.save(output_file)\n",
    "#     print(f\"PowerPoint presentation saved as {output_file}\")\n",
    "\n",
    "# # Example Call\n",
    "# create_ppt_from_dict(ppt_content, state_output[\"extracted_images\"], \"modern\", \"hydrocarbon_2.pptx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
